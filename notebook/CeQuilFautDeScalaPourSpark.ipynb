{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "println(\"Hello World!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new java.io.File(\"/home/jovyan/work/data/shakespeare\").list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ce qu'il faut de Scala pour Spark\n",
    "\n",
    "Bienvenue. Ce notebook vous enseigne les principes fondamentaux de [Scala](http://scala-lang.org) nécessaires à l'utilisation de l'API Scala d'[Apache Spark](http://spark.apache.org). Spark utilise les meilleures fonctionnalités de Scala et évite d'utiliser celles qui sont plus difficiles et obscures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction : Pourquoi Scala ?\n",
    "Spark vous laisse le choix entre Scala, Java, Python, R et SQL pour vos travaux. Les _data engineers_ préfèreront Scala, Java ou SQL et ce sont eux qui sont chargés de construire des infrastructures scalables et résilientes pour le _Big Data_. Les _data scientists_ vont préferer Python, R et SQL et vont pouvoir construire des modèles pour analyser la donnée à l'aide du machine learning. Ils utiliseront aussi du SQL lors de l'exploration des données.\n",
    "\n",
    "Bien sûr certains data engineers ne se limiteront pas à Scala et Java et pourront aussi utiliser Python et R et vice-versa.\n",
    "\n",
    "Quelques avantages à utiliser Scala pour Spark :\n",
    "* **Performance:** Spark étant écrit en Scala, vous obtiendrez une meilleure performance et une API plus complète en utilisant Scala. Il est vrai qu'avec les [DataFrames](http://spark.apache.org/docs/latest/sql-programming-guide.html), la performance ne dépend pas du langage choisi. Si vous avez besoin d'utiliser les [RDD](http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds), alors Scala vous donnera de meilleures performances avec Java arrivant en deuxième position.\n",
    "* **Debugging:** Quand vous êtes face à une erreur de runtime, comprendre la stack d'erreur vous sera plus facile si vous connaissez Scala.\n",
    "* **Code concis et expressif:** Comparé à Java, coder en Scala est beaucoup plus concis. Cela vous permet d'être plus productif et de pouvoir écrire vos idées en code sans avoir à batailler avec des API moins flexibles qui reflètent des contraintes de langage idiomatiques (vous pourrez observer cela au fur et à mesure)\n",
    "* **Type Safety:** Comparé à Python et R, coder en Scala vous permet de bénéficier du _static typing_ avec un _type inference_. _Static typing_ signifie que le parser Scala pourra trouver beaucoup plus d'erreur dans vos expressions lors de la compilation au lieu de les découvrir au runtime. Cependant le _type inference_ vous permet de ne pas à avoir à écrire explicitement certaines informations sur le type car ce sera Scala qui s'en chargera pour vous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pourquoi pas Scala ?\n",
    "Scala n'est pas un langage parfait, deux désavantages :\n",
    "* **Librairies:** Python et R ont tous les deux un riche ecosystème de librairies dédiées à l'analyse de données. Bien que Scala s'améliore sur ce point, Python et R restent devant.\n",
    "* **Fonctionnalités avancées:** Maîtriser des fonctionnalités avancées d'un langage vous donne beaucoup de possibilités dans l'élaboration de vos programmes. Mais si vous ne comprenez pas ces fonctionnalités, elles peuvent vous rendre la tâche plus ardue tant bien que votre unique but était de finir votre tâche coûte que coûte. Scala a des concepts compliqués, particulièrement dans son _type system_. Heureusement, Spark cache ces concepts avancés."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pour en savoir plus sur Scala\n",
    "Nous ne pourrons qu'aborder qu'une toute petite partie des concepts de Scala ici. Vous en apprendrez assez pour les utiliser mais éventuellement vous allez devoir en savoir un peu plus.\n",
    "\n",
    "Quand vous aurez besoin de plus d'informations, allez voir ces ressources :\n",
    "\n",
    "* [Programming Scala, Second Edition](http://shop.oreilly.com/product/0636920033073.do): Introduction à Scala\n",
    "* [Scala Language Website](http://scala-lang.org/): Comment télécharger Scala, où trouver de la documentation(e.g., [Scaladoc](http://www.scala-lang.org/api/current/#package): documentation de la librairie Scala comme [Javadocs](https://docs.oracle.com/javase/8/docs/api/)), et autres informations.\n",
    "* [Lightbend Scala Services](http://www.lightbend.com/services/) training, consulting, et support pour Scala.\n",
    "* [Lightbend Fast Data Platform](http://www.lightbend.com/fast-data-platform/): Notre nouvelle distribution pour la _Fast Data_ (stream processing), incluant Spark, Flink, Kafka, Akka Streams, Kafka Streams, HDFS, et notre outil de gestion de production et de monitoring, tournant sur Mesosphere DC/OS.\n",
    "\n",
    "Pour l'instant je vous recommande d'ouvrir la page du Scaladoc et de l'API Scala pour Spark. Vous pouvez y accéder en cliquant sur ces deux liens :\n",
    "* Scaladocs pour <a href=\"http://www.scala-lang.org/api/current/#package\" target=\"scala_scaladocs\">Scala</a>.\n",
    "* Scaladocs pour <a href=\"http://spark.apache.org/docs/latest/api/scala/index.html#package\" target=\"spark_scaladocs\">Spark</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Astuce pour utiliser Scaladoc:**\n",
    "* Utilisez la barre de recherche dans le coin en haut à gauche pour trouver un _type_ particulier. (Par exemple, essayez de trouver RDD) \n",
    "* Pour chercher une _méthode_ particulière, cliquez sur un caractère en dessous de la barre de recherche pour la première lettre de la méthode et scrollez y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pré-requis\n",
    "Sera assumé une expérience avec un langage de programmation quelconque ainsi qu'une familiarité avec Java mais si vous ne connaissez pas Java, une simple recherche vous suffira à comprendre.\n",
    "Ceci n'est pas une introduction à Spark, une expérience avec Spark est utile mais certains concepts seront expliqués brièvement.\n",
    "\n",
    "Tout au long vous pourrez trouver plus d'informations sur les sujets importants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A propos des Notebooks\n",
    "Vous utilisez [Jupyter](http://jupyter.org/) [All Spark Notebook Docker image](https://hub.docker.com/r/jupyter/all-spark-notebook/). Comme décrit dans le [GitHub README](https://github.com/deanwampler/JustEnoughScalaForSpark) vous importez ce notebook dans Jupyter qui tourne dans un container Docker.\n",
    "\n",
    "Les notebooks vous permettent de faire un mélange de documentation comme cette [Markdown](https://daringfireball.net/projects/markdown/) \"cellule\", avec des cellules qui contiennent du code, des graphs, etc. Dans la vie réelle, cela correspondrait à un cahier qu'un étudiant ou scientifique pourrait utiliser lorsqu'il travaille en laboratoire.\n",
    "\n",
    "Le menu et la barre d'outils en haut vous donne des options pour évaluer une cellule, créer et supprimer des cellules, etc. Pensez à retenir les principaux raccourcis car vous les utiliserez souvent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Astuces:**\n",
    "\n",
    "> Vous pouvez utiliser le menu _Help > Keyboard Shortcuts_, puis capturer la page en tant qu'image. Apprenons quelques raccourcis chaque jour.\n",
    "\n",
    "> Pour le moment, sachez juste que vous pouvez cliquer dans n'importe quelle cellule pour changer de focus. Quand vous êtes dans une cellule, `shift+enter` évalue la cellule (parcourt la cellule et affiche le Markdown ou bien fait tourner le code), puis passe à la cellule suivante. Essayez cela pour quelques cellules.\n",
    "\n",
    "Une fonctionnalité utile est de pouvoir éditer une cellule que vous avez lancé précédemment et la relancer. C'est très utile lorsque vous expérimentez avec votre code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L'environnement\n",
    "Configurons l'environnement pour qu'il nous montre toujours les types des expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%showTypes on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quand vous commencez ce notebook, le plugin Jupyter Spark plugin crée un [SparkContext](http://spark.apache.org/docs/latest/programming-guide.html#initializing-spark) pour vous. C'est le point d'entrée de n'importe quelle application Spark (quand bien même vous utilisez la plus récente `SparkSession`). Ce SparkContext ou SparkSession sait comment se connecter à votre cluster (ou tourner localement sur la même JVM), comment configuer les propriétés, etc. Il lance aussi une interface web qui vous permet de monitorer vos jobs lancés. L'instance du `SparkContext` est appelée `sc`. La prochaine cellule confirme simplement qu'il existe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelques informations utiles :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"Spark version:      \" + sc.version)\n",
    "println(\"Spark master:       \" + sc.master)\n",
    "println(\"Running 'locally'?: \" + sc.isLocal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargeons des données (et commençons à apprendre le Scala)\n",
    "Nous allons écrire de vrais programmes Spark et les utiliser pour apprendre Scala et Spark en même temps. \n",
    "\n",
    "Mais tout d'abord, nous allons avoir besoin d'utiliser quelques fichiers texte qui contiennent des pièces de Shakespeare. Les prochaines cellules définissent des méthodes helper pour configurer le tout. Nous apprendrons des concepts Scala à la volée."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** \"méthode\" vs. \"fonction\"\n",
    "\n",
    "> Scala utilise une convention commune provenant de l'orienté objet, où le terme _méthode_ est utilisé pour une fonction qui est attachée à une classe ou instance. Contrairement à Java, tout du moins avant Java 8, Scala possède aussi des _fonctions_ qui ne sont pas associés à une classe ou instance particulière.\n",
    "\n",
    "> Dans notre prochain exemple de code, nous définissons plusieurs _méthodes_ helper pour afficher des informations, mais vous ne verrez pas de définition de classe ici. Ainsi, quelle est la classe associée à ces méthodes ? Quand vous utilisez Scala dans un notebook, vous utilisez en fait l'interpréteur Scala, qui prend chaque expression et définition que nous avons écrit et les met dans une classe cachée. L'interpréteur fait cela pour générer du byte code valide pour la JVM.\n",
    "\n",
    "> Malheureusement, cela peut-être assez confus de savoir quand il faut utiliser une méthode ou une fonction et cela reflète la nature hybride de Scala comme un langage orienté objet et un langage fonctionnel. Heureusement, dans la majorité des cas, nous pouvons utiliser des méthodes et fonctions de façon interchangeable, donc ne vous préoccupez pas trop de la distinction à partir de maintenant.\n",
    "\n",
    "> Nous allons maintenant définir des méthodes. Nous allons voir ce qu'est une véritable _fonction_ bientôt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici deux méthodes pour printer un message d'erreur ou un simple message d'information. Nous allons expliquer la syntaxe dans la cellule qui suit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/*\n",
    " * \"info\" takes a single String argument, prints it on a line,\n",
    " * and returns it. \n",
    " */\n",
    "def info(message: String): String = {\n",
    "    println(message)\n",
    "\n",
    "    // The last expression in the block, message, is the return value. \n",
    "    // \"return\" keyword not required.\n",
    "    // Do no additional formatting for the return string.\n",
    "    message  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/*\n",
    " * \"error\" takes a single String argument, prints a formatted error message,\n",
    " * and returns the message. \n",
    " */\n",
    "def error(message: String): String = {   \n",
    "    \n",
    "    // Print the string passed to \"println\" and add a linefeed (\"ln\"):\n",
    "    // See the next cell for an explanation of how the string is constructed.\n",
    "    val fullMessage = s\"\"\"\n",
    "        |********************************************************************\n",
    "        |\n",
    "        |  ERROR: $message\n",
    "        |\n",
    "        |********************************************************************\n",
    "        |\"\"\".stripMargin\n",
    "    println(fullMessage)\n",
    "    \n",
    "    fullMessage\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essayons les."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val infoString = info(\"All is well.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val errorString = error(\"Uh oh...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errorString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les définitions des méthodes possèdent les éléments suivants dans cet ordre :\n",
    "* Le mot clé `def` \n",
    "* Le nom de la méthode (`error` et `info` ici)\n",
    "* La liste des arguments entre parenthèses. S'il n'y a pas d'arguments, les parenthèses vides peuvent être omises. Commun pour `toString` et les getter qui retournent un champ dans une instance, etc.\n",
    "* Deux points `:` suivi par le type de retour de la méthode. Ce type est souvent inferé par Scala et il n'est donc pas obligatoire, mais il est fortement recommandé de le mettre tout le temps !\n",
    "* Un signe `=` qui sépare la _signature_ de la méthode du _corps_\n",
    "* Le corps entre accolades `{ ... }`, cependant si le corps consiste en une seule expression alors les accolades sont optionnelles\n",
    "* La dernière expression dans le corps est utilisée comme valeur de retour. Le mot clé `return` est déconseillé\n",
    "* Les point-virgules `;` sont inferés et ne sont pas utilisés dans la grande majorité des cas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardez la liste des arguments pour `error`. Il s'agit de `(message: String)`, où `message` est le nom de l'argument et son type est `String`. La convention pour les _annotations de type_, `name: Type`, est aussi utilisé pour le type de retour, `error(...): String`. Les annotations de type sont requis par Scala pour les arguments des méthodes. Elles sont optionnelles dans la majorité des cas pour le type de retour. Nous allons voir que Scala peut inférer les types de beaucoup d'expressions et de déclarations de variables.\n",
    "\n",
    "Scala utilise les mêmes conventions de commentaires que Java, `// ...` pour une seule ligne, et `/* ... */` pour un bloc de commentaire.\n",
    "\n",
    "> **Note:** Expression vs. Déclaration\n",
    "\n",
    "> Une _expression_ a une valeur, tandis qu'une _déclaration_ n'en a pas. Ainsi lorsque nous assignons une expression à une variable, la valeur que l'expression retourne est assignée à une variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans `error`, nous utilisons une combinaison d'interpolation de string avec la syntaxe `s\"\"\"...\"\"\"` :\n",
    "* **String avec trois guillemets :** `\"\"\"...\"\"\"`. Utile lorsque la string contient des retours à la ligne, comme dans `error`. (Nous allons aussi voir un autre avantage plus tard)\n",
    "* **Interpolation de String :** Utilisé en mettant `s` au début de la string `s\"...\"` ou `s\"\"\"...\"\"\"`. Cela nous permet de mettre des références à des variables et des expressions où la conversion en string sera insérée automatiquement. Par exemple : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s\"\"\"Utilisez des accolades pour les expressions : ${sc.version}.\n",
    "Elles sont optionnelles quand vous utilisez juste une variable : $sc\n",
    "Faites attention dans les cas comme : ${sc}etunautretruc\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une autre fonctionnalité que nous utilisons pour les strings entre trois guillemets est la possiblité d'enlever le premier espace de chaque ligne. La méthode `stripMargin` enlève tous les espaces avant en incluant `|`. Cela vous permet d'indenter ces lignes pour que votre code soit formaté proprement mais sans avoir d'espace dans votre string. Dans l'exemple suivant, la string résultante possède des espaces en début de ligne et à la fin de la ligne. Observez ce qui arrive avec les espaces avant `line2` et `line3` quand la string entière est affichée :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s\"\"\"\n",
    "    |line 1\n",
    "    |  line 2\n",
    "    |  |  line 3\n",
    "    |\"\"\".stripMargin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les caractères 'littéraux' sont indiqués avec des apostrophes, '/', tandis que les string sont indiquées avec des guillemets, \"/\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables mutables vs. valeurs immutables\n",
    "Regardons comment déclarer une valeur immutable avec `val` :\n",
    "* `val immutableValue = ...`: Une fois initialisée, nous ne pouvons pas attribuer une valeur _différente_ à `immutableValue`.\n",
    "* `var mutableVariable = ...`: Nous pouvons attribuer de nouvelles valeurs à `mutableVariable` autant de fois qu'on le souhaite.\n",
    "\n",
    "Il est _très recommandé_ de n'utiliser que des `vals` à moins que vous ayez une très bonne raison d'avoir recours à de la mutabilité, qui est une source très commune de bugs !\n",
    "\n",
    "> Un `val immutableValue` peut pointer vers une instance qui _elle_ est mutable par exemple un [Array](http://www.scala-lang.org/api/current/#scala.Array). Dans ce cas là, même si nous ne pouvons pas assigner un nouveau Array à `immutableValue`, nous pouvons changer les éléments dans l'Array ! Dis d'une autre façon, l'immutabilité n'est pas _transitive_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Préparer les fichiers\n",
    "Ce notebook possède déjà les fichiers de données nécessaires, à savoir plusieurs pièces de Shakespeare. Elles sont dans\n",
    "le dossier `/home/jovyan/work/data/shakespeare` dans le container (`data/shakespeare` dans le projet git). Il y a un fichier par pièce.\n",
    "\n",
    "Nous allons écrire du code Scala pour vérifier que les fichiers sont bien présents et en même temps apprendre un peu de Scala."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beaucoup de types utilisés en Scala proviennent de la librairie Java (JDK). Comme Scala compile vers du byte code JVM, vous pouvez utiliser n'importe quelle librairie Java en Scala. Nous étions en train d'utiliser [java.lang.String](https://docs.oracle.com/javase/8/docs/api/java/lang/String.html). Nous allons maintenant utiliser [java.io.File](https://docs.oracle.com/javase/8/docs/api/java/io/File.html) pour travailler avec les différents fichiers et dossiers.\n",
    "\n",
    "Comme avant, nous allons utiliser des commentaires pour expliquer quelques nouvelles notions de Scala."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Import File. Unlike Java, the semicolon ';' is not required.\n",
    "import java.io.File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici le dossier où les fichiers devraient être situés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val shakespeare = new File(\"/home/jovyan/work/data/shakespeare\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le `if` en Scala est en fait une expression (en Java ce sont des _déclarations_). L'expression `if` retourne `true` ou `false` et l'assigne à un `success` que nous allons utiliser juste après."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val success = if (shakespeare.exists == false) {   // doesn't exist already?\n",
    "    error(s\"Data directory path doesn't exist! $shakespeare\")  // ignore returned string\n",
    "    false\n",
    "} else {\n",
    "    info(s\"$shakespeare exists\")\n",
    "    true\n",
    "}\n",
    "println(\"success = \" + success)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vérifions maintenant que les fichiers sont bien là."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val pathSeparator = File.separator\n",
    "val targetDirName = shakespeare.toString\n",
    "val plays = Seq(\n",
    "    \"tamingoftheshrew\", \"comedyoferrors\", \"loveslabourslost\", \"midsummersnightsdream\",\n",
    "    \"merrywivesofwindsor\", \"muchadoaboutnothing\", \"asyoulikeit\", \"twelfthnight\")\n",
    "\n",
    "if (success) {\n",
    "    println(s\"Checking that the plays are in $shakespeare:\")\n",
    "    val failures = for {\n",
    "        play <- plays\n",
    "        playFileName = targetDirName + pathSeparator + play\n",
    "        playFile = new File(playFileName)\n",
    "        if (playFile.exists == false)\n",
    "    } yield {\n",
    "        s\"$playFileName:\\tNOT FOUND!\"\n",
    "    }\n",
    "  \n",
    "    println(\"Finished!\")\n",
    "    if (failures.size == 0) {\n",
    "        info(\"All plays found!\")\n",
    "    } else {\n",
    "        println(\"The following expected plays were not found:\")\n",
    "        failures.foreach(play => error(play))\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous utilisons ici un `for` _comprehension_. Ce sont des _expressions_, et non pas des _déclarations_ comme les `for` Java. Elles ont la forme :\n",
    "\n",
    "```\n",
    "for {\n",
    "  play <- plays\n",
    "  ...\n",
    "} yield { block_of_final_expressions }\n",
    "```\n",
    "Nous itérons sur une collection `plays`, et assignons à chaque élément la variable `play` (qui est en fait une valeur immutable pour chaque itération). \n",
    "\n",
    "Après avoir assigné `play`, les prochaines étapes dans la `for` comprehension l'utilisent. Premièrement, une instance de  [java.io.File](https://docs.oracle.com/javase/8/docs/api/java/io/File.html) `playFile`, est créée. Ensuite `playFile` est utilisé pour évaluer un prédicat - le fichier existe t'il déjà ? (Il devrait !)\n",
    "\n",
    "Si le fichier existe déjà alors `false` est retourné, ce qui casse la boucle et nous passons à la prochaine itération.\n",
    "Si le fichier n'existe pas, alors le mot clé `yield` dit à Scala que nous voulons utiliser l'expression qui suit pour construire un nouvel élément, une string _interpolée_ pour les pièces manquantes. Parmi ces éléments retournés, pouvant aller de zéro à un nombre, une nouvelle collection est construite. Le bloc final `if` détermine si la nouvelle collection a zéro élément (attendu), puis affiche un message `info`. Si des fichiers manquaient, alors un message `error` serait affiché pour chacun des fichiers manquant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utiliser des fonctions comme argument\n",
    "\n",
    "Notez comment nous avons affiché les `successes`. L'idiome `collection.foreach(println)` est utile pour itérer sur des éléments et les afficher, un par ligne. Mais comment cela marche exactement ? (Nous utiliserons `plays` au lieu de `failures`, parce que le deuxième est normalement toujours vide !)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"Pass println as the function to use for each element:\")\n",
    "plays.foreach(println)\n",
    "\n",
    "println(\"\\nUsing an anonymous function that calls println: `str => println(str)`\")\n",
    "println(\"(Note that the type of the argument `str` is inferred to be String.)\")\n",
    "plays.foreach(str => println(str))\n",
    "\n",
    "println(\"\\nAdding the argument type explicitly. Note that the parentheses are required.\")\n",
    "plays.foreach((str: String) => println(str))\n",
    "\n",
    "println(\"\\nWhy do we need to name this argument? Scala lets us use _ as a placeholder.\")\n",
    "plays.foreach(println(_))\n",
    "\n",
    "println(\"\\nFor longer functions, you can use {...} instead of (...).\")\n",
    "println(\"Why? Because it gives you the familiar multiline block syntax with {...}\")\n",
    "plays.foreach {\n",
    "  (str: String) => println(str)\n",
    "}\n",
    "\n",
    "println(\"\\nThe _ placeholder can be used *once* for each argument in the list.\")\n",
    "println(\"As an assume, use `reduceLeft` to sum some integers.\")\n",
    "val integers = 0 to 10   // Return a \"range\" from 0 to 10, inclusive\n",
    "integers.reduceLeft((i,j) => i+j)\n",
    "integers.reduceLeft(_+_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notre premier programme Spark\n",
    "Ouf ! Nous avons déjà appris beaucoup de Scala en faisant des tâches typiques de data science (par exemple récupérer de la donnée)\n",
    "Maintenant nous pouvons implémenter un algorithme en utilisant Spark, _Inverted Index_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverted Index - Quand vous ne voulez plus compter les mots...\n",
    "\n",
    "Vous allez avoir besoin d'utiliser _Inverted Index_ quand vous allez créer votre prochain \"Google killer\". Il prend en entrée un corpus de documents (des pages webs par exemple), tokenize les mots et sort pour chaque mot une liste des documents qui contiennent ce mot avec à côté le nombre de fois où ce mot apparaît.\n",
    "\n",
    "C'est un algorithme un peu plus intéressant que le _Word Count_, qui est en quelque sorte le \"hello world\" que tout le monde fait quand une personne apprend Spark.\n",
    "\n",
    "Le terme _inverted_  signifie que nous commençons avec des mots comme valeurs d'entrée tandis que les clés sont des identifiants de documents et que nous allons inverser le fait d'utiliser les mots comme clé et les identifiants de document comme valeur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici notre première version dans son entièreté. C'est une _unique, longue expression_. Notez les points `.` à la fin des sous expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val iiFirstPass1 = sc.wholeTextFiles(shakespeare.toString).\n",
    "    flatMap { location_contents_tuple2 => \n",
    "        val words = location_contents_tuple2._2.split(\"\"\"\\W+\"\"\")\n",
    "        val fileName = location_contents_tuple2._1.split(pathSeparator).last\n",
    "        words.map(word => ((word, fileName), 1))\n",
    "    }.\n",
    "    reduceByKey((count1, count2) => count1 + count2).\n",
    "    map { word_file_count_tup3 => \n",
    "        (word_file_count_tup3._1._1, (word_file_count_tup3._1._2, word_file_count_tup3._2)) \n",
    "    }.\n",
    "    groupByKey.\n",
    "    sortByKey(ascending = true).\n",
    "    mapValues { iterable => \n",
    "        val vect = iterable.toVector.sortBy { file_count_tup2 => \n",
    "            (-file_count_tup2._2, file_count_tup2._1)\n",
    "        }\n",
    "        vect.mkString(\",\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant cherchons à décomposer en étapes, en assignant chaque étape à une variable. La verbosité supplémentaire que cela apporte nous permettra de voir ce que Scala infère pour le type retourné de chaque expression, dans des buts d'apprentissage.\n",
    "\n",
    "C'est une des fonctionnalités très agréable de Scala. Nous n'avons pas à mettre manuellement d'information de type la plupart du temps comme nous aurions dû le faire avec du code Java. Au contraire, nous laissons le compilateur nous donner un retour sur ce que nous venons de créer. C'est très utile lorsque vous apprenez une nouvelle API, comme Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val fileContents = sc.wholeTextFiles(shakespeare.toString)\n",
    "fileContents   // force the notebook to print the type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La deuxième ligne, avec `fileContents` tout seul, est là pour que le notebook nous montre les informations de type.(Essayez de l'enlever et de réévaluer la cellule, rien ne sera affiché)\n",
    "\n",
    "La sortie nous indique que `fileContents` possède le type [RDD[(String,String)]](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD), cependant `RDD` est une classe de base qui est en fait une instance de `MapPartitionsRDD`, une implémentation \"privée\" de la sous classe `RDD`. \n",
    "\n",
    "Un nom suivi de crochets, `[...]`, signifie que le `RDD[...]` a besoin d'un ou plusieurs paramètres de type dans les crochets. Dans ce cas là, il y a un unique paramètre de type qui représente le type des enregistrements du `RDD`. \n",
    "\n",
    "Le paramètre de type unique est donné par `(String,String)`, qui est un raccourci bien utile pour [Tuple2[String,String]](http://www.scala-lang.org/api/current/index.html#scala.Tuple2).\n",
    "Cela signifie que nous avons des tuples de deux éléments comme enregistrements, où le premier élément est une `String` représentant le chemin absolu d'un fichier et que le second élément est aussi une `String` qui représente le contenu du fichier. C'est ce que retourne `SparkContext.wholeTextFiles` pour nous. Nous utiliserons le chemin du fichier pour se rappeler où nous avons trouvé des mots, tandis que le contenu contiendra les mots même.\n",
    "\n",
    "Pour résumer, ces deux types sont équivalents :\n",
    "* `RDD[(String,String)]` - Notez les parenthèses entre les crochets, `[(...)]`.\n",
    "* `RDD[Tuple2[String,String]]` - Notez les crochets dans les crochets `[...[...]]`, et non pas `[(...)]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons voir sous peu que nous pouvons aussi écrire des _instances_ de [Tuple2[T1,T2]](http://www.scala-lang.org/api/current/index.html#scala.Tuple2) avec la même syntaxe, e.g., `(\"foo\", 101)`, pour un tuple `(String,Int)`, et similairement pour des tuples d'_arité supérieur_ (jusqu'à 22 elements...), e.g., `(\"foo\", 101, 3.14159, (\"bar\", 202L))`. Éxécutez la prochaine cellule pour voir le type du tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\"foo\", 101, 3.14159, (\"bar\", 202L))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avez-vous compris ? Voyez-vous qu'il s'agit d'un tuple de quatre élément et non pas d'un tuple de cinq élément ? C'est parce que `(\"bar\", 202L)` est un tuple imbriqué. C'est le quatrième élément du tuple extérieur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice:** Essayez de créer des tuples avec des éléments de type différent, utilisez la prochaine cellule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(1,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combien de `fileContents` avons-nous ? Pas énormément. Nous devrions normalement avoir le même nombre que le nombre de fichier que nous avons chargé en haut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileContents.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE:** Nous avons utilisé la méthode`RDD.count`, alors que la majorité des collections Scala ont une méthode `size`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant pour la prochaine étape de notre calcul. En premier, nous allons tokenizer le contenu des mots en les séparant sur les caractères non alphanumériques, donc toutes les instances d'espaces, de retours à la ligne, de ponctuations, etc.\n",
    "\n",
    "Ensuite, le chemin absolu étant verbeux et le même préfixe étant répété pour tous les fichiers, prenons juste le dernier élément, qui est le nom unique du fichier.\n",
    "\n",
    "Ainsi nous allons former des tuples avec des mots et des noms de fichier.\n",
    "\n",
    "> **Note:** Cette tokenization est très brute. Elle ne prend pas en compte les contractions comme `it's` et les mots séparés par un trait d'union comme `world-changing`. Lorsque vous allez tuer Google, soyez sûr d'utiliser une vraie technique de tokenization NLP, NLP pour Natural Language Processing ou Traitement de Langage Naturel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val wordFileNameOnes = fileContents.flatMap { location_contents_tuple2 => \n",
    "    // example input record: (file_path, \"all the words in the file\")\n",
    "    // mytuple._2 => give me the 2nd element\n",
    "    val words = location_contents_tuple2._2.split(\"\"\"\\W+\"\"\")              \n",
    "    // mytuple._1 => give me the 1st element\n",
    "    val fileName = location_contents_tuple2._1.split(pathSeparator).last  \n",
    "    // create a new tuple to return. Note how we structured it!\n",
    "    words.map(word => ((word, fileName), 1))\n",
    "}\n",
    "wordFileNameOnes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je trouve que cela est dur à lire et nous allons bientôt voir une solution plus élégante avec une syntaxe alternative.\n",
    "\n",
    "Essayons de comprendre la différence entre `map` et `flatMap`. Si j'appellais `fileContents.map`, cela retournerait exactement _un_ nouvel enregistrement pour chaque enregistrement dans _fileContents_. Ce que nous voulons en réalité sont des nouveaux enregistrements pour chaque couple de mot-nomDeFichier, qui correspondrait à un nombre plus large (mais la donnée dans chaque enregistrement serait beaucoup plus petite)\n",
    "\n",
    "Utiliser `fileContents.flatMap` nous donne ce que nous voulons. Au lieu de retourner en sortie un enregistrement pour chaque enregistrement en entrée, un `flatMap` retourne une _collection_ de nouveaux enregistrements, de taille supérieure à 0, pour _chaque_ enregistrement d'entrée. Ces collections sont ensuite _aplaties_ dans une grosse collection, un autre `RDD` dans notre cas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que devrais faire `flatMap` pour chaque enregistrement ? Nous passons une _fonction_ pour définir le comportement. Nous utilisons donc une fonction sans nom ou _anonyme_. La syntaxe est `liste_des_arguments => body`:\n",
    "\n",
    "```scala\n",
    "location_contents_tuple2 => \n",
    "    val words = ...\n",
    "    ...\n",
    "}\n",
    "```\n",
    "\n",
    "Nous avons un argument unique, l'enregistrement, que nous avons nommé `location_contents_tuple2`, une façon verbeuse de dire que nous avons à faire à un tuple de deux éléments avec en entrée le chemin du fichier et son contenu. Nous n'avons pas besoin d'un paramètre de type après `location_contents_tuple2` car cela est inféré par Scala. La flèche `=>` sépare la liste des arguments du corps.\n",
    "\n",
    "Quand une fonction prend plus qu'un seul argument ou que vous ajoutez des annotations de type explicite (`: (String,Int,Double)`), alors vous aurez besoin de parenthèses. Voici trois exemples :\n",
    "\n",
    "```scala\n",
    "(some_tuple3: (String,Int,Double)) => ...\n",
    "(arg1, arg2, arg3) => ...\n",
    "(arg1: String, arg2: Int, arg3: Double) => ...\n",
    "```\n",
    "Nous laissons Scala inférer le type des arguments, et dans notre cas c'est `(String,String)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une seconde, nous avons précedemment dit que nous passions une fonction en tant qu'argument de `flatMap`. Si c'est le cas, pourquoi utilisons nous des accolades `{...}` autour de l'argument de cette fonction au lieu de parenthèses `(...)` qu'il serait normal de retrouver lorsque vous passez des arguments à une méthode comme `flatMap`? \n",
    "\n",
    "C'est parce que Scala nous permet d'utiliser des accolades au lieu de parenthèses pour avoir la syntaxe familière du bloc `{...}` que nous connaissons tous et aimons pour les expressions `if` et `for`. Nous pourrions utiliser soit des accolades ou des parenthèses ici. La convention dans la communauté Scala est d'utiliser des accolades pour les fonctions anonymes qui font plusieurs lignes et des parenthèses pour les expressions tenant sur une ligne."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant pour chaque `location_contents_tuple2`, nous accédons au _premier_ élément en utilisant la méthode `_1` et le _deuxième_ élément en utilisant `_2`.\n",
    "\n",
    "Le fichier `contents` est dans le deuxième élément. Nous les séparons en appelant la méthode Java `String.split`, qui prend une string correspondant à une _expression régulière_. Ici nous spécifions une expression régulière pour un ou plusieurs caractères non alphanumériques. `String.split` retourne un `Array[String]` des mots. \n",
    "\n",
    "```scala\n",
    "val words = location_contents_tuple2._2.split(\"\"\"\\W+\"\"\")\n",
    "```\n",
    "\n",
    "Pour le premier élément du tuple, nous extrayons le nom du fichier en fin de path. Ce n'est pas nécessaire, mais ça permet d'avoir une sortie plus lisible si nous retirons le long préfixe commun du path.\n",
    "\n",
    "```scala\n",
    "val fileName = location_contents_tuple2._1.split(pathSeparator).last\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalement, nous utilisons `Array.map` (et non pas `RDD.map`) dans la fonction anonyme passée au `flatMap` pour transformer chaque `word` en un tuple de la forme `((word, fileName), 1)`.\n",
    "\n",
    "```scala\n",
    "words.map(word => ((word, fileName), 1))\n",
    "```\n",
    "\n",
    "Pourquoi avons nous imbriqué un tuple de `(word, fileName)` dans un tuple \"extérieur\" avec un `1` comme deuxième élément ? Pourquoi ne pas seulement avoir créé un tuple de trois éléments `(word, fileName, 1)`? C'est parce que nous utilisons `(word, fileName)` comme une _clé_ dans la prochaine étape, où nous allons trouver des combinaisons uniques de word-fileName (en utilisant l'équivalent du `group by`). Ainsi, utiliser le `(word, fileName)` imbriqué comme la clé _key_ est plus pratique. La _valeur_ `1` _value_ est un count \"seed\", que nous allons utiliser pour compter les occurences uniques des paires de `(word, fileName)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Notes:**\n",
    "> * Pour des raisons historiques, les indices de tuple commencent à 1 et non 0. Les Arrays et autre collections de Scala ont un index commencant à 0.\n",
    "> * Nous avons précédemment dit que les arguments d'une _méthode_ ont besoin d'être déclarés avec des types. Ce n'est pas nécessairement requis pour les arguments de _fonctions_ comme ici.\n",
    "> * Un autre bénéfice d'une string avec trois guillemets qui les rend sympathique pour les expressions régulières est que vous n'avez pas à échapper vos caractères comme `\\W`. Si nous avions utilisé un seul guillemet alors nous aurions du écrire `\"\\\\W+\"`. A vous de faire votre choix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comptons le nombre de lignes que nous avons et regardons quelques lignes. Nous utiliserons la méthode `RDD.take` pour prendre les 10 premières lignes, itérer dessus et les afficher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordFileNameOnes.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordFileNameOnes.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons demandé des résultats, nous forçons donc Spark à lancer un job pour calculer le résultat. Les pipelines Spark comme `iiFirstPass1` sont _lazy_; rien n'est calculé tant que nous ne demandons pas de résultat. \n",
    "\n",
    "Quand vous apprenez, il est utile d'afficher des données pour comprendre mieux ce qui se passe. Faites attention toutefois car cela utilise plus de ressources.\n",
    "\n",
    "Le premier enregistrement nous montre \"\" (vide) comme mot :\n",
    "\n",
    "```\n",
    "((,asyoulikeit),1)\n",
    "```\n",
    "\n",
    "Aussi certains mots sont en majuscules :\n",
    "```\n",
    "((DRAMATIS,asyoulikeit),1)\n",
    "```\n",
    "(Vous pouvez voir que ces mots en majuscule apparaissent si vous regardez dans les fichiers sources) Plus tard nous allons filtrer tous ces mots vides et passer tous les mots en minuscule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant joignons toutes les paires uniques de `(word,fileName)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val uniques = wordFileNameOnes.reduceByKey((count1, count2) => count1 + count2)\n",
    "uniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En SQL vous pouvez utiliser `GROUP BY` pour ça (en incluant les requêtes SQL que vous pourriez écrire avec l'API Spark [DataFrame](http://spark.apache.org/docs/latest/sql-programming-guide.html)). Néanmoins, dans l'API `RDD`, c'est trop couteux pour nos besoins car nous ne nous préoccupons pas des groupes eux-mêmes, la longue liste des paires répétées de `(word,fileName)`. Nous nous préoccupons juste de combien d'éléments il y a dans chaque groupe, c'est à dire leur _size_. C'est la raison du `1` dans les tuples et pourquoi nous utilisons `RDD.reduceByKey`. Cela ramène ensemble toutes les lignes avec la même clé, les paires uniques de `(word,fileName)`, et applique ensuite une fonction anonyme pour \"réduire\" les valeurs, les`1`. Nous faisons juste une somme après pour calculer le count des groupes.\n",
    "\n",
    "Notez que la fonction anonyme `reduceByKey` attend deux arguments, nous avons donc besoin de parenthèses autour de la liste des arguments. Puisque la fonction tient sur une seule ligne, nous utilisons des parenthèses au lieu d'accolades.\n",
    "\n",
    "> **Note:** Toutes les méthodes `*ByKey` opèrent sur des tuples de deux éléments et traitent le premier élément comme la clé par défaut."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combien y en a t-il ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniques.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme attendu d'un `GROUP BY`, le nombre d'enregistrements est plus petit qu'avant. Il y a environ 1/6 de lignes comparé à avant, cela signifie qu'en moyenne chaque `(word,fileName)` apparaît 6 fois."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniques.take(30).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour _inverted index_, nous voulons que les dernières clés soient des mots, il faut donc restructurer les tuples de `((word,fileName),count)` vers `(word,(fileName,count))`. Maintenant, nous allons encore sortir des tuples à deux éléments mais le `word` sera la clé et le `(fileName,count)` sera la valeur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val words = uniques.map { word_file_count_tup3 => \n",
    "    (word_file_count_tup3._1._1, (word_file_count_tup3._1._2, word_file_count_tup3._2)) \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les méthodes de tuple `_1._2` sont assez difficiles à lire et la logique devient obscure. Nous allons voir par la suite une alternative beaucoup plus élégante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons utiliser l'opération `group by`, car nous en avons besoin pour garder les groupes. Appeller `RDD.groupByKey` \n",
    "utilise le premier élément du tuple, présentement juste les `words`, pour ramener ensemble toutes les occurences de mots uniques. Ensuite nous allons trier les mots par ordre alphabétique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val wordGroups = words.groupByKey.sortByKey(ascending = true)\n",
    "wordGroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notez que chaque groupe est en fait un [Iterable](http://www.scala-lang.org/api/current/index.html#scala.collection.Iterable), i.e., une abstraction pour une certaine forme de collection. (C'est actuellement une collection privée définie par Spark appelée `CompactBuffer`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordGroups.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordGroups.take(30).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalement, nettoyons ces `CompactBuffers`. Nous pouvons les convertir en [Vector](http://www.scala-lang.org/api/current/index.html#scala.collection.immutable.Vector) (une collection avec une performance de _O(1)_ pour la plupart des opérations), puis les ordonner par ordre décroissant de count, afin que les lieux qui mentionnent le _plus_ le mot correspondant apparaissent en _premier_ dans la liste. (Réfléchissez à comment vous voudriez qu'un moteur de recherche fonctionne...) \n",
    "\n",
    "Notez que nous utilisons `Vector.sortBy`, et pas un sort provenant de `RDD`. Ce sortBy prend une fonction qui accepte tous les éléments d'une collection et retourne quelque chose utilisé pour trier la collection. En retournant `(-fileNameCountTuple2._2, fileNameCountTuple2)`, je dis, \"triez par count _décroissant_ en premier, puis triez par le nom du fichier\". `-fileNameCountTuple2._2` cause les counts à être triés par ordre décroissant car je retourne une valeur négative donc les counts plus larges seront inférieurs aux counts plus petits, par exemple `-3 < -2`.\n",
    "\n",
    "Finalement, je prend le `Vector` résultant et je crée un CSV avec les éléments en utilisant la méthode helper `mkString`.\n",
    "\n",
    "Qu'est donc `RDD.mapValues` ? Je pourrais utiliser `RDD.map`,mais je ne change pas les clés (les mots), donc au lieu d'avoir à faire au tuple avec les deux éléments, `mapValues` passe seulement la valeur du tuple et reconstruit un nouveau tuple `(clé,valeur)` avec la nouvelle valeur que ma fonction retourne. Ainsi, `mapValues` est plus pratique à utiliser que `map` quand j'ai un tuple de deux éléments et que je ne modifie pas les clés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val iiFirstPass2 = wordGroups.mapValues { iterable => \n",
    "    val vect = iterable.toVector.sortBy { file_count_tup2 => \n",
    "        (-file_count_tup2._2, file_count_tup2._1)\n",
    "    }\n",
    "    vect.mkString(\",\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C'est bon ! Le nombre de records est le même que `wordGroups` (comprenez-vous pourquoi ?), observons donc quelques records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iiFirstPass2.take(30).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cela paraît raisonnable.\n",
    "\n",
    "Nous allons maintenant améliorer le code en utilisant une feature très puissante, le _pattern matching_, qui permet de rendre le code à la fois plus concis et facile à comprendre. C'est ma fonctionnalité *préferée* de Scala. \n",
    "\n",
    "Avant cela, essayez d'améliorer le code par vous même.\n",
    "\n",
    "**Exercices:**\n",
    "\n",
    "* Ajoutez un filtre pour enlever le mot vide \"\". Vous pouvez faire cela de deux manières différentes, en utilisant [RDD.filter](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD) (cherchez dans [Scaladoc page]((http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD) pour la méthode `filter`), _ou_ en utilisant la méthode similaire implémentée par Scala, [scala.collection.Seq.filter](http://www.scala-lang.org/api/current/index.html#scala.collection.Seq). Les deux versions prennent une fonction _prédicat_, fonction qui retourne  `true` si le record doit être _conservée_ et `false` dans le cas contraire. Pensez-vous qu'un des deux choix est meilleur que l'autre ? Pourquoi ? Ou ces deux choix sont-ils au final similaires ? Les raisons peuvent inclure la compréhension du code et la performance.\n",
    "\n",
    "* Convertissez tous les mots en minuscule. Il faut juste appeler `toLowerCase` sur une string. A quel endroit est-il judicieux de placer cet appel ?\n",
    "\n",
    "J'implémenterais ces deux changements dans les améliorations à suivre en bas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE:** Si vous préférez faire une copie du code dans une nouvelle cellule, utilisez le menu _Insert_ en haut pour ajoutez des cellules. Ou vous pouvez apprendre un autre raccourci clavier `ESC`, puis `A` pour insérer avant ou `B` pour insérer après. Vous pouvez ensuite appuyer sur entrée pour éditer la cellule. Remarquez la barre d'outil pour configurer le format de la cellule. Cette cellule que vous lisez est par exemple en _Markdown_. Utilisez _Code_ pour les cellules contenant votre code source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern Matching\n",
    "\n",
    "Nous avons étudié un vrai programme et nous avons appris pas mal de Scala. Améliorons ça avec ma fonctionnalité préférée de Scala : _le pattern matching_.\n",
    "\n",
    "Voici la version \"premier passage\" à nouveau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val iiFirstPass1b = sc.wholeTextFiles(shakespeare.toString).\n",
    "    flatMap { location_contents_tuple2 => \n",
    "        val words = location_contents_tuple2._2.split(\"\"\"\\W+\"\"\")\n",
    "        val fileName = location_contents_tuple2._1.split(pathSeparator).last\n",
    "        words.map(word => ((word, fileName), 1))\n",
    "    }.\n",
    "    reduceByKey((count1, count2) => count1 + count2).\n",
    "    map { word_file_count_tup3 => \n",
    "        (word_file_count_tup3._1._1, (word_file_count_tup3._1._2, word_file_count_tup3._2)) \n",
    "    }.\n",
    "    groupByKey.\n",
    "    sortByKey(ascending = true).\n",
    "    mapValues { iterable => \n",
    "        val vect = iterable.toVector.sortBy { file_count_tup2 => \n",
    "            (-file_count_tup2._2, file_count_tup2._1)\n",
    "        }\n",
    "        vect.mkString(\",\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, voici une nouvelle implémentation qui utilise le _pattern matching_.\n",
    "\n",
    "J'y ai fait deux autres ajouts : les solutions des derniers exercices, qui retire les mots vides \"\" et corrige les majuscules mélangées, en utilisant les ajouts suivants :\n",
    "\n",
    "* `filter(word => word.size > 0)` pour retirer les mots vides. (En Spark et dans les collections Scala, `filter` a un sens positif : \"qu'est-ce qui doit être retenu ?\"). C'est indiqué par le commentaire `// #1`.\n",
    "* `word.toLowerCase` pour convetir tous les mots uniformément en minuscule, tel que des mots comme HAMLET, Hamlet et hamlet dans le texte original sont traités comme étant le même mot, puisque nous comptons les occurences des mots. Voir le commentaire `// #2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val ii1 = sc.wholeTextFiles(shakespeare.toString).\n",
    "    flatMap {\n",
    "        case (location, contents) => \n",
    "            val words = contents.split(\"\"\"\\W+\"\"\").\n",
    "                filter(word => word.size > 0)                      // #1\n",
    "            val fileName = location.split(pathSeparator).last\n",
    "            words.map(word => ((word.toLowerCase, fileName), 1))   // #2\n",
    "    }.\n",
    "    reduceByKey((count1, count2) => count1 + count2).\n",
    "    map { \n",
    "        case ((word, fileName), count) => (word, (fileName, count)) \n",
    "    }.\n",
    "    groupByKey.\n",
    "    sortByKey(ascending = true).\n",
    "    mapValues { iterable => \n",
    "        val vect = iterable.toVector.sortBy { \n",
    "            case (fileName, count) => (-count, fileName) \n",
    "        }\n",
    "        vect.mkString(\",\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparez avec les solutions d'exercice plus haut. J'ai ajouté le filtrage dans la fonction passée à `flatMap`. Mon choix réduit le nombre d'enregistrements en sortie de `flatMap` d'au plus un enregistrement par ligne en entrée, ce qui ne devrait pas avoir un impact significatif sur les performances. Le filtrage en lui-même ajoute un peu de surcharge.\n",
    "\n",
    "La façon dont Spark implémente des étapes comme `map`, `flatMap`, `filter` entraîne la même surcharge que si j'utilisais `RDD.filter`. Notez que nous pouvons aussi faire un filtrage plus tard dans la pipeline, après `groupByKey`, par exemple. Donc, quelque soit l'approche que vous implémentez, c'est probablement bien. Vous pouvez réaliser un profiling de performance des différentes approches, mais vous ne trouverez pas de différence significative à moins d'utiliser un data set d'entrée très important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vérifions si nous obtenons toujours des résultats raisonables. Maintenant je vais utiliser l'API [DataFrame](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrame) de Spark pour ses options d'affichage pratique. Les `DataFrames` font partis de [Spark SQL](http://spark.apache.org/docs/latest/sql-programming-guide.html). \n",
    "\n",
    "Tout d'abord, nous avons besoin de créer une instance de [SQLContext](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SQLContext) qui nous permettra d'accéder à ces fonctionnalités."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val sqlContext = new SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, nous convertissons le `RDD` en `DataFrame` avec `sqlContext.createDataFrame` et nous utilisons `toDF` (convertir vers un autre `DataFrame` ?) avec un nouveau nom pour chaque \"colonne\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val ii1DF = sqlContext.createDataFrame(ii1).toDF(\"word\", \"locations_counts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La _cellule magique_ `%%dataframe` founit un sympathique affichage tabulaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dataframe\n",
    "ii1DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, maintenant explorons la nouvelle implémentation. Je commence comme précédemment, en appelant `wholeTextFiles` :\n",
    "\n",
    "```scala\n",
    "val ii = sc.wholeTextFiles(shakespeare.toString).\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction que je passe à `flatMap` maintenant ressemble à ça :\n",
    "\n",
    "```scala\n",
    "flatMap { \n",
    "    case (location, contents) => \n",
    "        val words = contents.split(\"\"\"\\W+\"\"\").\n",
    "            filter(word => word.size > 0)                      // #1\n",
    "        val fileName = location.split(pathSeparator).last\n",
    "        words.map(word => ((word.toLowerCase, fileName), 1))   // #2\n",
    "}.\n",
    "```\n",
    "\n",
    "Comparez la avec la version précédente (en ignorant les améliorations pour les mots vides et les majuscules, marqués avec les commentaires \\#1 et \\#2) :\n",
    "\n",
    "```scala\n",
    "flatMap { location_contents_tuple2 => \n",
    "    val words = location_contents_tuple2._2.split(\"\"\"\\W+\"\"\")\n",
    "    val fileName = location_contents_tuple2._1.split(pathSeparator).last\n",
    "    words.map(word => ((word, fileName), 1))\n",
    "}.\n",
    "```\n",
    "\n",
    "À la place de `location_contents_tuple2` un nom de variable pour tout le tuple, J'ai écrit `case (location, contents)`. Le mot-clé `case` indique que j'effectue un _pattern match_ sur l'objet passé à la fonction. Si c'est un tuple de deux éléments (et je sais que nous serons toujours dans ce cas), alors on _extrait_ le premier élément et on l'assigne à une variable nommée `location` puis on extrait le second élément et on l'assigne à un variable nommée `contents`.\n",
    "\n",
    "Maintenant, au lieu d'accéder à la position (`location`) et au contenu (`content`) avec la syntaxe quelque peu obscure et verbeuse `location_contents_tuple2._1` et `location_contents_tuple2._2`, respectivement, j'utilise des noms éloquents, `location` et `content`. Le code devient plus concis et plus lisible.\n",
    "\n",
    "Je vais ci-dessous explorer d'avantage le pattern matching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'étape `reduceByKey` reste inchangée :\n",
    "\n",
    "```scala\n",
    "reduceByKey((count1, count2) => count1 + count2).\n",
    "```\n",
    "\n",
    "Pour plus de clarté, ceci n'est pas une expression de pattern-matching ; il n'y a pas de mot-clé `case`. C'est juste une fonction \"régulière\" qui prend deux paramètres, pour les deux éléments que je veux additionner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mon amélioration préférée est dans la ligne suivante :\n",
    "\n",
    "```scala\n",
    "map { \n",
    "    case ((word, fileName), count) => (word, (fileName, count)) \n",
    "}.\n",
    "```\n",
    "\n",
    "Comparez la avec l'obscure version précédente :\n",
    "\n",
    "```scala\n",
    "map { word_file_count_tup3 => \n",
    "    (word_file_count_tup3._1._1, (word_file_count_tup3._1._2, word_file_count_tup3._2)) \n",
    "}.\n",
    "```\n",
    "\n",
    "La nouvelle implémentation apporte de la clarté sur ce que je suis en train de faire : juste déplacer des parenthèses ! C'est tout ce que ça prend pour aller des clés `(word, fileName)` avec `count` en tant que valeur à des clés `word` et `(fileName, count)` en tant que valeur. Notez que le pattern matchimg fonctionne très bien avec les structures imbriquées, comme `((word, fileName), count)`.\n",
    "\n",
    "J'espère que vous pouvez apprécier à quel point cette expression est élégante et concise ! Notez comment je pense à la transformation suivante que j'ai besoin de faire en préparation pour le group-by final, de basculer de `((word, fileName), count)` à `(word, (fileName, count))` et _je l'ai simplement écrit exactement tel que je me l'étais représenté !_\n",
    "\n",
    "Du code comme celui-ci fait de l'écriture de code Scala Spark une expérience sublime pour moi. J'espère que c'est le cas pour vous aussi ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les deux prochaines expressions sont inchangées :\n",
    "\n",
    "```scala\n",
    "groupByKey.\n",
    "sortByKey(ascending = true).\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le `mapValues` final utilise maintenant le pattern matching pour trier le `Vector` dans chaque enregistrement :\n",
    "\n",
    "```scala\n",
    "mapValues { iterable => \n",
    "    val vect = iterable.toVector.sortBy { \n",
    "        case (fileName, count) => (-count, fileName) \n",
    "    }\n",
    "    vect.mkString(\",\")\n",
    "}\n",
    "```\n",
    "\n",
    "Comparez le à la version originale, c'est à nouveau plus facile à lire :\n",
    "\n",
    "```scala\n",
    "mapValues { iterable => \n",
    "    val vect = iterable.toVector.sortBy { file_count_tup2 => \n",
    "        (-file_count_tup2._2, file_count_tup2._1)\n",
    "    }\n",
    "    vect.mkString(\",\")\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction que j'ai passée à `sortBy` retourne un tuple utilisé pour trier, avec `-count` pour forcer le tri numérique _descendant_ (le plus grand en premier) et `fileName` pour trier dans un deuxième temps par nom de fichier, pour les décomptes identiques. Je pourrais ignorer le tri par nom de fichier et simplement retourner `-count` (sans tuple). Cependant, si vous avez besoin d'une sortie reproductible dans un système réparti comme Spark, par exemple pour la validation par tests unitaires, alors le tri secondaire par nom de fichier est utile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notre version finale : support des requêtes SQL\n",
    "Pour jouer un peu plus avec Spark, écrivons des requêtes SQL pour explorer les données résultantes.\n",
    "\n",
    "Pour faire ça, affinons d'abord la sortie. Au lieu de créer une chaîne de caractères pour la liste de couples `(location,count)`, qui est opaque pour notre schéma SQL (ie. juste une chaîne de caractères), \"dézippons\" la collection en deux tableaux, un pour les `locations` et un pour les `counts`. De cette manière, si nous accèdons au premier élément de chaque tableau, nous aurons des champs séparés qui fonctionneront bien mieux avec les requêtes Spark SQL.\n",
    "\n",
    "\"Zipper\" et \"dézipper\" fonctionnent comme pour une fermeture éclair. Si j'ai une collection de tuples, comme `List[(String, Int)]`, je convertis cette seule collection de valeurs \"zippées\" dans deux collections (dans un tuple) de valeurs simples, `(List[String], List[Int])`. Zipper est l'opération inverse.\n",
    "\n",
    "Voici notre implémentation finale, `ii1` réécrit avec ce changement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val ii = sc.wholeTextFiles(shakespeare.toString).\n",
    "    flatMap {\n",
    "        case (location, contents) => \n",
    "            val words = contents.split(\"\"\"\\W+\"\"\").\n",
    "                filter(word => word.size > 0)                      // #1\n",
    "            val fileName = location.split(pathSeparator).last\n",
    "            words.map(word => ((word.toLowerCase, fileName), 1))   // #2\n",
    "    }.\n",
    "    reduceByKey((count1, count2) => count1 + count2).\n",
    "    map { \n",
    "        case ((word, fileName), count) => (word, (fileName, count)) \n",
    "    }.\n",
    "    groupByKey.\n",
    "    sortByKey(ascending = true).\n",
    "    map {                         // Must use map now, because we'll format new records. \n",
    "      case (word, iterable) =>    // Hence, pattern match on the whole input record.\n",
    "\n",
    "        val vect = iterable.toVector.sortBy { \n",
    "            case (fileName, count) => (-count, fileName) \n",
    "        }\n",
    "\n",
    "        // Use `Vector.unzip`, which returns a single, two element tuple, where each\n",
    "        // element is a collection, one for the locations and one for the counts. \n",
    "        // I use pattern matching to extract these two collections into variables.\n",
    "        val (locations, counts) = vect.unzip  \n",
    "        \n",
    "        // Lastly, I'll compute the total count across all locations and return \n",
    "        // a new record with all four fields. The `reduceLeft` method takes a function\n",
    "        // that knows how to \"reduce\" the collection down to a final value, working \n",
    "        // from the left.\n",
    "        val totalCount = counts.reduceLeft((n1,n2) => n1+n2)\n",
    "        \n",
    "        (word, totalCount, locations, counts)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons changé l'appel `mapValues` final par un appel `map`, parce que nous allons construire entièrement de nouveaux enregistrements, pas juste de nouvelles valeurs avec les mêmes clés. D'où les enregistrements complets, des tuples de deux éléments sont passés plutôt que juste des valeurs. Ainsi nous pouvons utiliser le pattern matching sur le tuple:\n",
    "\n",
    "\n",
    "```scala\n",
    "    map {                         // Must use map now, because we'll format new records.\n",
    "      case (word, iterable) =>    // Hence, pattern match on the whole input record.\n",
    "\n",
    "        val vect = iterable.toVector.sortBy { \n",
    "            case (fileName, count) => (-count, fileName) \n",
    "        }\n",
    "```\n",
    "\n",
    "Nous avons un `Vector[(String, Int)]` de tuples de deux éléments `(fileName, count)`. Nous utilisons `Vector.unzip` pour créer un seul tuple de deux éléments (où chaque élément est maintenant une collection) : une pour la position et une pour les décomptes. Le type est `(Vector[String], Vector[Int])`.\n",
    "\n",
    "Nous pouvons aussi utiliser le pattern matching avec les assignations ! Dans ce cas, nous décomposons immédiatement le tuple :\n",
    "\n",
    "```scala\n",
    "        // I use pattern matching to extract these two collections into variables.\n",
    "        val (locations, counts) = vect.unzip  \n",
    "```\n",
    "\n",
    "Finalement, il est pratique de connaître combien de positions et de décomptes est-ce que nous avons. Nous allons donc calculer dans une nouvelle colonne pour le décompte et nous formaterons un tuple de 4 éléments en sortie.\n",
    "\n",
    "```scala\n",
    "        // Lastly, I'll compute the total count across all locations and return \n",
    "        // a new record with all four fields. The `reduceLeft` method takes a function\n",
    "        // that knows how to \"reduce\" the collection down to a final value, working \n",
    "        // from the left.\n",
    "        val totalCount = counts.reduceLeft((n1,n2) => n1+n2)\n",
    "\n",
    "        (word, totalCount, locations, counts)\n",
    "    }\n",
    "```    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK ! Maintenant, créons un [DataFrame](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrame) avec cette donnée. La méthode `toDF` retourne simplement le même `DataFrame`, mais avec les noms de colonne appropriés, au lieu de noms générés automatiquement par `createDataFrame` (eg. `_c1`, `_c2`, etc.)\n",
    "\n",
    "Mettre le `DataFrame` en cache dans la mémoire évite à Spark de recalculer `ii` avec les fichiers en entrée _chaque fois_ que j'écris une requête !\n",
    "\n",
    "Enfin, pour utiliser SQL, j'ai besoin \"d'enregistrer\" une table temporaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val iiDF = sqlContext.createDataFrame(ii).toDF(\"word\", \"total_count\", \"locations\", \"counts\")\n",
    "iiDF.cache\n",
    "iiDF.registerTempTable(\"inverted_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revoyons le schéma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iiDF.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La requête SQL ci-dessous extrait la première position par décompte pour chaque mot et aussi le décompte total sur toutes les positions pour le mot. Le dialecte Spark SQL supporte la syntaxe Hive SQL pour extraire les éléments depuis des _arrays_, des _maps_ et des _structs_ ([détails](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF#LanguageManualUDF-CollectionFunctions)). Ici, j'accède au premier élément (indice zéro) de chaque _array_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%SQL\n",
    "SELECT word, total_count, locations[0] AS top_location, counts[0] AS top_count \n",
    "FROM inverted_index "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Malheureusement, le formattage de sortie pour les \"cellules magiques\" `%%SQL` n'est pas configurable. Le `%%DataFrame` magique supporte les mises en page de largeur variable et fournit aussi plus d'options d'affichage. Voyons tout d'abord ces options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, voici la requête précédente à nouveau avec une clause `WHERE` en plus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val topLocations = sqlContext.sql(\"\"\"\n",
    "    SELECT word,  total_count, locations[0] AS top_location, counts[0] AS top_count\n",
    "    FROM inverted_index \n",
    "    WHERE word LIKE '%love%' OR word LIKE '%hate%'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et avec le `%%dataframe` _magique_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%dataframe --limit 100\n",
    "topLocations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un export du _traitement langage naturel_ (_Naturel Language Processing_ - NLP) pourrait vous expliquer que _love_, _loved_, _loves_, etc. représentent le même mot, parce qu'il s'agit de différentes conjugaisons du verbe _to love_ et _love_ est un nom aussi. De la même manière, est-ce que _gloves_ (pluriel) et _glove_ (singulier) devraient être traités différemment ?\n",
    "\n",
    "Ce que nous devons vraiment faire, c'est d'extraire le _radical_ de ces mots et de les utiliser. Les outils de NLP permettent d'extraire cette recherche du radical pour vous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il y a une méthode utile appelée `show` sur les `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topLocations.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par défaut, elle tronque les colonnes larges et n'affiche que 20 lignes. Vous pouvez modifier ces paramètres :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topLocations.show(numRows = 40, truncate = false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note :** Paramètres nommés\n",
    "\n",
    "> J'ai utilisé _paramètres nommés_ ici, `show(numRows = 40, truncate = false)`, pour la lisibilité. Ils sont optionnels en Scala, tant que vous passez les valeurs dans le même ordre que la déclaration des paramètres. Vous pouvez aussi utiliser des paramètres nommés pour écrire les arguments dans n'importe quel ordre souhaité, indépendamment de l'ordre de la déclaration. Ainsi, j'aurais pu écrire `(40, false)`, mais vous auriez légitimement demandé ce que signifie `false` dans ce contexte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercices :** \n",
    "\n",
    "Allez voir <a href=\"#ExerciseSolutions\">Appendix</a> pour les solutions aux deux premiers exercices.\n",
    "\n",
    "* `glove`, `gloves`, `whate` et `whatever` ne sont vraiment pas les `love` et `hate` que nous voulons ;) Comment pourriez-vous modifier la requête pour qu'elle soit plus spécifique ?\n",
    "* Modifiez la requête pour retourner les deux premières positions et décomptes.\n",
    "* Avant d'aller plus loin, essayez d'écrire une autre requête. Éditez la requête dans la cellule suivante :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val sql1 = sqlContext.sql(\"\"\"\n",
    "    SELECT * FROM inverted_index\n",
    "\"\"\")\n",
    "sql1.show(10, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retirer les \"Stop Words\"\n",
    "Avez-vous remarqué que l'un des enregistrements que nous avons vu ci-dessous était pour le mot \"a\" ? Ce n'est pas très utile si vous utilisez cette donnée pour de la recherche textuelle, pour de _l'extraction d'émotion_, etc. Les _stop words_ (ou _mots vides_) comme _a_, _an_, _the_, _he_, _she_, _it_, etc., peuvent aussi être retirés.\n",
    "\n",
    "Rappelez-vous de `filter` que j'ai ajouté pour retirer \"\", `word => word.size > 0`. Je pourrais le remplacer par `word => keep(word)`, où `keep` est une méthode qui réalise tous les filtrages additionnels que je veux, comme retirer les _stop words_.\n",
    "\n",
    "**Exercice :**\n",
    "\n",
    "* Implémentez la méthode `keep(word: String):Boolean` et changez la fonction `filter` pour l'utiliser. `keep` devra retourner `false` pour une petite liste de _stop words_ hard codés (faites votre propre liste ou cherchez en une). (Allez voir <a href=\"#ExerciseSolutions\">Appendix</a> pour la solution)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plus sur la syntaxe de Pattern Matching\n",
    "Nous avons seulement gratté la surface du pattern matching. Découvrons la plus en détails.\n",
    "\n",
    "Ici nous avons une autre fonction anonyme qui utilise du pattern matching qui étend la fonction précédente que nous avons passée à `flatMap` :\n",
    "\n",
    "```scala\n",
    "{\n",
    "    case (location, \"\") => \n",
    "        Array.empty[((String, String), Int)]  // Return an empty array\n",
    "    case (location, contents) => \n",
    "        val words = contents.split(\"\"\"\\W+\"\"\")\n",
    "        val fileName = location.split(pathSep).last\n",
    "        words.map(word => ((word, fileName), 1))\n",
    "}.\n",
    "```\n",
    "\n",
    "Vous pouvez avoir plusieurs clauses `case`, certains d'eux pouvant correspondre sur des valeurs littérales spécifiques (\"\" dans ce cas-là) et d'autres qui sont plus générales. La première clause case s'occupe des fichiers sans contenu. La seconde clause est la même qu'avant.\n",
    "\n",
    "Le pattern matching est _eager_. L'ordre est important car le premier match réalisé sera celui écrit en premier. Si vous inversez l'ordre ici, alors la `case (location, \"\")` ne marchera jamais et le compilateur vous affichera un avertissement \"unreachable code\".\n",
    "\n",
    "Notez que vous n'avez pas à mettre les lignes après le `=>` dans les accolades `{...}` (bien que vous pouvez le faire). Les mots-clés `=>` et `case` (ou le dernier `}`) sont suffisants pour délimiter ces blocs. Aussi pour un bloc contenant une seule expression, comme pour la première clause, vous pouvez mettre l'expression sur la même ligne après le `=>` si vous voulez (et si ça rentre).\n",
    "\n",
    "FInalement si aucun des case n'est réalisé, alors une exception [MatchError](http://www.scala-lang.org/api/current/index.html#scala.MatchError) est lancée. Dans notre cas, nous savons _toujours_ que nous avons des tuples à deux éléments, ainsi les exemples jusqu'ici sont corrects.\n",
    "\n",
    "Voici un exemple final imaginé pour illustrer ce qui est possible, en utilisant une séquence d'objets de types différents :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val stuff = Seq(1, 3.14159, 2L, 4.4F, (\"one\", 1), (404F, \"boo\"), ((11, 12), 21, 31), \"hello\")\n",
    "\n",
    "stuff.foreach {\n",
    "    case i: Int               => println(s\"Found an Int:   $i\")\n",
    "    case l: Long              => println(s\"Found a Long:   $l\")\n",
    "    case f: Float             => println(s\"Found a Float:  $f\")\n",
    "    case d: Double            => println(s\"Found a Double: $d\")\n",
    "    case (x1, x2) => \n",
    "        println(s\"Found a two-element tuple with elements of arbitrary type: ($x1, $x2)\")\n",
    "    case ((x1a, x1b), _, x3) => \n",
    "        println(s\"Found a three-element tuple with 1st and 3th elements: ($x1a, $x1b) and $x3\")\n",
    "    case default              => println(s\"Found something else: $default\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelques notes.\n",
    "* Un littéral comme `1` est inféré en tant que `Int`, tandis que `3.14159` est inféré en tant que `Double`. Ajoutez `L` ou `F`, pour inférer respectivement `Long` ou `Float` à la place.\n",
    "* Notez comment nous avons mélangé la vérification de types spécifiques, par exemple `i: Int` avec des types moins typés comme `(x1, x2)`, qui attend un tuple à deux éléments. Tous les mots `i`, `l`, `f`, `d`, `x1`, `x2`, `x3`, et `default` sont des noms de variables arbitraires. `default` n'est pas un mot-clé, mais un choix arbitraire pour le nom d'une variable. Nous pourrions utiliser ce que nous voulons.\n",
    "* La dernière clause `default` définit une variable sans information de type. Ainsi, cela match _tout_, ce qui est la raison pour laquelle cette clause doit apparaître en dernier. C'est la syntaxe utilisée quand vous n'êtes pas sûr du type des choses que vous être en train de faire correspondre et que vous voulez éviter une possible [MatchError](http://www.scala-lang.org/api/current/index.html#scala.MatchError).\n",
    "* Si vous voulez matcher quelque chose qui _existe_, mais dont vous n'avez pas besoin de mettre dans une variable, alors utilisez `_` comme dans l'exemple du tuple à trois éléments.\n",
    "* L'exemple du tuple à trois éléments montre que \"l'imbriquage\" arbitraire d'expressions est supporté, où le premier élément attendu est un tuple de deux éléments.\n",
    "\n",
    "Toutes les fonctions anonymes que nous avons vues qui utilisent ces clauses de pattern matching ont ce format :\n",
    "\n",
    "```scala\n",
    "{ \n",
    "    case firstCase => ...\n",
    "    case secondCase => ...\n",
    "    ... \n",
    "}```\n",
    "\n",
    "Ce format a un nom spécial. Il est appelé _fonction partielle_. Tout ce que cela veut dire est seulement que nous \"promettons\" d'accepter des arguments qui correspondent à au moins une de nos clauses `case` et non pas à n'importe quelle entrée.\n",
    "\n",
    "L'autre type de fonction anonyme que nous avons vu est la _fonction totale_.\n",
    "\n",
    "Souvenez-vous lorsqu'il a été dit que les fonctions totales peuvent utiliser soit `(...)` ou `{...}` autour d'elles, en fonction du \"look\" que vous voulez leur donner. Pour les _fonction partielles_, vous _devez_ utiliser `{...}`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Souvenez-vous que nous avons utilisé du pattern matching avec affectation :\n",
    "\n",
    "```scala\n",
    "val (locations, counts) = vect.unzip  \n",
    "```\n",
    "[Vector.unzip](http://www.scala-lang.org/api/current/#scala.collection.immutable.Vector) retourne un tuple de deux éléments, où chaque élément est une collection. Nous matchons sur ce tuple et affectons chaque pièce à une variable. Voici un autre exemple imaginé avec un tuple comportant des éléments imbriqués."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val (a, (b, (c1, c2), d)) = (\"A\", (\"B\", (\"C1\", \"C2\"), \"D\"))\n",
    "println(s\" $a, $b, $c1, $c2, $d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essayez d'ajouter un élément `\"E\"` au côté droit du tuple sans changer le côté gauche. Que se passe-t-il ?\n",
    "Essayez d'enlever le `\"D\"` et le `\"E\"`. Que se passe-t-il maintenant ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous reviendrons à un dernier exemple de pattern matching quand nous discuterons des _case classes_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Le modèle Objet de Scala\n",
    "Scala est un langage de prommation _hybride_, orienté objet et fonctionnel. La philosophie de Scala est que nous exploitons l'orienté objet pour l'encapsulation des détails, c-à-d _la modularité_, mais nous utilisons la programmation fonctionnelle pour sa précision logique lorsque nous implémentons ces détails. La majorité de ce que nous avons vu jusque-là relève plus de la programmation fonctionnelle. Une grande partie de la manipulation et l'analyse des données est en fait des mathématiques. La programmation fonctionnelle essaye de rester le plus proche possible du fonctionnement des fonctions et valeurs des mathématiques.\n",
    "\n",
    "Cependant en codant des programmes Spark non-triviaux, il est parfois utile d'exploiter les fonctionnalités orientés objet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes vs. Instances\n",
    "Scala utilise la même différenciation entre les classes et les instances que vous trouverez en Java. Les classes sont comme des _templates_ utilisées pour créer des instances.\n",
    "\n",
    "Nous avons parlé des _types_ de diverses choses, comme `word` qui est un `String` et `totalCount` qui est un `Int`. Une classe définit un _type_ aussi.\n",
    "\n",
    "Voici un exemple d'une classe que nous pourrions utiliser pour représenter l'index inversé des lignes que nous venons de créer :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IIRecord1(\n",
    "    word: String, \n",
    "    total_count: Int, \n",
    "    locations: Array[String], \n",
    "    counts: Array[Int]) {\n",
    "    \n",
    "    /** CSV formatted string, but use [a,b,c] for the arrays */\n",
    "    override def toString: String = {\n",
    "        val locStr = locations.mkString(\"[\", \",\", \"]\")  // i.e., \"[a,b,c]\"\n",
    "        val cntStr = counts.mkString(\"[\", \",\", \"]\")  // i.e., \"[1,2,3]\"\n",
    "        s\"$word,$total_count,$locStr,$cntStr\"\n",
    "    }\n",
    "}\n",
    "\n",
    "new IIRecord1(\"hello\", 3, Array(\"one\", \"two\"), Array(1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quand nous définissons une classe, la liste des arguments après le nom de la classe est la liste des arguments pour le _constructeur primaire_. Vous pouvez définir des constructeurs secondaires aussi mais ce n'est pas très commun de le faire pour des raisons que nous allons voir sous peu.\n",
    "\n",
    "Notez que quand vous overridez une méthode qui est définie dans une classe parent comme `Object.toString` de Java, Scala a besoin que vous ajoutiez le mot-clé `override`.\n",
    "\n",
    "Nous avons créé une _instance_ de `IIRecord1` en utilisant `new` comme en Java.\n",
    "\n",
    "Finalement, remarque supplémentaire, nous avons utilisé jusqu'ici beaucoup de `Int` (entiers) pour les différents counts, mais en réalité dans le \"big data\" nous devrions probablement utiliser des `Long`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objets\n",
    "\n",
    "Nous avons intentionnellement utilisé le mot _instance_ pour les choses que nous créons à partir des classes. Cela est dû au fait que Scala possède la fonctionnalité de créer seulement une instance de la classe avec [Singleton Design Pattern](https://en.wikipedia.org/wiki/Singleton_pattern). Dans ce cas, il faudra utiliser le mot-clé `object`.\n",
    "\n",
    "En Java par exemple vous devez définir une classe avec la méthode `static void main(String[] arguments)` comme point d'entrée de votre programme. En Scala vous pouvez utiliser un `object` qui contiendra le `main` comme montré dans l'exemple suivant :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object MySparkJob {\n",
    "\n",
    "    val greeting = \"Hello Spark!\"\n",
    "    \n",
    "    def main(arguments: Array[String]) = {\n",
    "        println(greeting)\n",
    "        \n",
    "        // Create your SparkContext, etc., etc.\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme pour les classes, le nom de l'objet peut être ce que vous voulez. Il n'y a pas de mot-clé `static` en Scala. A la place d'ajouter des méthodes et champs `static` dans les classes comme en Java, vous pouvez les mettre dans des objets comme montré ici.\n",
    "\n",
    "> **NOTE:** Parce que le compilateur Scala doit générer du byte code valide pour la JVM, ces définitions sont converties en un équivalent Java qui contient des définitions static."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Classes\n",
    "Les tuples sont utiles pour représenter des lignes et les décomposer avec du pattern matching. Cependant, il serait agréable si les champs étaient _nommés_ ainsi que _typés_. Une bonne utilisation pour une classe comme `IIRecord1` est de représenter cette structure et de nous donner des champs nommés. Affinons donc la définition de la classe afin d'exploiter des fonctionnalités supplémentaires très utile en Scala.\n",
    "\n",
    "Considérons la définition suivante d'une _case class_ qui représente notre type d'enregistrement final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case class IIRecord(\n",
    "    word: String, \n",
    "    total_count: Int = 0, \n",
    "    locations: Array[String] = Array.empty, \n",
    "    counts: Array[Int] = Array.empty) {\n",
    "\n",
    "    /** \n",
    "     * Different than our CSV output above, but see toCSV.\n",
    "     * Array.toString is useless, so format these ourselves. \n",
    "     */\n",
    "    override def toString: String = \n",
    "        s\"\"\"IIRecord($word, $total_count, $locStr, $cntStr)\"\"\"\n",
    "    \n",
    "    /** CSV-formatted string, but use [a,b,c] for the arrays */\n",
    "    def toCSV: String = \n",
    "        s\"$word,$total_count,$locStr,$cntStr\"\n",
    "        \n",
    "    /** Return a JSON-formatted string for the instance. */\n",
    "    def toJSONString: String = \n",
    "        s\"\"\"{\n",
    "        |  \"word\":        \"$word\", \n",
    "        |  \"total_count\": $total_count, \n",
    "        |  \"locations\":   ${toJSONArrayString(locations)},\n",
    "        |  \"counts\"       ${toArrayString(counts, \", \")}\n",
    "        |}\n",
    "        |\"\"\".stripMargin\n",
    "\n",
    "    private def locStr = toArrayString(locations)\n",
    "    private def cntStr = toArrayString(counts)\n",
    "\n",
    "    // \"[_]\" means we don't care what type of elements; we're just\n",
    "    // calling toString on them!\n",
    "    private def toArrayString(array: Array[_], delim: String = \",\"): String = \n",
    "        array.mkString(\"[\", delim, \"]\")  // i.e., \"[a,b,c]\"\n",
    "\n",
    "    private def toJSONArrayString(array: Array[String]): String =\n",
    "        toArrayString(array.map(quote), \", \")\n",
    "    \n",
    "    private def quote(word: String): String = \"\\\"\" + word + \"\\\"\"  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons dit que définir des constructeurs secondaires n'était pas très commun. En partie c'est parce que nous avons utilisé une fonctionnalité pratique, la capacité de définir des valeurs par défaut pour les arguments de méthodes, en incluant le constructeur primaire. La possibilité d'utiliser des valeurs par défaut signifie que nous pouvons créer des instances sans avoir à donner tous les arguments explicitement tant qu'il y a une valeur par défaut définie et de même pour l'appel des méthodes. Considérez ces deux exemples suivants :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val hello = new IIRecord(\"hello\")\n",
    "val world = new IIRecord(\"world!\", 3, Array(\"one\", \"two\"), Array(1, 2))\n",
    "\n",
    "println(\"\\n`toString` output:\")\n",
    "println(hello)\n",
    "println(world)\n",
    "\n",
    "println(\"\\n`toJSONString` output:\")\n",
    "println(hello.toJSONString)\n",
    "println(world.toJSONString)\n",
    "\n",
    "println(\"\\n`toCSV` output:\")\n",
    "println(hello.toCSV)\n",
    "println(world.toCSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons ajouté `toJSONString` pour illustrer l'ajout de méthodes _publiques_, la visibilité par défaut et les méthodes _privées_ à la définition d'une classe. Quand il n'y a pas de méthodes ou de variables sans champs à définir, nous pouvons omettre le corps entièrement, pas de `{}` nécessaire.\n",
    "\n",
    "Souvenez-vous du mot-clé `override` qui est utilisé lorsque nous redéfinissons `toString`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qu'en est-il du mot-clé `case`? Ce mot-clé dit au compilateur de faire plusieurs choses utiles pour nous, éliminant ainsi pas mal de code superflu que nous pourrions trouver dans d'autres langages, comme Java :\n",
    "\n",
    "1. Traitez chaque argument de constructeur comme un champ privé immutable (`val`) de l'instance.\n",
    "1. Générez une méthode de lecteur publique pour le champ avec le même nom (par exemple `word`).\n",
    "1. Générez des implémentations _correctes_ des méthodes `equals` et `hashCode`, souvent implémentées incorrectement, ainsi que la méthode par défaut `toString`. Vous pouvez utiliser vos propres définitions en les ajoutant explicitement dans le corps. Nous avons fait cela pour `toString` afin de formatter les tableaux de manière plus agréable que la méthode par défaut `Array[_].toString`.\n",
    "1. Générez un `object IIRecord`, donc avec le même nom. L'objet sera appelé _objet compagnon_.\n",
    "1. Générez une méthode \"factory\" dans l'objet compagnon qui prend la même liste d'argument et qui instancie une instance.\n",
    "1. Générez des méthodes helper dans l'objet compagnon qui supportent le pattern matching.\n",
    "\n",
    "Les points 1 et 2 font que chaque argument se comporte comme s'il était public, en lecture seule des champs de l'instance mais ils sont implémentés comme décrit.\n",
    "\n",
    "Le troisième point est important pour comprendre le comportement standard. Les instances de case class sont souvent utilisées comme clés dans [Maps](http://www.scala-lang.org/api/current/index.html#scala.collection.Map) et [Sets](http://www.scala-lang.org/api/current/index.html#scala.collection.Set), les RDD Spark RDD et méthodes de DataFrame, etc. En fait vous devriez utiliser _uniquement_ vos case classes ou les types inclus dans Scala possédant des bonnes définitions des méthodes `hashCode` et `equals` (comme `Int` et les autres types numériques, `String`, tuples, etc.) comme clés.\n",
    "\n",
    "Pour le quatrième point, _l'objet compagnon_ est généré automatiquement par le compilateur. Le compilateur ajoute une méthode \"factory\" mentioné dans le cinquième point, et des méthodes qui supportent le pattern matching, expliqué dans le sixième point. Vous pouvez définir ces méthodes explicitement par vous même ainsi que les champs pour retenir un état. Le compilateur insérera toujours ces méthodes. Cependant jetez un oeil à <a href=\"#Ambiguities\">Ambiguities with Companion Objects</a>. La conclusion est que vous ne devriez pas définir des case classes dans un notebook comme celui ci avec des méthodes en plus dans l'objet compagnon à cause de problèmes de parsing que cela peut engendrer.\n",
    "\n",
    "Le cinquième point signifie que `new` est rarement utilisé quand des instances sont créées. Cependant, les deux lignes suivantes sont équivalentes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val hello1 = new IIRecord(\"hello1\")\n",
    "val hello2 = IIRecord(\"hello2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qu'arrive-t-il en réalité dans le second cas sans `new` ? La méthode \"factory\" est en fait appelée `apply`. En Scala quand vous mettez une liste d'arguments après n'importe quelle _instance_, en incluant les `objets` comme dans le cas de `hello2`, une méthode `apply` va être recherchée par Scala pour être appelée. Les arguments devront correspondre à la liste des arguments à apply (nombre d'arguments, types des arguments, prise en compte des valeurs par défaut des arguments, etc.). Ainsi en réalité la déclaration de `hello2` est :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val hello2b = IIRecord.apply(\"hello2b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez exploiter cette fonctionnalité dans vos autres classes aussi. Nous avons parlé du mot \"stemming\" en haut. Supposons que vous écriviez une libraire de stemming et déclariez un objet comme point d'entrée. Ici nous allons faire quelque chose de simple; nous assumons qu'un \"s\" à la fin du mot signifie que le mot est au pluriel et nous allons l'enlever (c'est une mauvaise présomption...) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object stem {\n",
    "    def apply(word: String): String = word.replaceFirst(\"s$\", \"\") // insert real implementation!\n",
    "}\n",
    "\n",
    "println(stem(\"dog\"))\n",
    "println(stem(\"dogs\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notez que l'appel à `stem` ressemble à un appel de fonction ou de méthode. Scala permet aux objets et aux classes d'avoir un nom qui commence avec une lettre minuscule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalement, le 6ème point signifie que nous pouvons utiliser nos case classes spécifiques dans des expressions de pattern matching. Nous n'allons pas regarder les méthodes implémentées dans l'objet compagnon et comment ils supportent le pattern matching. Nous allons seulement utiliser de la \"magie\" dans l'exemple suivant qui \"parcourt\" nos instances définies précedemment, `hello` et `world`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Seq(hello, world).map {\n",
    "    case IIRecord(word, 0, _, _) => s\"$word with no occurrences.\"\n",
    "    case IIRecord(word, cnt, locs, cnts) => \n",
    "        s\"$word occurs $cnt times: ${locs.zip(cnts).mkString(\", \")}\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La première clause case ignore les locations et les counts car nous savons que ce seront des arrays vides si le count total est 0 !\n",
    "\n",
    "La seconde clause case utilise la méthode `zip` pour mettre les locations et les counts ensemble. Souvenez-vous que nous avons utilisé `unzip` pour créer des collections séparées."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets et DataFrames\n",
    "Jusque là, nous avons principalement utilisé l'API RDD de Spark. Les case classes sont souvent utilisées pour représenter des \"schema\" de records quand on utilise des RDD. Elles le sont également avec un nouveau type, [Dataset[T]](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset), qui est similaire à `RDD[T]`, où `T` représente un type de records.\n",
    "\n",
    "Les [DataFrames](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrame) possèdent un problème qui est que les champs sont non-typés jusqu'à leur accès. Les `Datasets` rétablissent l'assurance du type des `RDD` en utilisant une case class comme la définition d'un schema.\n",
    "\n",
    "Les `Datasets` ont été introduits avec Spark 1.6.0, mais elles étaient quelque peu incomplètes dans les versions 1.6.X. Dans Spark 2.0.0, `Dataset` devient la classe \"parente\" de `DataFrame`. Cela signifie qu'il est recommandé d'utiliser la plus grande assurance de type de `Dataset`, mais vous pouvez toujours utiliser `DataFrame` si vous le souhaitez. Maintenant, `DataFrame` est l'équivalent de `Dataset[Row]`, où [Row](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Row) est la représentation grossière des row et columns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essayons cela. Mais d'abord, nous devons importer du code venant de SparkSQL. Scala vous laisse importer du code n'importe où, alors qu'avec Java les imports se font au début du fichier source. Scala vous laisse aussi importer des membres des instances, pas juste des imports statiques supportés par Java.\n",
    "\n",
    "La prochaine cellule importe donc des \"implicits\" de l'instance [SQLContext](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SQLContext) qui est déjà dans le scope. Malheureusement, à cause d'une ambiguïté du scope impliquant les notebooks et l'interpréteur Scala, nous devons assigner `sqlContext` à une nouvelle variable et _ensuite_ importer avec celle-ci :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val sqlc = sqlContext\n",
    "import sqlc.implicits._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons expliquer ce que sont les `implicits` <a href=\"#implicits\">plus tard</a>. Pour le moment, ils nous \"permettent\" d'appeler la méthode `as` sur notre `DataFrame` `iiDF`, ce qui la convertit en `Dataset[IIRecord]`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val iiDS = iiDF.as[IIRecord]\n",
    "iiDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iiDS.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Scala pour Spark avancé\"\n",
    "Nous avons déjà abordé beaucoup de choses dans ce notebook, en se concentrant sur les sujets essentiels que vous devez savoir de Scala pour un usage quotidien. Appelons ces choses \"Scala pour Spark débutant\".\n",
    "\n",
    "Dès maintenant, nous vous conseillons de créer un nouveau notebook et de jouer avec Spark en utilisant ce que vous avez appris jusqu'à maintenant et de revenir ici si vous tombez sur quelque chose qui n'a pas été encore abordé jusque là. Il y a une chance pour que ce que vous devez savoir se trouve dans la partie qui suit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tout importer dans un package\n",
    "En Java, `import foo.bar.*;` signifie que nous allons tout importer du package `bar`.\n",
    "\n",
    "En Scala, `*` est en fait un nom de méthode légal; par exemple lorsque vous définissez multiplication pour un type numérique crée comme `Matrix`. Ainsi un import avec une `*` serait ambigu. Scala utilise donc `_` au lieu de `*`, `import foo.bar._` (avec le point-virgule qui est inféré)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "À quoi ressemblerait la définition de la méthode `*` ? Elle ressemblerait à quelque chose comme :\n",
    "\n",
    "```scala\n",
    "case class Matrix(rows: Array[Array[Double]]) {  // Each row is an Array[Double]\n",
    "\n",
    "    /** Multiple this matrix by another. */\n",
    "    def *(other: Matrix): Matrix = ...\n",
    "    \n",
    "    /** Add this matrix by another. */\n",
    "    def +(other: Matrix): Matrix = ...\n",
    "    \n",
    "    ...\n",
    "}\n",
    "\n",
    "val row1: Array[Array[Double]] = ...\n",
    "val row2: Array[Array[Double]] = ...\n",
    "val m1 = Matrix(rows1)\n",
    "val m2 = Matrix(rows2)\n",
    "val m1_times_m2 = m1 * m2\n",
    "val m1_plus_m2 = m1 + m2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Syntaxe des opérateurs\n",
    "\n",
    "Une seconde ! Que signifie `m1 * m2` ? Est-ce que cela ne devrait pas être `m1.*(m2)` ? Cela serait vraiment pratique pour utiliser la \"syntaxe opérateur\", plus précisément appelé la _notation d'opérateur infixe_ pour des méthodes diverses comme `*` et `+` ici. Le parser Scala supporte cela avec un simple relâchement des règles; quand une méthode prend en paramètre un argument unique, alors vous pouvez omettre le point `.` et les parenthèses `(...)`. Ces deux lignes sont équivalentes :\n",
    "\n",
    "```scala\n",
    "val m1_times_m2 = m1.*(m2)\n",
    "val m1_times_m2 = m1 * m2\n",
    "```\n",
    "\n",
    "Cela peut amener à du code confus, particulièrement pour les débutants en Scala, donc utilisez la avec précaution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traits\n",
    "Les _traits_ sont similaires aux _interfaces_ Java 8, utilisés pour définir des abstractions, mais avec la possibilité de fournir des implémentations par \"défaut\" des méthodes déclarées. Contrairement aux interfaces de Java 8, les traits peuvent aussi avoir des champs qui représentent \"l'état\" de l'information à propos des instances. Il y a une ligne fine entre les _traits_ et les _classes abstraites_, là encore avec des méthodes membres ou champs qui ne sont pas définis.\n",
    "Dans les deux cas, les sous-types de trait et/ou d'une classe abstraite doivent définir tous les membres non définis si vous voulez pouvoir construire des instances de celles-ci.\n",
    "\n",
    "Pourquoi avoir des traits et des classes abstraites ? C'est parce que Java ne permet _qu'un seul héritage_; il ne peut y avoir qu'un type _parent_, qui est normalement utilisé là où vous voudriez utiliser une classe abstraite, mais Scala vous permet de \"mixer\" des traits (ou d'utiliser un trait comme la classe parente - assez confus). Un bon exemple de trait \"mélangé\" est celui qui implémente le logging. Tous les \"services\" peuvent ajouter le trait de logging pour avoir un accès \"instantané\" à cette fonctionnalité réutilisable. Cela ressemblerait au code suivant :\n",
    "\n",
    "```scala\n",
    "// Assume severity `Level` and `Logger` types defined elsewhere...\n",
    "trait Logging {\n",
    "\n",
    "    def log(level: Level, message: String): Unit = logger.log(level, message)\n",
    "    \n",
    "    private logger: Logger = ...\n",
    "}\n",
    "\n",
    "abstract class Service {\n",
    "    def run(): Unit   // No body, so abstract!\n",
    "}\n",
    "\n",
    "class MyService extends Service with Logging {\n",
    "    def run(): Unit = {\n",
    "        log(INFO, \"Staring MyService...\")\n",
    "        ...\n",
    "        log(INFO, \"Finished MyService\")\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "`Unit` en Scala est l'équivalent Java de `void`. C'est un vrai type avec une valeur de retour unique contrairement à `void`, et nous l'utilisons dans le sens \"rien d'utile va être retourné\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intervalles\n",
    "Et si vous vouliez des nombres entre une valeur de départ et de fin ? Utilisez un [Range](http://www.scala-lang.org/api/current/index.html#scala.collection.immutable.Range), qui possède une syntaxe littérale `1 until 100`, `2 to 200 by 3`. \n",
    "\n",
    "`Range` comprend toujours la borne inférieure. Utiliser `to` dans `Range` fait en sorte que la borne supérieure soit _incluse_. Utiliser `until` le rend _exclusif_. Utilisez `by` pour spécifier un delta, qui par défaut est de `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 until 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 to 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 to 10 by 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quand vous avez besoin d'un jeu de données pour jouer avec Spark, les intervalles peuvent être pratiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val rdd7 = sc.parallelize(1 to 50).\n",
    "    map(i => (i, i%7)).\n",
    "    groupBy{ case (i, seven) => seven }.\n",
    "    sortByKey()\n",
    "rdd7.take(7).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[SparkContext](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext) a aussi une méthode `range` qui fait la même chose que `sc.parallelize(some_range)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpréteur Scala (REPL) vs. Notebooks vs. Compilateur Scala\n",
    "<a name=\"REPL\"></a>\n",
    "Ce notebook est en train d'utiliser un interpréteur Scala, le _REPL_ (\"read, eval, print, loop\") pour parser le code Scala. La distribution Spark est fournie avec un script `spark-shell` qui vous permet aussi d'utiliser l'interpréteur depuis une ligne de commande mais sans la belle interface utilisateur d'un notebook.\n",
    "\n",
    "Si vous utilisez le `spark-shell`, il y a quelques différences de comportement que vous devriez savoir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utiliser le mode :paste\n",
    "Par défaut, l'interpréteur Scala traite _chaque ligne_ que vous entrez de manière séparée. Cela peut engendrer des surprises comparé à la manière de fonctionner du _compilateur_ Scala, où le code sera traité dans le même fichier et le même contexte.\n",
    "\n",
    "Par exemple, dans le code suivant, l'expression continue sur la seconde ligne, elle est comprise par le compilateur mais pas par l'interpréteur.\n",
    "\n",
    "```scala\n",
    "(1 to 100)\n",
    ".map(i => i*i)\n",
    "```\n",
    "\n",
    "L'interpréteur pense qu'il a fini d'analyser l'expression quand il arrive à une nouvelle ligne après le littéral [Range](http://www.scala-lang.org/api/current/index.html#scala.collection.immutable.Range), `1 to 100`. L'interpréteur lance ensuite une erreur sur le `.` de la ligne suivante. En parallèle, le compilateur continue de compiler et ignore les nouvelles lignes dans ce cas-là.\n",
    "\n",
    "Ce notebook fait la même chose que l'interpréteur mais dans quelques cas, les notebooks vont utiliser une commande, `:paste` qui indique au parser d'analyser toutes les lignes qui suivent ensemble, comme ce que le compilateur ferait jusqu'à \"end of input\" que vous pouvez indiquer avec `CTRL-D`.\n",
    "\n",
    "Vous ne pouvez pas l'expérimenter avec ce notebook, mais votre session ressemblerait à quelque chose comme ça :\n",
    "\n",
    "```scala\n",
    "scala> :paste\n",
    "// Entering paste mode (ctrl-D to finish)\n",
    "\n",
    "(1 to 10)\n",
    ".map(i => i*i)\n",
    "<CTRL-D>\n",
    "\n",
    "// Exiting paste mode, now interpreting.\n",
    "\n",
    "res0: scala.collection.immutable.IndexedSeq[Int] = Vector(1, 4, 9, 16, 25, 36, 49, 64, 81, 100)\n",
    "\n",
    "scala>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ambiguïtés avec les objets compagnons\n",
    "<a name=\"Ambiguïtés\"></a>\n",
    "Lors de la rédaction de ce notebook, nous _voulions_ démontrer en utilisant l'objet compagnon `IIRecord` la définition d'une méthode explicitement, mais cela mène à une ambiguïté plus tard dans ce notebook si vous voulez utiliser cette méthode. Le notebook confondra la case class et l'objet.\n",
    "\n",
    "C'est regrettable mais il est vrai que lorsque vous commencez à définir des case classes plus complexes, avec plus que des méthodes triviales et des additions explicites à l'objet compagnon par défaut, vous devriez vraiment définir ces types en dehors du notebook dans une librairie compilée que vous pourrez utiliser dans ce notebook.\n",
    "\n",
    "Les détails sont en dehors de notre cadre ici, mais vous pouvez créer un projet avec votre code Scala et le \"builder\" avec votre outil de build favori. [SBT](http://www.scala-sbt.org/) est un choix populaire pour Scala, mais Maven, Gradle, etc peuvent être utilisés.\n",
    "\n",
    "Vous devez générer un fichier _jar_ avec les artefacts compilés, puis quand vous lancez votre `spark-shell`, soumettez votre job Spark avec un `spark-submit` ou utilisez un environnement de notebook comme celui-ci en spécifiant les jars pour les inclure. Pour `spark-shell` et `spark-submit`, invoquez le avec l'option `--jars myproject.jar`. Pour Toree with Jupyter, regardez la discussion sur [FAQ page](https://toree.incubator.apache.org/documentation/user/faq.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hiérarchie de type Scala\n",
    "La hiérarchie de type dans Scala est similaire à celle de Java mais avec des différences notables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Scala Type Hierarchy](http://docs.scala-lang.org/resources/images/classhierarchy.img_assist_custom.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En Java, tous les _références de types_ proviennent de [java.lang.Object](https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html). Le nom _référence de type_ reflète le fait que les instances de tous ces types sont alloués sur le _heap_ et que les variables du programme sont des références à ces heap.\n",
    "\n",
    "Les types primitifs, `int`, `long`, etc ne sont pas considérés comme faisant partie de la hiérarchie de type et sont traités spécialement. C'est en partie pour optimiser les performances car les instances de ces types tiennent dans le registre du processeur et les valeurs sont envoyées dans la pile d'éxécution. Cependant, ces valeurs sont types \"boxed\", par exemple `Interger`, `Long`, etc et font parti de l'hiérarchie de type que vous êtes obligé d'utiliser avec les collections Java par exemple (avec l'exception des tableaux).\n",
    "\n",
    "Scala traite les primitives au niveau du code comme des types de références. Vous n'utilisez pas `new Int(100)` par exemple, mais vous pouvez appeler des méthodes sur des instances de `Int`. Le code généré utilise en général des primitives de la JVM optimisés.\n",
    "\n",
    "Ainsi, l'hiérarchie de type Scala définit un type [Any](http://www.scala-lang.org/api/current/#scala.Any) qui est un parent des types de références et des types de valeurs (pour les primitives). Chacun de ces sous-hiérarchies ont des types parent, [AnyRef](http://www.scala-lang.org/api/current/#scala.AnyRef) qui est la même chose que [java.lang.Object](https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html), et [AnyVal](http://www.scala-lang.org/api/current/#scala.AnyVal) est le parent des types de valeur.\n",
    "\n",
    "Finalement, pour plus de \"solidité\", le système de type Scala définit un type réel qui représente [Null](http://www.scala-lang.org/api/current/#scala.Null) et [Nothing](http://www.scala-lang.org/api/current/#scala.Nothing). En définissant `Null` comme étant le sous-type de tous les types de références `AnyRefs` (mais non pas `AnyVals`), Scala supporte au niveau du typage la pratique regrettable d'utiliser `null` pour une valeur de référence.\n",
    "\n",
    "Cependant, `null` n'est pas autorisé pour un `AnyVal`, donc le vrai \"type au fond\" de la hiérarchie est `Nothing`. Nous allons expliquer l'utilité de cela dans la prochaine section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"TryOptionNull\"></a>\n",
    "### Try vs. Option vs. null\n",
    "\n",
    "Rappelez-vous de la signature de notre méthode `curl` au début de ce notebook.\n",
    "\n",
    "```scala\n",
    "def curl(sourceURLString: String, targetDirectoryString: String): File = ...\n",
    "```\n",
    "\n",
    "Elle retourne un `File` quand tout va bien, mais elle peut aussi renvoyer une exception. Une alternative est de retourner un `Try[File]` où [Try](http://www.scala-lang.org/api/current/index.html#scala.util.Try) encapsule les deux cas dans la valeur de retour, comme nous le verrons plus tard. Nous verrons également une autre alternative, [Option](http://www.scala-lang.org/api/current/index.html#scala.Option)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supposons maintenant que nous déclarions `curl` pour qu'elle retourne [util.Try[T]](http://www.scala-lang.org/api/current/index.html#scala.util.Try), où `T` est un `java.io.File`. La seule chose à changer dans le corps de la méthode est de tout simplement ajouter un `Try` avant la parenthèse ouvrante :\n",
    "\n",
    "```\n",
    "def curl(sourceURLString: String, targetDirectoryString: String): Try[File] = Try {...}\n",
    "```\n",
    "\n",
    "Maintenant avec cette signature, le lecteur sait que la méthode peut potentiellement échouer. Si un appel échoue, une exception adéquate sera retournée enveloppée dans une sous-classe de `Try` appelée [util.Failure[T]](http://www.scala-lang.org/api/current/index.html#scala.util.Failure). Toutefois, si `curl` réussi, le `File` sera retourné enveloppé dans une autre sous-classe de `Try` : [util.Success[T]](http://www.scala-lang.org/api/current/index.html#scala.util.Success).\n",
    "\n",
    "Grâce à l'assurance de type de Scala, l'appelant de `curl` doit déterminer quel résultat est retourné et le gérer de manière adéquate. Cela signifie que l'appelant doit déterminer si `Success` ou `Failure` a été retourné, et le traiter de manière adéquate.\n",
    "\n",
    "Scala ne possède pas de déclarations d'exceptions comme Java. Donc en regardant la version orginiale de notre signature, il n'y a pas de manière évidente de voir si la méthode renvoit une exception _ou_ un `null` en cas d'échec.\n",
    "\n",
    "```scala\n",
    "def curl(sourceURLString: String, targetDirectoryString: String): File = {...}\n",
    "```\n",
    "\n",
    "Si nous choisissons d'attraper les exceptions en interne et retournons `null`, l'appelant doit se rappeler de vérifier le cas pour `null`. Sinon, le célèbre [NullPointerException](https://docs.oracle.com/javase/8/docs/api/java/lang/NullPointerException.html) peut apparaître occasionellement si l'appelant suppose qu'une valeur non-`null` est retournée. Donc utiliser `Try[T]` nous protège de cette faille. _Cela aide l'utilisateur à faire la bonne chose !_\n",
    "\n",
    "Utiliser `Try` au lieu de simplement renvoyer une exception signifie également que `curl` retourne toujours, \"normalement\", quelque chose, pour que l'appelant puisse avoir un contrôle total sur la stack d'appel et pour ne pas qu'une logique spéciale d'exception-catching soit requise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelles sont les sous-classes valides de `Try` ? Il n'y en a que deux : `Success` et `Failure`. Ce serait une erreur d'autoriser les utilisateurs de définir d'autres sous-classes comme `PeutEchouerMaisQuiSait`, car les utilisateurs de `Try` voudront toujours savoir qu'il n'y a que deux possibilités en pattern matching. Scala ajoute un mot-clé pour renforcer ce comportement logique. `Try` se déclare en fait de la façon suivante :\n",
    "\n",
    "```scala\n",
    "sealed abstract class Try[+T] extends AnyRef\n",
    "```\n",
    "\n",
    "(`AnyRef` est identique au supertype `Object` de Java.) Le mot-clé `sealed` fait qu'on ne peut déclarer _aucune_ sous-classe de `Try`; _sauf_ si c'est dans le même fichier source que cette déclaration (qui est dans la bibliothèque que l'auteur a écrit). Ainsi, les utilisateurs de `Try` ne peuvent pas déclarer leur propre sous-classes, ce qui altèrerait la structure logique de cette hiérarchie de type et autres code d'utilisateurs qui seraient dépendants de cette structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et si nous avons une situtation dans laquelle il est illogique d'inclure une exception mais que nous voulons le même traitement logique ? C'est là où vient [Option[T]](http://www.scala-lang.org/api/current/index.html#scala.Option).\n",
    "\n",
    "`Option` est similaire à `Try`, c'est un `sealed` abstract type avec deux possibles sous-types :\n",
    "\n",
    "* [Some[T]](http://www.scala-lang.org/api/current/index.html#scala.None): J'ai une instance `T` pour votre résultat, à l'intérieur de `Some[T]`\n",
    "* [None](http://www.scala-lang.org/api/current/index.html#scala.None): Je n'ai pas de valeur pour votre résultat, désolé.\n",
    "\n",
    "Notez qu'un hash map est un bon exemple où nous avons ou pas une valeur pour une clé donnée. Par conséquent, pour l'abstraction  [Map[K,V]](http://www.scala-lang.org/api/current/index.html#scala.collection.Map) de Scala où `K` est le type de la clé et `V` est le type de la valeur, la méthode `get` a cette signature :\n",
    "\n",
    "```scala\n",
    "def get(key: K): Option[V]\n",
    "```\n",
    "\n",
    "Encore une fois, vous voyez avec cette signature que vous allez avoir ou non une instance de valeur pour la clé donnée en entrée, _et_ vous **devez** déterminer si vous obtenez un `Some[V]` ou `None` comme résultat. Encore une fois, nous évitons de retourner `null` et de risquer un `NullPointerExceptions` si nous oublions de gérer ce cas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment déterminons-nous alors quel `Option[T]` a été retourné ? Voyons quelques exemples utilisant `Option`. Pouvez vous deviner ce qu'ils font ? Vérifier l'[Option Scaladocs](http://www.scala-lang.org/api/current/#scala.Option) pour confirmer. `Try` peut être utilisé de manière similaire, avec d'autres moyens disponibles que nous ne discuterons pas ici (mais regardez le [Try Scaladocs](http://www.scala-lang.org/api/current/#scala.util.Try))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val options = Seq(None, Some(2), Some(3), None, Some(5))\n",
    "\n",
    "options.foreach { o =>\n",
    "    println(o.getOrElse(\"None\"))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options.foreach {\n",
    "    case None    => println(None)\n",
    "    case Some(i) => println(i)  // Note how we extract the enclosed value.\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si vous voulez juste ignorer les valeurs `None`, utilisez un _for comprehension_ :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for {\n",
    "    option <- options  // loop through the options, assign each to \"option\"\n",
    "    value  <- option   // extract the value from the Some, or if None, skip to the next loop\n",
    "} println(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalement, vous vous demandez peut-être comment `None` est déclaré. Considérez l'exemple suivant :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val opts: Seq[Option[String]] = Seq(Some(\"hello\"), None, Some(\"world!\"))\n",
    "opts.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cela marche, donc cela veut dire que `None` est une sous-classe valide d'`Option[String]`. C'est en fait vrai pour tout `Option[T]`. Comment un seul object peut-il être un sous-type valide de _tous_ ces options ? Voilà comment il est déclaré (en ignorant quelques détails) : \n",
    "\n",
    "```scala\n",
    "object None extends Option[Nothing] {...}\n",
    "\n",
    "```\n",
    "\n",
    "`None` ne porte aucune information d'\"état\" car il n'enveloppe pas d'instance comme le fait `Some[T]`. Ainsi, nous n'avons besoin que d'une instance pour toutes ces utilisations, donc il est déclaré comme un objet. Rappelez-vous ce que nous avons mentionné au-dessus que le système de type possède un type [Nothing](http://www.scala-lang.org/api/current/#scala.Nothing) qui est un sous-type de tous les autres types. Sans rentrer dans les détails, si une variable est de type `Option[String]`, alors vous pouvez utiliser un `Option[Nothing]` pour elle (i.e., ce dernier est un sous-type du premier). C'est pour cela que `Nothing` est utile : pour les cas comme `None`, pour que nous puissions avoir une instance de cela, mais qui obéit quand meme aux règles du système de types orienté objet de Scala."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implicits\n",
    "<a name=\"implicits\"></a>  \n",
    "Scala possède un mécanisme puissant appelé _implicits_ qui est utilisé dans l'API Spark Scala. Les implicits représentent un sujet vaste, nous nous attarderons donc seulement sur les usages de ceux-ci qui sont importants de comprendre. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conversion de types\n",
    "Au-dessus nous avons utilisé des méthodes de `RDD` comme `reduceByKey`, mais si vous cherchez cette méthode dans la [page RDD de Scaladoc](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD), vous ne la trouverez pas. À la place, elle est définie dans le type [PairRDDFunctions](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions) (avec les autres méthodes de la forme `*ByKey`). Comment alors pouvons nous utiliser ces méthodes comme si elles étaient définies pour des `RDD` ??\n",
    "\n",
    "Quand le compilateur Scala voit un bout de code appeler une méthode qui n'existe pas pour un type, il cherche une _conversion implicit_ dans le scope, ce qui transforme l'instance dans un autre type possédant la méthode requise (c-à-d en l'enveloppant). La signature complète déduite pour la méthode doit correspondre à la définition de la classe enveloppante.\n",
    "\n",
    "> **Note:** Si vous ne trouvez pas une méthode dans la [Scaladocs Spark](http://spark.apache.org/docs/latest/api/scala/index.html#package) pour un type où vous pensez qu'elle devrait être définie, pensez à regarder les helper types associés à la méthode.\n",
    "\n",
    "Voici un petit exemple Scala de ce fonctionnement :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// A sample class. Note it doesn't define a `toJSON` method:\n",
    "case class Person(name: String, age: Int = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// To scope them, define implicit conversions within an object\n",
    "object implicits {\n",
    "\n",
    "    // `implicit` keyword tells the compiler to consider this conversion.\n",
    "    // It takes a `Person`, returning a new instance of `PersonToJSONString`,\n",
    "    // then resolves the invocation of `toJSON`.\n",
    "    implicit class PersonToJSONString(person: Person) {\n",
    "        def toJSON: String = s\"\"\"{\"name\": ${person.name}, \"age\": ${person.age}}\"\"\"\n",
    "    }\n",
    "}\n",
    "\n",
    "import implicits._        // Now it is visible in the current scope.\n",
    "\n",
    "val p = Person(\"Dean Wampler\", 39)\n",
    "\n",
    "// Magic conversion to `PersonToJSONString`, then `toJSON` is called.\n",
    "p.toJSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour les `RDDs`, les conversions implicites vers `PairRDDFunctions` et autres types supports sont gérées pour vous. Cependant, lorsque vous utilisez Spark SQL et l'API [DataFrame](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrame), vous devrez vous-même importer quelques-unes de ces conversions comme cela :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val sqlc = sqlContext\n",
    "import sqlc.implicits._  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val wtc = iiDF.select($\"word\", $\"total_count\")\n",
    "wtc.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La syntaxe de référence de colonne `$\"name\"` est implémentée en utilisant le même mécanisme de la librairie Scala qui implémente les _interpolated strings_, `s\"$foo\"`. Le `import sqlc.implicits._` la rend utilisable.\n",
    "\n",
    "Notez que nous avons importé quelque chose d'une _instance_, au lieu d'un package ou un type comme dans Java. Cela peut être une fonctionnalité utile dans Scala, mais également délicate. Si vous essayez `import sqlContext.implicits._`, vous aurez une erreur de compilation qui requiert un \"stable identifier\". Il s'avère qu'assigner la valeur `val sqlc = sqlContext` en premier permet de remplir cette condition. Cela est unique à l'environnement du notebook. Normalement, vous ne rencontrerez pas ce problème si vous utilisez le `spark-shell` qui vient avec la distribution Spark, ou si vous écrivez un programme Spark et le compilez avec le compilateur Scala.\n",
    "\n",
    "Cependant, ce serait mieux si Spark définissait cet objet `implicits` dans l'object compagnon de `SQLContext` à la place d'instances de celui-ci !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour compléter, mais sans rapport avec les implicits, l'API `DataFrame` vous permet d'écrire des requêtes à la SQL avec une API programmatique. Si vous voulez utiliser des fonctions intégrées comme `min`, `max`, etc. sur les colonnes, vous devez faire l'`import` suivant :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant nous pouvons utiliser `min`, `max`, `avg`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val mma = iiDF.select(min(\"total_count\"), max(\"total_count\"), avg(\"total_count\"))\n",
    "mma.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implicit Method Arguments\n",
    "Une autre utilisation des implicits qui mérite d'être su est les _arguments implicites_ de méthodes. Vous rencontrerez ce mécanisme en lisant le Spark Scaladocs, même si vous pourriez ne jamais comprendre qu'en fait vous l'utilisez dans votre code !\n",
    "\n",
    "Souvenez vous que, précédemment, j'ai mentionné le fait de pouvoir définir des valeurs par défaut pour les arguments de méthodes. Je l'ai utilisé pour l'argument `age` dans `Person`:\n",
    "\n",
    "```scala\n",
    "case class Person(name: String, age: Int = 0)\n",
    "```\n",
    "\n",
    "Parfois nous avons besoin de quelque chose de plus sophistiqué. Par exemple, notre librairie possède un groupe de méthodes ayant besoin chacun d'un argument spécial qui leur prodigue une information de \"contexte\" utile, mais vous ne voulez pas que l'utilisateur soit obligé de passer cet argument à chaque fois. À d'autres moments vous pourriez utiliser les arguments implicites pour que l'API soit plus \"propre\", tout en gardant le contrôle sur ce qui est autorisé de mettre.\n",
    "\n",
    "Voici un exemple qui est en partie inspiré de la méthode [Seq.sum](http://www.scala-lang.org/api/current/#scala.collection.Seq) de Scala. Ne serait-ce pas formidable si en ayant une collection de choses que nous pouvons \"additioner\" ensemble, qu'il suffirait d'appeller la méthode `sum` pour cette collection ? Faisons cela de manière un peu différente, avec une méthode helper `sum` en dehors de `Seq`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trait Add[T] {\n",
    "    def add(t1: T, t2: T): T\n",
    "}\n",
    "\n",
    "// Nested implicits so they don't conflict with the previous object implicits.\n",
    "object Adder {\n",
    "    object implicits {\n",
    "        implicit val intAdd = new Add[Int] { \n",
    "            def add(i1: Int, i2: Int): Int = i1+i2 \n",
    "        }\n",
    "        implicit val doubleAdd = new Add[Double] { \n",
    "            def add(d1: Double, d2: Double): Double = d1+d2 \n",
    "        }\n",
    "        implicit val stringAdd = new Add[String] { \n",
    "            def add(s1: String, s2: String): String = s1+s2 \n",
    "        }\n",
    "        // etc...\n",
    "    }\n",
    "}\n",
    "\n",
    "import Adder.implicits._\n",
    "\n",
    "// NOTE: TWO argument lists!\n",
    "def sum[T](ts: Seq[T])(implicit adder: Add[T]): T = {\n",
    "    ts.reduceLeft((t1, t2) => adder.add(t1, t2))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(0 to 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(0.0 to 5.5 by 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(Seq(\"one\", \"two\", \"three\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Will fail, because there's no Add[Char] in scope:\n",
    "sum(Seq('a', 'b', 'c'))   // Characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les valeurs implicites `intAdd`, `doubleAdd`, et `stringAdd` ont donc été utilisées par l'interpréteur Scala pour l'argument `adder` dans la seconde `liste d'arguments` de `sum`. Notez que vous devez utiliser une seconde liste d'arguments et que tous les arguments de cette liste doivent être implicites.\n",
    "\n",
    "Nous aurions pu éviter d'utiliser des arguments implicites si nous avions défini des méthodes custom `sum` pour chaque type. Cela aurait été plus simple dans ce cas trivial, mais pour des méthodes non-triviales les arguments implicites valent plus le coup pour éviter de la duplication. Un autre avantage de ce mécanisme est que l'utilisateur peut definir sa propre instance implicite `Add[T]` pour des domain types (par exemple `Money`) et ils \"fonctionneraient\".\n",
    "\n",
    "L'API collections de Scala utilise ce fonctionnement pour savoir comment construire une nouvelle collection du même type que la collection mise en entrée quand on utilise `map`, `flatMap`, `reduceLeft`, etc.\n",
    "\n",
    "Spark utilise ce pattern pour les [Encoders](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Encoder) dans Spark SQL. Les Encoders sont utilisés pour sérialiser des valeurs dans le nouveau et compact encodage de mémoire introduite par le projet _Tungsten_ (par exemple [ici](https://spark-summit.org/2015/events/deep-dive-into-project-tungsten-bringing-spark-closer-to-bare-metal/)). Voici un exemple pour créer un [Dataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset), où la méthode `toDS` est \"ajoutée\" en premier dans un [Seq](http://www.scala-lang.org/api/current/#scala.collection.Seq) à l'aide d'une conversion implicite (en particulier [SQLImplicits.localSeqToDatasetHolder](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SQLImplicits) qui est utilisable dans le scope grâce à l'import `import sqlc.implicits._`), et ensuite `toDS` utilise `Encoders` en interne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(0 to 10).toDS()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "J'apprécie l'effort que vous avez fourni pour faire ce notebook. J'espère qu'il vous a plu autant que j'ai eu du plaisir à l'écrire. Si vous avez des suggestions d'améliorations, vous pouvez les poster dans le [repo GitHub](https://github.com/deanwampler/JustEnoughScalaForSpark).\n",
    "\n",
    "Vous savez maintenant ce qu'il faut de Scala pour utiliser l'API Spark Scala. J'espère que vous pourrez apprécier la force et l'élégance de Scala. J'espère aussi que vous choisirez de l'utiliser pour toutes vos tâches de data ingénieur, pas juste pour Spark.\n",
    "\n",
    "Qu'en est-il de la data science ? Il y a beaucoup de personnes qui utilisent Scala pour la data science en Spark, mais aujourd'hui, Python et R ont de plus riches librairies pour les Mathématiques et le Machine Learning. Cela changera au cours du temps, mais pour l'instant, vous devrez décider quelle langage correspond le mieux à vos besoins.\n",
    "\n",
    "En utilisant Scala, il y aura plus de choses que vous voudrez comprendre que nous n'avons pas couvert, comme les idiomes communs, les conventions, et les outils utilisés dans la communauté Scala. Les références au début du notebook vous donneront les informations dont vous aurez besoin.\n",
    "\n",
    "Meilleurs voeux.\n",
    "\n",
    "[Dean Wampler, Ph.D.](mailto:deanwampler@gmail.com)<br/>\n",
    "[@deanwampler](http://twitter.com/deanwampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annexe : Solutions Exercices\n",
    "<a name=\"Solutions aux exercices\"></a>\n",
    "Voyons les solutions aux exercices qui n'ont pas été résolus tout à l'heure dans le notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtre pour les pièces de théâtres qui ont \"of\" dans leur nom\n",
    "Vous pouvez ajouter une condition (commentaire `// <== here`) immédiatement après avoir défini `play`. Vous pourriez le faire plus tard, après l'une des deux expressions suivantes, mais vous feriez des calculs inutiles. Changez `true` à `false` pour afficher les pièces de théâtres qui ne contiennent pas \"of\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val list2 = for {\n",
    "    play <- plays \n",
    "    if (play.contains(\"of\") == true)                            // <== here\n",
    "    playFileString = targetDirName + pathSeparator + play\n",
    "    playFile = new File(playFileString)\n",
    "} yield {\n",
    "    val successString = if (playFile.exists) \"Success!\" else \"NOT FOUND!!\"\n",
    "    \"%-40s\\t%s\".format(playFileString, successString)\n",
    "}\n",
    "list2.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Des mots \"Love\" et \"Hate\" plus spécifiques\n",
    "Un choix raisonnable pour éviter de voir `glove`, `whatever`, etc. est de trouver seulement les mots commençant par `love` et `hate`. Gardons aussi `unlove`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val topLocationsLoveHate = sqlContext.sql(\"\"\"\n",
    "    SELECT word,  total_count, locations[0] AS top_location, counts[0] AS top_count\n",
    "    FROM inverted_index \n",
    "    WHERE word LIKE 'love%' OR word LIKE 'unlove%' OR word LIKE 'hate%'\n",
    "\"\"\")\n",
    "topLocationsLoveHate.show(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retourner les premières localisations et totaux\n",
    "Nous utilisons l'API `DataFrame` pour écrire une requête SQL qui retourne les premières localisations et totaux. Ajouter le suivant est trivial. Que remarquez vous sur ce qui est retourné lorsqu'il n'y a pas de seconde localisation et total ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val topTwoLocations = sqlContext.sql(\"\"\"\n",
    "    SELECT word, total_count, \n",
    "        locations[0] AS first_location,  counts[0] AS first_count,\n",
    "        locations[1] AS second_location, counts[1] AS second_count\n",
    "    FROM inverted_index \n",
    "    WHERE word LIKE '%love%' OR word LIKE '%hate%'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topTwoLocations.show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enlever les mots vides\n",
    "Rappelez-vous qu'il vous a été demandé d'implémenter une méthode : `keep(word: String):Boolean` qui filtre les mots vides.\n",
    "\n",
    "Premièrement, implémentons `keep`. Vous pouvez trouver des listes de mots vides sur le net. Une de ces listes pour l'anglais peut être trouvée [ici]( * From http://norm.al/2009/04/14/list-of-english-stop-words/). Elle inclut plusieurs mots qui peuvent ne pas être considérés comme des mots vides. Néanmoins, j'utiliserai juste une liste plus petite ici.\n",
    "\n",
    "Notez que j'utiliserai un [Set](http://www.scala-lang.org/api/current/index.html#scala.collection.immutable.Set) de Scala pour contenir les mots vides. Nous voulons une performance de consultation en _0(1)_.  Nous voulons juste savoir si le mot est dans le set ou pas.\n",
    "\n",
    "J'ajouterai également \"\" pour enlever le test évident pour celui-ci.\n",
    "\n",
    "Finalement, nous intégrerons tout cela dans un nouvel `object` Scala. Cette encapsulation en plus permet d'éviter des problèmes occasionnels avec des erreurs comme \"task not serializable\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AVERTISSEMENT**: La définition de la prochaine cellule pourra déclencher une erreur `Task not serializable` dans la cellule qui suit où elle est utilisée. Si cela est le cas, c'est une anomalie de l'interpréteur Scala qui fonctionne avec l'environnement du notebook. Ce code devrait marcher sans problème dans les applications Spark que vous codez, c-à-d que vous compilez en applications avec `scalac`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object IIStopWords {\n",
    "    val stopWords = Set(\"\", \"a\", \"an\", \"and\", \"I\", \"he\", \"she\", \"it\", \"the\")\n",
    "\n",
    "    /**\n",
    "     * If the set contains the word, we return false - we don't want to keep it!\n",
    "     * Note we assume the word has already been converted to lower case!\n",
    "     */\n",
    "    def keep(word: String): Boolean = stopWords.contains(word) == false  \n",
    "    \n",
    "    def compute(sc: org.apache.spark.SparkContext, input: String) = {\n",
    "        sc.wholeTextFiles(input).\n",
    "        flatMap {\n",
    "            case (location, contents) => \n",
    "                val words = contents.split(\"\"\"\\W+\"\"\").\n",
    "                    map(word => word.toLowerCase).  // Do this early, before keep()\n",
    "                    filter(word => keep(word))      // <== filter here\n",
    "                val fileName = location.split(java.io.File.separator).last\n",
    "                words.map(word => ((word, fileName), 1))\n",
    "        }.\n",
    "        reduceByKey((count1, count2) => count1 + count2).\n",
    "        map { \n",
    "            case ((word, fileName), count) => (word, (fileName, count)) \n",
    "        }.\n",
    "        groupByKey.\n",
    "        sortByKey(ascending = true).\n",
    "        map { \n",
    "            case (word, iterable) => \n",
    "                val vect = iterable.toVector.sortBy { \n",
    "                    case (fileName, count) => (-count, fileName) \n",
    "                }\n",
    "                val (locations, counts) = vect.unzip  \n",
    "                val totalCount = counts.reduceLeft((n1,n2) => n1+n2)        \n",
    "                (word, totalCount, locations, counts)\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val iiStopWords = IIStopWords.compute(sc, \"/home/jovyan/work/data/shakespeare\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iiStopWords.take(100).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une dernière chose, nous avons maintenant `filter(word => keep(word))`, mais notez comment nous utilisons `println` dans la cellule précédente pour afficher les résultats. Nous pouvons faire quelque chose de semblable avec `filter` et écrire à la place `filter(keep)`.\n",
    "\n",
    "Qu'est-ce que cela veut dire exactement ? Cela dit au compilateur de \"convertir la _méthode_ `keep` vers une _fonction_ et la passer dans `filter`.\" Cela fonctionne car `keep` fait déjà ce que `filter` veut : prendre une seule chaîne de caractère en argument et retourner un boolean.\n",
    "\n",
    "Passer directement `keep` est en fait différent de passer `word => keep(word)`, qui est une fonction _anonyme_ qui _appelle_ keep. Nous utilisons `keep` elle-même comme une fonction, au lieu de construire une fonction qui utilise `keep`."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
