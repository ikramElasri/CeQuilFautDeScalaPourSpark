{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World!\n"
     ]
    }
   ],
   "source": [
    "println(\"Hello World!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(merrywivesofwindsor, twelfthnight, midsummersnightsdream, loveslabourslost, asyoulikeit, comedyoferrors, muchadoaboutnothing, tamingoftheshrew)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new java.io.File(\"/home/jovyan/work/data/shakespeare\").list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ce qu'il faut de Scala pour Spark\n",
    "\n",
    "Dean Wampler, Ph.D. [@deanwampler](http://twitter.com/deanwampler) ([email](mailto:deanwampler@gmail.com))\n",
    "\n",
    "Bienvenue. Ce notebook vous enseigne les principes fondamentaux de [Scala](http://scala-lang.org) nécessaires à l'utilisation de l'API Scala d'[Apache Spark](http://spark.apache.org). Spark utilise les meilleures features de Scala et évite d'utiliser celles qui sont plus difficiles et obscures. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction : Pourquoi Scala ?\n",
    "Spark vous laisse le choix entre Scala, Java, Python, R et SQL pour vos travaux. Les _data engineers_ préfèreront Scala, Java ou SQL et ce sont eux qui sont chargés de construire des infrastructures scalables et résilientes pour le _Big Data_. Les _data scientists_ vont préferer Python, R et SQL et vont pouvoir construire des modèles pour analyser la donnée à l'aide du machine learning. Ils utiliseront aussi du SQL lors de l'exploration des données.\n",
    "\n",
    "Bien sûr certains data engineers ne se limiteront pas à Scala et Java et pourront aussi utiliser Python et R et vice-versa.\n",
    "\n",
    "Quelques avantages à utiliser Scala pour Spark :\n",
    "* **Performance:** Spark étant écrit en Scala, vous obtiendrez une meilleure performance et une API plus complète en utilisant Scala. Il est vrai qu'avec les [DataFrames](http://spark.apache.org/docs/latest/sql-programming-guide.html), la performance ne dépend pas du langage choisi. Si vous avez besoin d'utiliser les [RDD](http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds), alors Scala vous donnera de meilleures performances avec Java arrivant en deuxième position.\n",
    "* **Debugging:** Quand vous êtes face à une erreur de runtime, comprendre la stack d'erreur vous sera plus facile si vous connaissez Scala.\n",
    "* **Code concis et expressif:** Comparé à Java, coder en Scala est beaucoup plus concis. Cela vous permet d'être plus productif et de pouvoir écrire vos idées en code sans avoir à batailler avec des API moins flexibles qui reflètent des contraintes de langage idiomatiques (vous pourrez observer cela au fur et à mesure)\n",
    "* **Type Safety:** Comparé à Python et R, coder en Scala vous permet de bénéficier du _static typing_ avec un _type inference_. _Static typing_ signifie que le parser Scala pourra trouver beaucoup plus d'erreur dans vos expressions lors de la compilation au lieu de les découvrir au runtime. Cependant le _type inference_ vous permet de ne pas à avoir à écrire explicitement certaines informations sur le type car ce sera Scala qui s'en chargera pour vous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pourquoi pas Scala ?\n",
    "Scala n'est pas un langage parfait, deux désavantages :\n",
    "* **Librairies:** Python et R ont tous les deux un riche ecosystème de librairies dédiées à l'analyse de données. Bien que Scala s'améliore sur ce point, Python et R restent devant.\n",
    "* **Fonctionnalités avancées:** Maîtriser des fonctionnalités avancées d'un langage vous donne beaucoup de possibilités dans l'élaboration de vos programmes. Mais si vous ne comprenez pas ces fonctionnalités, elles peuvent vous rendre la tâche plus ardue tant bien que votre unique but était de finir votre tâche coûte que coûte. Scala a des concepts compliqués, particulièrement dans son _type system_. Heureusement, Spark cache ces concepts avancés."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pour plus en savoir plus sur Scala\n",
    "Nous ne pourrons qu'aborder qu'une toute petite partie des concepts de Scala ici. Vous en apprendrez assez pour les utiliser mais éventuellement vous allez devoir en savoir un peu plus.\n",
    "\n",
    "Quand vous aurez besoin de plus d'informations, allez voir ces ressources :\n",
    "\n",
    "* [Programming Scala, Second Edition](http://shop.oreilly.com/product/0636920033073.do): Introduction à Scala\n",
    "* [Scala Language Website](http://scala-lang.org/): Comment télécharger Scala, où trouver de la documentation(e.g., [Scaladoc](http://www.scala-lang.org/api/current/#package): documentation de la librairie Scala comme [Javadocs](https://docs.oracle.com/javase/8/docs/api/)), et autres informations.\n",
    "* [Lightbend Scala Services](http://www.lightbend.com/services/) training, consulting, et support pour Scala.\n",
    "* [Lightbend Fast Data Platform](http://www.lightbend.com/fast-data-platform/): Notre nouvelle distribution pour la _Fast Data_ (stream processing), incluant Spark, Flink, Kafka, Akka Streams, Kafka Streams, HDFS, et notre outil de gestion de production et de monitoring, tournant sur Mesosphere DC/OS.\n",
    "\n",
    "Pour l'instant je vous recommande d'ouvrir la page du Scaladoc et de l'API Scala pour Spark. Vous pouvez y accéder en cliquant sur ces deux liens :\n",
    "* Scaladocs for <a href=\"http://www.scala-lang.org/api/current/#package\" target=\"scala_scaladocs\">Scala</a>.\n",
    "* Scaladocs for <a href=\"http://spark.apache.org/docs/latest/api/scala/index.html#package\" target=\"spark_scaladocs\">Spark</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Astuce pour utiliser Scaladoc:**\n",
    "* Utilisez la barre de recherche dans le corner en haut à gauche pour trouver un _type_ particulier. (Par exemple, essayez de trouver RDD) \n",
    "* Pour chercher une _méthode_ particulière, cliquez sur un caractère en dessous de la barre de recherche pour la première lettre de la méthode et scrollez y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pré-requis\n",
    "Sera assumé une expérience avec un langage de programmation quelconque ainsi qu'une familiarité avec Java mais si vous ne connaissez pas Java, une simple recherche vous suffira à comprendre.\n",
    "Ceci n'est pas une introduction à Spark, une expérience avec Spark est utile mais certains concepts seront expliqués brièvement.\n",
    "\n",
    "Tout au long vous pourrez trouver plus d'informations sur les sujets importants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A propos des Notebooks\n",
    "Vous utilisez [Jupyter](http://jupyter.org/) [All Spark Notebook Docker image](https://hub.docker.com/r/jupyter/all-spark-notebook/). Comme décrit dans le [GitHub README](https://github.com/deanwampler/JustEnoughScalaForSpark) vous importez ce notebook dans Jupyter qui tourne dans un container Docker.\n",
    "\n",
    "Les notebooks vous permettent de faire un mélange de documentation comme cette [Markdown](https://daringfireball.net/projects/markdown/) \"cellule\", avec des cellules qui contiennent du code, des graphs, etc. Dans la vie réelle, cela correspondrait à un cahier qu'un étudiant ou scientifique pourrait utiliser lorsqu'il travaille en laboratoire.\n",
    "\n",
    "Le menu et la barre d'outils en haut vous donne des options pour évaluer une cellule, créer et supprimer des cellules, etc. Pensez à retenir les principaux raccourcis car vous les utiliserez souvent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Astuces:**\n",
    "\n",
    "> Vous pouvez utiliser le menu _Help > Keyboard Shortcuts_, puis capturer la page en tant qu'image. Apprenons quelques raccourcis chaque jour.\n",
    "\n",
    "> Pour le moment, sachez juste que vous pouvez cliquer dans n'importe quelle cellule pour changer de focus. Quand vous êtes dans une cellule, `shift+enter` évalue la cellule (parcourt la cellule et affiche le Markdown ou bien fait tourner le code), puis passe à la cellule suivante. Essayez cela pour quelques cellules.\n",
    "\n",
    "Une fonctionnalité utile est de pouvoir éditer une cellule que vous avez lancé précedemment et la relancer. C'est très utile lorsque vous expérimentez avec votre code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L'environment\n",
    "Configurons l'environnement pour qu'il nous montre toujours les types des expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Types will be printed.\n"
     ]
    }
   ],
   "source": [
    "%showTypes on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quand vous commencez ce notebook, le plugin Jupyter Spark plugin crée un [SparkContext](http://spark.apache.org/docs/latest/programming-guide.html#initializing-spark) pour vous. C'est le point d'entrée de n'importe quelle application Spark (quand bien même vous utilisez la plus récente `SparkSession`). Ce SparkContext ou SparkSession sait comment se connecter à votre cluster (ou tourner localement sur la même JVM), comment configuer les propriétés, etc. Il lance aussi une interface web qui vous permet de monitorer vos jobs lancés. L'instance du `SparkContext` est appelée `sc`. La prochaine cellule confirme simplement qu'il existe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.SparkContext = org.apache.spark.SparkContext@190942b2\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelques informations utiles :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version:      2.4.0\n",
      "Spark master:       local[*]\n",
      "Running 'locally'?: true\n"
     ]
    }
   ],
   "source": [
    "println(\"Spark version:      \" + sc.version)\n",
    "println(\"Spark master:       \" + sc.master)\n",
    "println(\"Running 'locally'?: \" + sc.isLocal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargeons des données (et commençons à apprendre le Scala)\n",
    "Nous allons écrire de vrai programmes Spark et les utiliser pour apprendre Scala et Spark en même temps. \n",
    "\n",
    "Mais tout d'abord, nous allons avoir besoin d'utiliser quelques fichiers texte qui contiennent des pièces de Shakespeare. Les prochaines cellules définissent des méthodes helper pour configurer le tout. Nous apprendrons des concepts Scala à la volée."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** \"méthode\" vs. \"fonction\"\n",
    "\n",
    "> Scala utilise une convention commune provenant l'orienté objet, où le terme _méthode_ est utilisé pour une fonction qui est attachée à une classe ou instance. Contrairement à Java, tout du moins avant Java 8, Scala possède aussi des _fonctions_ qui ne sont pas associés à une classe ou instance particulière.\n",
    "\n",
    "> Dans notre prochain exemple de code, nous définissons plusieurs _méthodes_ helper pour afficher des informations, mais vous ne verrez pas de définition de classe ici. Ainsi, quelle est la classe associée à ces méthodes ? Quand vous utilisez Scala dans un notebook, vous utilisez en fait l'interpréteur Scala, qui prend chaque expression et définition que nous avons écrit et les met dans une classe cachée. L'interpréteur fait cela pour générer du byte code valide pour la JVM.\n",
    "\n",
    "> Malheuresement, cela peut-être assez confus de savoir quand il faut utiliser une méthode ou une fonction et cela reflète la nature hybride de Scala comme un langage orienté objet et un langage fonctionnel. Heureusement, dans la majorité des cas, nous pouvons utiliser des méthodes et fonctions interchangeablement, donc ne vous préoccupez pas trop de la distinction à partir de maintenant.\n",
    "\n",
    "> Nous allons maintenant définir des méthodes. Nous allons voir ce qu'est une véritable _fonction_ bientôt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici deux méthodes pour printer un message d'erreur ou un simple message d'information. Nous allons expliquer la syntaxe dans la cellule qui suit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "info: (message: String)String\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "/*\n",
    " * \"info\" takes a single String argument, prints it on a line,\n",
    " * and returns it. \n",
    " */\n",
    "def info(message: String): String = {\n",
    "    println(message)\n",
    "\n",
    "    // The last expression in the block, message, is the return value. \n",
    "    // \"return\" keyword not required.\n",
    "    // Do no additional formatting for the return string.\n",
    "    message  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "error: (message: String)String\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "/*\n",
    " * \"error\" takes a single String argument, prints a formatted error message,\n",
    " * and returns the message. \n",
    " */\n",
    "def error(message: String): String = {   \n",
    "    \n",
    "    // Print the string passed to \"println\" and add a linefeed (\"ln\"):\n",
    "    // See the next cell for an explanation of how the string is constructed.\n",
    "    val fullMessage = s\"\"\"\n",
    "        |********************************************************************\n",
    "        |\n",
    "        |  ERROR: $message\n",
    "        |\n",
    "        |********************************************************************\n",
    "        |\"\"\".stripMargin\n",
    "    println(fullMessage)\n",
    "    \n",
    "    fullMessage\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essayons les."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All is well.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "infoString = All is well.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "All is well."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val infoString = info(\"All is well.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********************************************************************\n",
      "\n",
      "  ERROR: Uh oh...\n",
      "\n",
      "********************************************************************\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "errorString: String = \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"\n",
       "********************************************************************\n",
       "  ERROR: Uh oh...\n",
       "********************************************************************\n",
       "\"\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "errorString: String = \n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val errorString = error(\"Uh oh...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\n",
       "********************************************************************\n",
       "  ERROR: Uh oh...\n",
       "********************************************************************\n",
       "\"\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "String = \n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errorString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les définitions de méthodes possèdent ces éléments suivants dans cet ordre :\n",
    "* Le mot clé `def` \n",
    "* Le nom de la méthode (`error` et `info` ici)\n",
    "* La liste des arguments entre parenthèses. S'il n'y a pas d'arguments, les parenthèses vides peuvent être omises. Commun pour `toString` et les getter qui retournent un champ dans une instance, etc.\n",
    "* Deux points suivi par le type de retour de la méthode. Ce type est souvent inferé par Scala et il n'est donc pas obligatoire mais il est fortement recommandé de le mettre tout le temps !\n",
    "* Un signe `=` qui sépare la _signature_ de la méthode du _corps_\n",
    "* Le corps entre accolades `{ ... }`, cependant si le corps consiste d'une seule expression alors les accolades sont optionnels\n",
    "* La dernière expression dans le corps est utilisé comme valeur de retour. Le mot clé `return` est déconseillé\n",
    "* Les point-virgules `;` sont inferés et ne sont pas utilisés dans la grande majorité des cas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardez la liste des arguments pour `error`. Il s'agit de `(message: String)`, où `message` est le nom de l'argument et son type est `String`. La convention pour les _annotations de type_, `name: Type`, est aussi utilisé pour le type de retour, `error(...): String`. Les annotations de type sont requis par Scala pour les arguments des méthodes. Elles sont optionnelles dans la majorité des cas pour le type de retour. Nous allons voir que Scala peut inférer les types de beaucoup d'expressions et de déclarations de variables.\n",
    "\n",
    "Scala utilise les mêmes conventions de commentaires que Java, `// ...` pour une ligne seule, et `/* ... */` pour un bloc de commentaire.\n",
    "\n",
    "> **Note:** Expression vs. Déclaration\n",
    "\n",
    "> Une _expression_ a une valeur, tandis qu'une _déclaration_ n'en a pas. Ainsi lorsque nous assignons une expression à une variable, la valeur que l'expression retourne est assignée à une variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans `error`, nous utilisons une combinaison d'interpolation de string avec la syntaxe `s\"\"\"...\"\"\"` :\n",
    "* **String avec trois guillemets :** `\"\"\"...\"\"\"`. Utile lorsque la string contient des retours à la ligne, comme dans `error`. (Nous allons aussi voir un autre avantage plus tard)\n",
    "* **Interpolation de String :** Utilisé en mettant `s` au début de la string `s\"...\"` ou `s\"\"\"...\"\"\"`. Cela nous permet de mettre des références à des variables et des expressions où la conversion en string sera insérée automatiquement. Par exemple : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Utilisez des accolades pour les expressions : 2.4.0.\n",
       "Elles sont optionnelles quand vous utilisez juste une variable : org.apache.spark.SparkContext@190942b2\n",
       "Faites attention dans les cas comme : org.apache.spark.SparkContext@190942b2etunautretruc\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "String = \n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s\"\"\"Utilisez des accolades pour les expressions : ${sc.version}.\n",
    "Elles sont optionnelles quand vous utilisez juste une variable : $sc\n",
    "Faites attention dans les cas comme : ${sc}etunautretruc\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une autre fonctionnalité que nous utilisons pour les strings entre trois guillemets est la possiblité d'enlever le premier espace de chaque ligne. La méthode `stripMargin` enlève tous les espaces avant en incluant `|`. Cela vous permet d'indenter ces lignes pour que votre code soit formaté proprement mais sans avoir d'espace dans votre string. Dans l'exemple suivant, la string résultante possède des espaces en début de ligne et à la fin de la ligne. Observez ce qui arrive avec les espaces avant `line2` et `line3` quand la string entière est affichée :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\n",
       "line 1\n",
       "  line 2\n",
       "  line 3\n",
       "\"\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "String = \n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s\"\"\"\n",
    "    |line 1\n",
    "    |  line 2\n",
    "    |  |  line 3\n",
    "    |\"\"\".stripMargin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les caractères 'littéraux' sont indiqués avec des apostrophes, '/', tandis que les string sont indiquées avec des guillemets, \"/\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Char = /\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "String = /\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables mutable vs. valeurs immutables\n",
    "Regardons comment déclarer une valeur immutable avec `val` :\n",
    "* `val immutableValue = ...`: Une fois initialisée, nous ne pouvons pas attribuer une valeur _différente_ à `immutableValue`.\n",
    "* `var mutableVariable = ...`: Nous pouvons attribuer de nouvelles valeurs à `mutableVariable` autant de fois qu'on le souhaite.\n",
    "\n",
    "Il est _très recommandé_ de n'utiliser que des `vals` à moins que vous ayez une très bonne raison d'avoir recours à de la mutabilité, qui est une source très commune de bugs !\n",
    "\n",
    "> Un `val immutableValue` peut pointer vers une instance qui _elle_ est mutable par exemple un [Array](http://www.scala-lang.org/api/current/#scala.Array). Dans ce cas là, même si nous ne pouvons pas assigner un nouveau Array à `immutableValue`, nous pouvons changer les éléments dans l'Array ! Dis d'une autre façon, l'immutabilité n'est pas _transitif_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Préparer les fichiers\n",
    "Ce notebook possède déjà les fichiers de données nécessaires, à savoir plusieurs pièces de Shakespeare. Elles sont dans\n",
    "le dossier `/home/jovyan/work/data/shakespeare` dans le container (`data/shakespeare` dans le projet git). Il y a un fichier par pièce.\n",
    "\n",
    "Nous allons écrire du code Scala pour vérifier que les fichiers sont bien présents et en même temps apprendre un peu de Scala."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beaucoup de types utilisés en Scala proviennent de la librairie Java (JDK). Parce que Scala compile vers du byte code JVM, vous pouvez utiliser n'importe quelle librairie Java en Scala. Nous étions en train d'utiliser [java.lang.String](https://docs.oracle.com/javase/8/docs/api/java/lang/String.html). Nous allons maintenant utiliser [java.io.File](https://docs.oracle.com/javase/8/docs/api/java/io/File.html) pour travailler avec les différents fichiers et dossiers.\n",
    "\n",
    "Comme avant, nous allons utiliser des commentaires pour expliquer quelques nouvelles notions de Scala."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Import File. Unlike Java, the semicolon ';' is not required.\n",
    "import java.io.File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici le dossier où les fichiers devraient être situés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "shakespeare = /home/jovyan/work/data/shakespeare\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "/home/jovyan/work/data/shakespeare"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val shakespeare = new File(\"/home/jovyan/work/data/shakespeare\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le `if` en Scala est en fait une expression (en Java ce sont des _déclarations_). L'expression `if` retourne `true` ou `false` et l'assigne à un `success` que nous allons utiliser juste après."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/data/shakespeare exists\n",
      "success = true\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "success = true\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val success = if (shakespeare.exists == false) {   // doesn't exist already?\n",
    "    error(s\"Data directory path doesn't exist! $shakespeare\")  // ignore returned string\n",
    "    false\n",
    "} else {\n",
    "    info(s\"$shakespeare exists\")\n",
    "    true\n",
    "}\n",
    "println(\"success = \" + success)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vérifions maintenant que les fichiers sont bien là."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking that the plays are in /home/jovyan/work/data/shakespeare:\n",
      "Finished!\n",
      "All plays found!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pathSeparator = /\n",
       "targetDirName = /home/jovyan/work/data/shakespeare\n",
       "plays = List(tamingoftheshrew, comedyoferrors, loveslabourslost, midsummersnightsdream, merrywivesofwindsor, muchadoaboutnothing, asyoulikeit, twelfthnight)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "All plays found!"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pathSeparator = File.separator\n",
    "val targetDirName = shakespeare.toString\n",
    "val plays = Seq(\n",
    "    \"tamingoftheshrew\", \"comedyoferrors\", \"loveslabourslost\", \"midsummersnightsdream\",\n",
    "    \"merrywivesofwindsor\", \"muchadoaboutnothing\", \"asyoulikeit\", \"twelfthnight\")\n",
    "\n",
    "if (success) {\n",
    "    println(s\"Checking that the plays are in $shakespeare:\")\n",
    "    val failures = for {\n",
    "        play <- plays\n",
    "        playFileName = targetDirName + pathSeparator + play\n",
    "        playFile = new File(playFileName)\n",
    "        if (playFile.exists == false)\n",
    "    } yield {\n",
    "        s\"$playFileName:\\tNOT FOUND!\"\n",
    "    }\n",
    "  \n",
    "    println(\"Finished!\")\n",
    "    if (failures.size == 0) {\n",
    "        info(\"All plays found!\")\n",
    "    } else {\n",
    "        println(\"The following expected plays were not found:\")\n",
    "        failures.foreach(play => error(play))\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous utilisons ici un `for` _comprehension_. Ce sont des _expressions_, et non pas des _déclarations_ comme les `for` Java. Elles ont la forme :\n",
    "\n",
    "```\n",
    "for {\n",
    "  play <- plays\n",
    "  ...\n",
    "} yield { block_of_final_expressions }\n",
    "```\n",
    "Nous itérons sur une collection `plays`, et assignons chacun à la variable `play` (qui est en fait une valeur immutable pour chaque itération). \n",
    "\n",
    "Après avoir assigné à `play`, les prochaines étapes dans la `for` comprehension l'utilise. Premièrement, une instance de  [java.io.File](https://docs.oracle.com/javase/8/docs/api/java/io/File.html) `playFile`, est crée. Ensuite `playFile` est utilisé pour évaluer un prédicat - le fichier existe t'il déjà ? (Il devrait !)\n",
    "\n",
    "Si le fichier existe déjà alors `false` est retourné, ce qui casse la boucle et nous passons à la prochaine itération.\n",
    "Si le fichier n'existe pas, alors le mot clé `yield` dit à Scala que nous voulons utiliser l'expression qui suit pour construire un nouvel élément, une string _interpolée_ pour les pièces manquantes. Parmi ces éléments retournés, pouvant aller de zéro à un nombre, une nouvelle collection est construite. Le bloc final `if` détermine si la nouvelle collection a zéro éléments (attendu), puis affiche un message `info`. Si des fichiers manquaient, alors un message `error` serait affiché pour chacun des fichiers manquant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utiliser des fonctions comme argument\n",
    "\n",
    "Notez comment nous avons affiché les `successes`. L'idiome `collection.foreach(println)` est utile pour itérer sur des éléments et les afficher, un par ligne. Mais comment cela marche t'il exactement ? (Nous utiliserons `plays` au lieu de `failures`, parce que le deuxième est normalement toujours vide !)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass println as the function to use for each element:\n",
      "tamingoftheshrew\n",
      "comedyoferrors\n",
      "loveslabourslost\n",
      "midsummersnightsdream\n",
      "merrywivesofwindsor\n",
      "muchadoaboutnothing\n",
      "asyoulikeit\n",
      "twelfthnight\n",
      "\n",
      "Using an anonymous function that calls println: `str => println(str)`\n",
      "(Note that the type of the argument `str` is inferred to be String.)\n",
      "tamingoftheshrew\n",
      "comedyoferrors\n",
      "loveslabourslost\n",
      "midsummersnightsdream\n",
      "merrywivesofwindsor\n",
      "muchadoaboutnothing\n",
      "asyoulikeit\n",
      "twelfthnight\n",
      "\n",
      "Adding the argument type explicitly. Note that the parentheses are required.\n",
      "tamingoftheshrew\n",
      "comedyoferrors\n",
      "loveslabourslost\n",
      "midsummersnightsdream\n",
      "merrywivesofwindsor\n",
      "muchadoaboutnothing\n",
      "asyoulikeit\n",
      "twelfthnight\n",
      "\n",
      "Why do we need to name this argument? Scala lets us use _ as a placeholder.\n",
      "tamingoftheshrew\n",
      "comedyoferrors\n",
      "loveslabourslost\n",
      "midsummersnightsdream\n",
      "merrywivesofwindsor\n",
      "muchadoaboutnothing\n",
      "asyoulikeit\n",
      "twelfthnight\n",
      "\n",
      "For longer functions, you can use {...} instead of (...).\n",
      "Why? Because it gives you the familiar multiline block syntax with {...}\n",
      "tamingoftheshrew\n",
      "comedyoferrors\n",
      "loveslabourslost\n",
      "midsummersnightsdream\n",
      "merrywivesofwindsor\n",
      "muchadoaboutnothing\n",
      "asyoulikeit\n",
      "twelfthnight\n",
      "\n",
      "The _ placeholder can be used *once* for each argument in the list.\n",
      "As an assume, use `reduceLeft` to sum some integers.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "integers = Range(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "println(\"Pass println as the function to use for each element:\")\n",
    "plays.foreach(println)\n",
    "\n",
    "println(\"\\nUsing an anonymous function that calls println: `str => println(str)`\")\n",
    "println(\"(Note that the type of the argument `str` is inferred to be String.)\")\n",
    "plays.foreach(str => println(str))\n",
    "\n",
    "println(\"\\nAdding the argument type explicitly. Note that the parentheses are required.\")\n",
    "plays.foreach((str: String) => println(str))\n",
    "\n",
    "println(\"\\nWhy do we need to name this argument? Scala lets us use _ as a placeholder.\")\n",
    "plays.foreach(println(_))\n",
    "\n",
    "println(\"\\nFor longer functions, you can use {...} instead of (...).\")\n",
    "println(\"Why? Because it gives you the familiar multiline block syntax with {...}\")\n",
    "plays.foreach {\n",
    "  (str: String) => println(str)\n",
    "}\n",
    "\n",
    "println(\"\\nThe _ placeholder can be used *once* for each argument in the list.\")\n",
    "println(\"As an assume, use `reduceLeft` to sum some integers.\")\n",
    "val integers = 0 to 10   // Return a \"range\" from 0 to 10, inclusive\n",
    "integers.reduceLeft((i,j) => i+j)\n",
    "integers.reduceLeft(_+_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notre premier programme Spark\n",
    "Ouf ! Nous avons déjà appris beaucoup de Scala en faisant des tâches typiques de data science (par exemple récupérer de la donnée)\n",
    "Maintenant nous pouvons implémenter un algorithme en utilisant Spark, _Inverted Index_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverted Index - Quand vous ne voulez plus comptez les mots...\n",
    "\n",
    "Vpus allez avoir besoin d'utiliser _Inverted Index_ quand vous aller créer votre prochain \"Google killer\". Il prend en entrée un corpus de documents (des pages webs par exemple), tokenize les mots et sort pour chaque mot une liste des documents qui contienne ce mot avec à côté le nombre de fois que ce mot apparaît.\n",
    "\n",
    "C'est un algorithme un peu plus intéressant que le _Word Count_, qui est en quelque sorte le \"hello world\" que tout le monde fait quand la personne apprend Spark.\n",
    "\n",
    "Le terme _inverted_  signifie que nous commencons avec des mots comme valeurs d'entrée tandis que les clés sont des identifiants de documents et que nous allons inverser le fait d'utiliser les mots comme clé et les identifiants de document comme valeur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici notre première version dans son entièreté. C'est une _unique, longue expression_. Notez les points `.` à la fin des sous expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "iiFirstPass1 = MapPartitionsRDD[9] at mapValues at <console>:44\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[9] at mapValues at <console>:44"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val iiFirstPass1 = sc.wholeTextFiles(shakespeare.toString).\n",
    "    flatMap { location_contents_tuple2 => \n",
    "        val words = location_contents_tuple2._2.split(\"\"\"\\W+\"\"\")\n",
    "        val fileName = location_contents_tuple2._1.split(pathSeparator).last\n",
    "        words.map(word => ((word, fileName), 1))\n",
    "    }.\n",
    "    reduceByKey((count1, count2) => count1 + count2).\n",
    "    map { word_file_count_tup3 => \n",
    "        (word_file_count_tup3._1._1, (word_file_count_tup3._1._2, word_file_count_tup3._2)) \n",
    "    }.\n",
    "    groupByKey.\n",
    "    sortByKey(ascending = true).\n",
    "    mapValues { iterable => \n",
    "        val vect = iterable.toVector.sortBy { file_count_tup2 => \n",
    "            (-file_count_tup2._2, file_count_tup2._1)\n",
    "        }\n",
    "        vect.mkString(\",\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant cherchons à décomposer en étapes, en assignant chaque étape à une variable. La verbosité supplémentaire que cela apporte nous permettra de voir ce que Scala infère pour le type retourné de chaque expression, dans des buts d'apprentissage.\n",
    "\n",
    "C'est une des fonctionnalités très agréable de Scala. Nous n'avons pas à mettre d'information de type manuellement la plupart du temps comme nous aurions du le faire avec du code Java. Au contraire, nous laissons le compilateur nous donner un retour sur ce que nous venons de créer. C'est très utile lorsque vous apprenez une nouvelle API, comme Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fileContents = /home/jovyan/work/data/shakespeare MapPartitionsRDD[11] at wholeTextFiles at <console>:30\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "/home/jovyan/work/data/shakespeare MapPartitionsRDD[11] at wholeTextFiles at <console>:30"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fileContents = sc.wholeTextFiles(shakespeare.toString)\n",
    "fileContents   // force the notebook to print the type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La deuxième ligne, avec `fileContents` tout seul, est là pour que le notebook nous montre les informations de type.(Essayez de l'enlever et de réévaluer la cellule, rien ne sera affiché)\n",
    "\n",
    "La sortie nous indique que `fileContents` possède le type [RDD[(String,String)]](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD), cependant `RDD` est une classe de base qui est en fait une instance de `MapPartitionsRDD`, une implémentation \"privée\" de la sous classe `RDD`. \n",
    "\n",
    "Un nom suivi de crochets, `[...]`, signifie que le `RDD[...]` a besoin d'un ou plusieurs paramètres de type dans les crochets. Dans ce cas là, il y a un unique paramètre de type qui représente le type des enregistrements du `RDD`. \n",
    "\n",
    "Le paramètre de type unique est donné par `(String,String)`, qui est un raccourci bien utile pour [Tuple2[String,String]](http://www.scala-lang.org/api/current/index.html#scala.Tuple2).\n",
    "Cela signifie que nous avons des tuples de deux éléments comme enregistrements, où le premier élément est une `String` représentant le chemin absolu d'un fichier et que le second élément est aussi une `String` qui représente le contenu du fichier. C'est ce que retourne `SparkContext.wholeTextFiles` pour nous. Nous utiliserons le chemin du fichier pour se rappeler où nous avons trouvé des mots, tandis que le contenu contiendra les mots même.\n",
    "\n",
    "Pour résumer, ces deux types sont équivalents :\n",
    "* `RDD[(String,String)]` - Notez les parenthèses entre les crochets, `[(...)]`.\n",
    "* `RDD[Tuple2[String,String]]` - Notez les crochets dans les crochets `[...[...]]`, et non pas `[(...)]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons voir sous peu que nous pouvons aussi écrire des _instances_ de [Tuple2[T1,T2]](http://www.scala-lang.org/api/current/index.html#scala.Tuple2) avec la même syntaxe, e.g., `(\"foo\", 101)`, pour un tuple `(String,Int)`, et similairement pour des tuples d'_arité supérieur_ (jusqu'à 22 elements...), e.g., `(\"foo\", 101, 3.14159, (\"bar\", 202L))`. Faites tourner la prochaine cellule pour voir le type du tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(foo,101,3.14159,(bar,202))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\"foo\", 101, 3.14159, (\"bar\", 202L))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avez-vous compris ? Voyez-vous qu'il s'agit d'un tuple de quatre élément et non pas d'un tuple de cinq élément ? C'est parce que `(\"bar\", 202L)` est un tuple imbriqué. C'est le quatrième élément du tuple extérieur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice:** Essayez de créer des tuples avec des éléments de type différent, utilisez la prochaine cellule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,2)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combien de `fileContents` avons-nous ? Pas énormément. Nous devrions normalement avoir le même nombre que le nombre de fichier que nous avons chargé en haut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileContents.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE:** Nous avons utilisé la méthode`RDD.count`, alors que la majorité des collections Scala ont une méthode `size`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant pour la prochaine étape dans notre calcul. En premier, nous allons tokenizer le contenu des mots en les séparant sur les caractères non alphanumériques, donc toutes les instances d'espaces, de retour à la lignes, de ponctuations, etc.\n",
    "\n",
    "Ensuite, le chemin absolu étant verbeux et le même préfixe étant répété pour tous les fichiers, extrayons juste le dernier élément, le nom unique du fichier.\n",
    "\n",
    "Ainsi nous allons former des tuples avec des mots et des noms de fichier.\n",
    "\n",
    "> **Note:** Cette tokenization est très brute. Elle ne prend pas en compte les contractions comme `it's` et les mots séparés par un trait d'union comme `world-changing`. Lorsque vous aller tuer Google, soyez sûr d'utiliser une vraie technique de tokenization NLP, NLP pour Natural Language Processing ou Traitement de Langage Naturel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wordFileNameOnes = MapPartitionsRDD[12] at flatMap at <console>:34\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[12] at flatMap at <console>:34"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val wordFileNameOnes = fileContents.flatMap { location_contents_tuple2 => \n",
    "    // example input record: (file_path, \"all the words in the file\")\n",
    "    // mytuple._2 => give me the 2nd element\n",
    "    val words = location_contents_tuple2._2.split(\"\"\"\\W+\"\"\")              \n",
    "    // mytuple._1 => give me the 1st element\n",
    "    val fileName = location_contents_tuple2._1.split(pathSeparator).last  \n",
    "    // create a new tuple to return. Note how we structured it!\n",
    "    words.map(word => ((word, fileName), 1))\n",
    "}\n",
    "wordFileNameOnes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je trouve que cela est dur à lire et nous allons bientôt voir une solution plus élegante avec une syntaxe alternative.\n",
    "\n",
    "Essayons de comprendre la différence entre `map` et `flatMap`. Si j'appellais `fileContents.map`, cela retournerait exactement _un_ nouvel enregistrement pour chaque enregistrement dans _fileContents_. Ce que nous voulons en réalité sont des nouveaux enregistrements pour chaque couple de mot-nomDeFichier, qui correspondrait à un nombre plus large (mais la donnée dans chaque enregistrement serait beaucoup plus petite)\n",
    "\n",
    "Utiliser `fileContents.flatMap` nous donne ce que nous voulons. Au lieu de retourner en sortie un enregistrement pour chaque enregistrement en entrée, un `flatMap` retourne une _collection_ de nouveaux enregistrements, de taille supérieure à 0, pour _chaque_ enregistrement d'entrée. Ces collections sont ensuite _aplaties_ dans une grosse collection, un autre `RDD` dans notre cas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que devrais faire `flatMap` pour chaque enregistrement ? Nous passons une _fonction_ pour définir le comportement. Nous utilisons donc une fonction sans nom ou _anonyme_. La syntaxe est `liste_des_arguments => body`:\n",
    "\n",
    "```scala\n",
    "location_contents_tuple2 => \n",
    "    val words = ...\n",
    "    ...\n",
    "}\n",
    "```\n",
    "\n",
    "Nous avons un argument unique, l'enregistrement, que nous avons nommé `location_contents_tuple2`, une façon verbeuse de dire que nous avons à faire à un tuple de deux éléments avec en entrée le chemin du fichier et son contenu. Nous n'avons pas besoin d'un paramètre de type après `location_contents_tuple2` car c'est inféré par Scala. La flèche `=>` sépare la liste des arguments du corps.\n",
    "\n",
    "Quand une fonction prend plus qu'un seul argument ou que vous ajoutez des annotations de type explicite (`: (String,Int,Double)`), alors vous aurez besoin de parenthèses. Voici trois exemples :\n",
    "\n",
    "```scala\n",
    "(some_tuple3: (String,Int,Double)) => ...\n",
    "(arg1, arg2, arg3) => ...\n",
    "(arg1: String, arg2: Int, arg3: Double) => ...\n",
    "```\n",
    "Nous laissons Scala inférer le type des arguments, et dans notre cas c'est `(String,String)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une seconde, nous avons précedemment dit que nous passions une fonction en tant qu'argument de `flatMap`. Si c'est le cas, pourquoi utilisons nous des accolades `{...}` autour de l'argument de cette fonction au lieu de parenthèses `(...)` qu'il serait normal de retrouver lorsque vous passez des arguments à une méthode comme `flatMap`? \n",
    "\n",
    "C'est parce que Scala nous permet d'utiliser des accolades au lieu de parenthèses pour avoir la syntaxe familière du bloc `{...}` que nous connaissons tous et aimons pour les expressions `if` et `for`. Nous pourrions utiliser soit des accolades ou des parenthèses ici. La convention dans la communauté Scala est d'utiliser des accolades pour les fonctions anonymes qui font plusieurs lignes et des parenthèses pour les expressions tenant sur une ligne."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant pour chaque `location_contents_tuple2`, nous accédons au _premier_ élément en utilisant la méthode `_1` et le _deuxième_ élément en utilisant `_2`.\n",
    "\n",
    "Le fichier `contents` est dans le deuxième élément. Nous les séparons en appelant la méthode Java `String.split`, qui prend une string correspondant à une _expression régulière_. Ici nous spécifions une expression régulière pour un ou plusieurs caractères non alphanumériques. `String.split` retourne un `Array[String]` des mots. \n",
    "\n",
    "```scala\n",
    "val words = location_contents_tuple2._2.split(\"\"\"\\W+\"\"\")\n",
    "```\n",
    "\n",
    "Pour le premier élément du tuple, nous extrayons le nom du fichier en fin de path. Ce n'est pas nécessaire, mais ça permet d'avoir une sortie plus lisible si nous retirons le long préfixe commun du path.\n",
    "\n",
    "```scala\n",
    "val fileName = location_contents_tuple2._1.split(pathSeparator).last\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalement, nous utilisons `Array.map` (et non pas `RDD.map`) dans la fonction anonyme passé au `flatMap` pour transformer chaque `word` en un tuple de la forme `((word, fileName), 1)`.\n",
    "\n",
    "```scala\n",
    "words.map(word => ((word, fileName), 1))\n",
    "```\n",
    "\n",
    "Pourquoi avons nous imbriqué un tuple de `(word, fileName)` dans un tuple \"extérieur\" avec un `1` comme deuxième élément ? Pourquoi ne pas seulement avoir crée un tuple de trois éléments `(word, fileName, 1)`? C'est parce que nous utilisons `(word, fileName)` comme une _clé_ dans la prochaine étape, où nous allons trouver des combinaisons uniques de word-fileName (en utilisant l'équivalent du `group by`). Ainsi, utiliser le `(word, fileName)` imbriqué comme la clé _key_ est plus pratique. La _valeur_ `1` _value_ est un count \"seed\", que nous allons utiliser pour compter les occurencese uniques des paires de `(word, fileName)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Notes:**\n",
    "> * Pour des raisons historiques, les indices de tuple commencent à 1 et non 0. Les Arrays et autre collections de Scala ont un index commencant à 0.\n",
    "> * Nous avons précédemment dit que les arguments d'une _méthode_ ont besoin d'être déclarés avec des types. Ce n'est pas nécessairement requis pour les arguments de _fonctions_ comme ici.\n",
    "> * Un autre bénéfice d'une string avec trois guillemets qui les rend sympathique pour les expressions régulières est que vous n'avez pas à échapper vos caractères comme `\\W`. Si nous avions utilisé un seul guillemet alors nous aurions du écrire `\"\\\\W+\"`. A vous de faire votre choix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comptons le nombre de lignes que nous avons et regardons quelques lignes. Nous utiliserons la méthode `RDD.take` pour prendre les 10 premières lignes, itérer dessus et les afficher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "173336"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordFileNameOnes.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((,merrywivesofwindsor),1)\n",
      "((THE,merrywivesofwindsor),1)\n",
      "((MERRY,merrywivesofwindsor),1)\n",
      "((WIVES,merrywivesofwindsor),1)\n",
      "((OF,merrywivesofwindsor),1)\n",
      "((WINDSOR,merrywivesofwindsor),1)\n",
      "((DRAMATIS,merrywivesofwindsor),1)\n",
      "((PERSONAE,merrywivesofwindsor),1)\n",
      "((SIR,merrywivesofwindsor),1)\n",
      "((JOHN,merrywivesofwindsor),1)\n"
     ]
    }
   ],
   "source": [
    "wordFileNameOnes.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons demandé des résultats, nous forçons donc Spark à lancer un job pour calculer le résultat. Les pipelines Spark comme `iiFirstPass1` sont _lazy_; rien n'est calculé tant que nous ne demandons pas de résultat. \n",
    "\n",
    "Quand vous apprenez, il est utile d'afficher des données pour comprendre mieux ce qui se passe. Faites attention toutefois car cela utilise plus de ressources.\n",
    "\n",
    "Le premier enregistrement nous montre \"\" (vide) comme mot :\n",
    "\n",
    "```\n",
    "((,asyoulikeit),1)\n",
    "```\n",
    "\n",
    "Aussi certains mots sont en majuscules :\n",
    "```\n",
    "((DRAMATIS,asyoulikeit),1)\n",
    "```\n",
    "(Vous pouvez voir que ces mots en majuscules apparaissent si vous regardez dans les fichiers sources) Plus tard nous allons filtrer tous ces mots vide et passer tous les mots en minuscule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant joignons toutes les paires uniques de `(word,fileName)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "uniques = ShuffledRDD[13] at reduceByKey at <console>:36\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ShuffledRDD[13] at reduceByKey at <console>:36"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val uniques = wordFileNameOnes.reduceByKey((count1, count2) => count1 + count2)\n",
    "uniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En SQL vous pouvez utiliser `GROUP BY` pour ça (en incluant les requêtes SQL que vous pourriez écrire avec l'API Spark [DataFrame](http://spark.apache.org/docs/latest/sql-programming-guide.html)). Néanmoins, dans l'API `RDD`, c'est trop couteux pour nos besoins car nous ne nous préoccupons pas des groupes eux-mêmes, la longue liste des paires répétées de `(word,fileName)`. Nous nous préoccupons juste de combien d'éléments il y a dans chaque groupe, c'est à dire leur _size_. C'est la raison du `1` dans les tuples et pourquoi nous utilisons `RDD.reduceByKey`. Cela ramène ensemble toutes les lignes avec la même clé, les paires unique de `(word,fileName)`, et applique ensuite une fonction anonyme pour \"réduire\" les valeurs, les`1`. Nous faisons juste une somme après pour calculer le count des groupes.\n",
    "\n",
    "Notez que la fonction anonyme `reduceByKey` attend deux arguments, nous avons donc besoin de parenthèses autour de la liste des arguments. Puisque la fonction tient sur une seule ligne, nous utilisons des parenthèses au lieu d'accolades.\n",
    "\n",
    "> **Note:** Toutes les méthodes `*ByKey` opèrent sur des tuples de deux éléments et traitent le premier élément comme la clé par défaut."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combien y en a t-il ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27276"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniques.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme attendu d'un `GROUP BY`, le nombre d'enregistrements est plus petit qu'avant. Il y a environ 1/6 de lignes comparé à avant, cela signifie qu'en moyenne chaque `(word,fileName)` apparaît 6 fois."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((dexterity,merrywivesofwindsor),1)\n",
      "((force,muchadoaboutnothing),2)\n",
      "((whole,comedyoferrors),2)\n",
      "((lamb,muchadoaboutnothing),2)\n",
      "((blunt,tamingoftheshrew),3)\n",
      "((letter,merrywivesofwindsor),19)\n",
      "((crest,asyoulikeit),1)\n",
      "((bestow,asyoulikeit),1)\n",
      "((rear,midsummersnightsdream),1)\n",
      "((crossing,tamingoftheshrew),1)\n",
      "((wronged,merrywivesofwindsor),4)\n",
      "((S,tamingoftheshrew),10)\n",
      "((HIPPOLYTA,midsummersnightsdream),19)\n",
      "((revolve,twelfthnight),1)\n",
      "((er,merrywivesofwindsor),11)\n",
      "((renown,asyoulikeit),1)\n",
      "((cubiculo,twelfthnight),1)\n",
      "((All,twelfthnight),3)\n",
      "((power,loveslabourslost),8)\n",
      "((Albeit,asyoulikeit),1)\n",
      "((lips,tamingoftheshrew),3)\n",
      "((upshot,twelfthnight),1)\n",
      "((approach,midsummersnightsdream),4)\n",
      "((mean,muchadoaboutnothing),5)\n",
      "((embossed,asyoulikeit),1)\n",
      "((varnish,loveslabourslost),2)\n",
      "((Apollo,midsummersnightsdream),1)\n",
      "((spangled,midsummersnightsdream),1)\n",
      "((gentlemen,comedyoferrors),1)\n",
      "((Rebuke,loveslabourslost),1)\n"
     ]
    }
   ],
   "source": [
    "uniques.take(30).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour _inverted index_, nous voulons que les dernières clés soient des mots, il faut donc restructurer les tuples de `((word,fileName),count)` vers `(word,(fileName,count))`. Maintent, nous allons encore sortir des tuples à deux éléments mais le `word` sera la clé et le `(fileName,count)` sera la valeur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "words = MapPartitionsRDD[14] at map at <console>:38\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[14] at map at <console>:38"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val words = uniques.map { word_file_count_tup3 => \n",
    "    (word_file_count_tup3._1._1, (word_file_count_tup3._1._2, word_file_count_tup3._2)) \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les méthodes de tuple `_1._2` sont assez difficiles à lire et la logique devient obscure. Nous allons voir par la suite une alternative beaucoup plus élégante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons utiliser l'opération `group by`, car nous en avons besoin pour garder les groupes. Appeller `RDD.groupByKey` \n",
    "utilise le premier élément du tuple, présentement juste les `words`, pour ramener ensemble toutes les occurences de mots uniques. Ensuite nous allons trier les mots par ordre alphabétique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wordGroups = ShuffledRDD[18] at sortByKey at <console>:40\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ShuffledRDD[18] at sortByKey at <console>:40"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val wordGroups = words.groupByKey.sortByKey(ascending = true)\n",
    "wordGroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notez que chaque groupe est en fait un [Iterable](http://www.scala-lang.org/api/current/index.html#scala.collection.Iterable), i.e., une abstraction pour une certaine forme de collection. (C'est actuellement une collection privée définie par Spark appelée `CompactBuffer`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11951"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordGroups.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(,CompactBuffer((tamingoftheshrew,1), (asyoulikeit,1), (merrywivesofwindsor,1), (comedyoferrors,1), (midsummersnightsdream,1), (twelfthnight,1), (loveslabourslost,1), (muchadoaboutnothing,1)))\n",
      "(A,CompactBuffer((loveslabourslost,78), (midsummersnightsdream,39), (muchadoaboutnothing,31), (merrywivesofwindsor,38), (comedyoferrors,42), (asyoulikeit,34), (twelfthnight,47), (tamingoftheshrew,59)))\n",
      "(ABOUT,CompactBuffer((muchadoaboutnothing,18)))\n",
      "(ACT,CompactBuffer((asyoulikeit,22), (comedyoferrors,11), (tamingoftheshrew,12), (loveslabourslost,9), (muchadoaboutnothing,17), (twelfthnight,18), (merrywivesofwindsor,23), (midsummersnightsdream,9)))\n",
      "(ADAM,CompactBuffer((asyoulikeit,16)))\n",
      "(ADO,CompactBuffer((muchadoaboutnothing,18)))\n",
      "(ADRIANA,CompactBuffer((comedyoferrors,85)))\n",
      "(ADRIANO,CompactBuffer((loveslabourslost,111)))\n",
      "(AEGEON,CompactBuffer((comedyoferrors,20)))\n",
      "(AEMELIA,CompactBuffer((comedyoferrors,16)))\n",
      "(AEMILIA,CompactBuffer((comedyoferrors,3)))\n",
      "(AEacides,CompactBuffer((tamingoftheshrew,1)))\n",
      "(AEgeon,CompactBuffer((comedyoferrors,7)))\n",
      "(AEgle,CompactBuffer((midsummersnightsdream,1)))\n",
      "(AEmilia,CompactBuffer((comedyoferrors,4)))\n",
      "(AEsculapius,CompactBuffer((merrywivesofwindsor,1)))\n",
      "(AGUECHEEK,CompactBuffer((twelfthnight,2)))\n",
      "(ALL,CompactBuffer((midsummersnightsdream,2), (tamingoftheshrew,2)))\n",
      "(AMIENS,CompactBuffer((asyoulikeit,16)))\n",
      "(ANDREW,CompactBuffer((twelfthnight,104)))\n",
      "(ANGELO,CompactBuffer((comedyoferrors,36)))\n",
      "(ANN,CompactBuffer((merrywivesofwindsor,1)))\n",
      "(ANNE,CompactBuffer((merrywivesofwindsor,27)))\n",
      "(ANTIPHOLUS,CompactBuffer((comedyoferrors,195)))\n",
      "(ANTONIO,CompactBuffer((muchadoaboutnothing,32), (twelfthnight,32)))\n",
      "(ARMADO,CompactBuffer((loveslabourslost,111)))\n",
      "(AS,CompactBuffer((asyoulikeit,24)))\n",
      "(AUDREY,CompactBuffer((asyoulikeit,18)))\n",
      "(Abate,CompactBuffer((midsummersnightsdream,1), (loveslabourslost,1)))\n",
      "(Abbess,CompactBuffer((comedyoferrors,2)))\n"
     ]
    }
   ],
   "source": [
    "wordGroups.take(30).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalement, nettoyons ces `CompactBuffers`. Nous pouvons les convertir en [Vector](http://www.scala-lang.org/api/current/index.html#scala.collection.immutable.Vector) (une collection avec une performance de _O(1)_ pour la plupart des opérations), puis les ordonner par ordre décroissant de count, afin que les lieux qui mentionnent le mot correspondant au _plus_ apparaissent en _premier_ dans la liste. (Réfléchissez à comment vous voudriez qu'un moteur de recherche fonctionne...) \n",
    "\n",
    "Notez que nous utilisons `Vector.sortBy`, et pas un sort provenant de `RDD`. Ce sortBy prend une function qui accepte tous les éléments d'une collection et retourne quelque chose utilisé pour trier la collection. En retournant `(-fileNameCountTuple2._2, fileNameCountTuple2)`, je dis, \"triez par count _décroissant_ en premier, puis triez par le nom du fichier\". `-fileNameCountTuple2._2` cause les counts à être triés par ordre décroissant car je retourne une valeur négative donc les counts plus larges seront inférieurs aux counts plus petits, par exemple `-3 < -2`.\n",
    "\n",
    "Finalement, je prend le `Vector` résultant et je crée un CSV avec les éléments en utilisant la méthode helper `mkString`.\n",
    "\n",
    "Qu'est donc `RDD.mapValues` ? Je pourrais utiliser `RDD.map`,mais je ne change pas les clés (les mots), donc au lieu d'avoir à faire au tuple avec les deux éléments, `mapValues` passe seulement la valeur du tuple et reconstruit un nouveau tuple `(clé,valeur)` avec la nouvelle valeur que ma fonction retourne. Ainsi, `mapValues` est plus pratique à utiliser que `map` quand j'ai un tuple de deux éléments et que je ne modifie pas les clés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "iiFirstPass2 = MapPartitionsRDD[19] at mapValues at <console>:42\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[19] at mapValues at <console>:42"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val iiFirstPass2 = wordGroups.mapValues { iterable => \n",
    "    val vect = iterable.toVector.sortBy { file_count_tup2 => \n",
    "        (-file_count_tup2._2, file_count_tup2._1)\n",
    "    }\n",
    "    vect.mkString(\",\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C'est bon ! Le nombre de records est le même que `wordGroups` (comprenez-vous pourquoi ?), observons donc quelques records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(,(asyoulikeit,1),(comedyoferrors,1),(loveslabourslost,1),(merrywivesofwindsor,1),(midsummersnightsdream,1),(muchadoaboutnothing,1),(tamingoftheshrew,1),(twelfthnight,1))\n",
      "(A,(loveslabourslost,78),(tamingoftheshrew,59),(twelfthnight,47),(comedyoferrors,42),(midsummersnightsdream,39),(merrywivesofwindsor,38),(asyoulikeit,34),(muchadoaboutnothing,31))\n",
      "(ABOUT,(muchadoaboutnothing,18))\n",
      "(ACT,(merrywivesofwindsor,23),(asyoulikeit,22),(twelfthnight,18),(muchadoaboutnothing,17),(tamingoftheshrew,12),(comedyoferrors,11),(loveslabourslost,9),(midsummersnightsdream,9))\n",
      "(ADAM,(asyoulikeit,16))\n",
      "(ADO,(muchadoaboutnothing,18))\n",
      "(ADRIANA,(comedyoferrors,85))\n",
      "(ADRIANO,(loveslabourslost,111))\n",
      "(AEGEON,(comedyoferrors,20))\n",
      "(AEMELIA,(comedyoferrors,16))\n",
      "(AEMILIA,(comedyoferrors,3))\n",
      "(AEacides,(tamingoftheshrew,1))\n",
      "(AEgeon,(comedyoferrors,7))\n",
      "(AEgle,(midsummersnightsdream,1))\n",
      "(AEmilia,(comedyoferrors,4))\n",
      "(AEsculapius,(merrywivesofwindsor,1))\n",
      "(AGUECHEEK,(twelfthnight,2))\n",
      "(ALL,(midsummersnightsdream,2),(tamingoftheshrew,2))\n",
      "(AMIENS,(asyoulikeit,16))\n",
      "(ANDREW,(twelfthnight,104))\n",
      "(ANGELO,(comedyoferrors,36))\n",
      "(ANN,(merrywivesofwindsor,1))\n",
      "(ANNE,(merrywivesofwindsor,27))\n",
      "(ANTIPHOLUS,(comedyoferrors,195))\n",
      "(ANTONIO,(muchadoaboutnothing,32),(twelfthnight,32))\n",
      "(ARMADO,(loveslabourslost,111))\n",
      "(AS,(asyoulikeit,24))\n",
      "(AUDREY,(asyoulikeit,18))\n",
      "(Abate,(loveslabourslost,1),(midsummersnightsdream,1))\n",
      "(Abbess,(comedyoferrors,2))\n"
     ]
    }
   ],
   "source": [
    "iiFirstPass2.take(30).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cela paraît raisonnable.\n",
    "\n",
    "Nous allons maintenant améliorer le code en utilisant une feature très puissante, le _pattern matching_, qui permet de rendre le code à la fois plus concis et facile à comprendre. C'est ma fonctionnalité *préferée* de Scala. \n",
    "\n",
    "Avant cela, essayez de faire d'améliorer le code par vous même.\n",
    "\n",
    "**Exercices:**\n",
    "\n",
    "* Ajoutez un filter pour enlever le mot vide \"\". Vous pouvez faire cela de deux manières différentes, en utilisant [RDD.filter](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD) (cherchez dans [Scaladoc page]((http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD) pour la méthode `filter`), _ou_ en utilisant la méthode similaire implémentée par Scala, [scala.collection.Seq.filter](http://www.scala-lang.org/api/current/index.html#scala.collection.Seq). Les deux versions prennent une fonction _prédicat_, fonction qui retourne  `true` si le record doit être _conservée_ et `false` dans le cas contraire. Pensez-vous qu'un des deux choix est meilleur que l'autre ? Pourquoi ? Ou ces deux choix sont-ils au final similaires ? Les raisons peuvent inclure la compréhension du code et la performance.\n",
    "\n",
    "* Convertissez tous les mots en minuscule. Il faut juste appeler `toLowerCase` sur une string. A quel endroit est-il judicieux de placer cet appel ?\n",
    "\n",
    "J'implémenterais ces deux changements dans les améliorations à suivre en bas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE:** Si vous préférez faire une copie du code dans une nouvelle cellule, utilisez le menu _Insert_ en haut pour ajoutez des cellules. Ou vous pouvez apprendre un autre raccourci clavier `ESC`, puis `A` pour insérer avant ou `B` pour insérer après. Vous pouvez ensuite appuyer sur entrée pour éditer la cellule. Notez la barre d'outil pour configurer le format de la cellule. Cette cellule que vous lisez est par exemple en _Markdown_. Utilisez _Code_ pour les cellules contenant votre code source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern Matching\n",
    "\n",
    "Nous avons étudié un vrai programme et nous avons appris pas mal de Scala. Améliorons ça avec ma fonctionnalité préférée de Scala : _le pattern matching_.\n",
    "\n",
    "Voici la version \"premier passage\" à nouveau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "iiFirstPass1b = MapPartitionsRDD[29] at mapValues at <console>:52\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[29] at mapValues at <console>:52"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val iiFirstPass1b = sc.wholeTextFiles(shakespeare.toString).\n",
    "    flatMap { location_contents_tuple2 => \n",
    "        val words = location_contents_tuple2._2.split(\"\"\"\\W+\"\"\")\n",
    "        val fileName = location_contents_tuple2._1.split(pathSeparator).last\n",
    "        words.map(word => ((word, fileName), 1))\n",
    "    }.\n",
    "    reduceByKey((count1, count2) => count1 + count2).\n",
    "    map { word_file_count_tup3 => \n",
    "        (word_file_count_tup3._1._1, (word_file_count_tup3._1._2, word_file_count_tup3._2)) \n",
    "    }.\n",
    "    groupByKey.\n",
    "    sortByKey(ascending = true).\n",
    "    mapValues { iterable => \n",
    "        val vect = iterable.toVector.sortBy { file_count_tup2 => \n",
    "            (-file_count_tup2._2, file_count_tup2._1)\n",
    "        }\n",
    "        vect.mkString(\",\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, voici une nouvelle implémentation qui utilise le _pattern matching_.\n",
    "\n",
    "J'y ai fait deux autres ajouts : les solutions des derniers exercices, qui retire les mots wides \"\" et corrige les majuscules mélangées, en utilisant les ajouts suivants :\n",
    "\n",
    "* `filter(word => word.size > 0)` pour retirer les mots vides. (En Spark et dans les collections Scala, `filter` a un sens positif : \"qu'est-ce qui doit être retenu ?\"). C'est indiqué par le commentaire `// #1`.\n",
    "* `word.toLowerCase` pour convetir tous les mots uniformément en minuscule, tel que des mots comme HAMLET, Hamlet et hamlet dans le texte original sont traités comme étant le même mot, puisque nous comptons les occurences des mots. Voir le commentaire `// #2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:27: error: not found: value shakespeare\n",
       "val ii1 = sc.wholeTextFiles(shakespeare.toString).\n",
       "                            ^\n",
       "<console>:30: error: value split is not a member of Any\n",
       "            val words = contents.split(\"\"\"\\W+\"\"\").\n",
       "                                 ^\n",
       "<console>:32: error: value split is not a member of Any\n",
       "            val fileName = location.split(pathSeparator).last\n",
       "                                    ^\n",
       "<console>:32: error: not found: value pathSeparator\n",
       "            val fileName = location.split(pathSeparator).last\n",
       "                                          ^\n",
       "<console>:40: error: not found: value ascending\n",
       "    sortByKey(ascending = true).\n",
       "              ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ii1 = sc.wholeTextFiles(shakespeare.toString).\n",
    "    flatMap {\n",
    "        case (location, contents) => \n",
    "            val words = contents.split(\"\"\"\\W+\"\"\").\n",
    "                filter(word => word.size > 0)                      // #1\n",
    "            val fileName = location.split(pathSeparator).last\n",
    "            words.map(word => ((word.toLowerCase, fileName), 1))   // #2\n",
    "    }.\n",
    "    reduceByKey((count1, count2) => count1 + count2).\n",
    "    map { \n",
    "        case ((word, fileName), count) => (word, (fileName, count)) \n",
    "    }.\n",
    "    groupByKey.\n",
    "    sortByKey(ascending = true).\n",
    "    mapValues { iterable => \n",
    "        val vect = iterable.toVector.sortBy { \n",
    "            case (fileName, count) => (-count, fileName) \n",
    "        }\n",
    "        vect.mkString(\",\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparez avec les solutions d'exercice plus haut. J'ai ajouté le filtrage dans la foncrtion passée à `flatMap`. Mon choix réduit le nombre d'enregistrement en sortie de `flatMap` d'au plus un enregistrement par ligne en entrée, ce qui ne devrait pas avoir un impact significatif sur les performance. Le filtrage en lui-même ajoute un peu de surcharge.\n",
    "\n",
    "La façon dont Spark implémente des étapes comme `map`, `flatMap`, `filter` entraine la même surcharge que si j'utilisais `RDD.filter`. Notez que nous pouvons aussi faire un filtrage plus tard dans la pipeline, après `groupByKey`, par exemple. Donc, quelque soit l'approche que vous implémentez, c'est probablement bien. Vous pouvez réaliser un profiling de performancedes différentes approches, mais vous ne trouverez pas de différence significative à moins d'utiliser un data set d'entrée très important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vérifions si nous obtenons toujours des résultats raisonable. Maintenant je vais utiliser l'API [DataFrame](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrame) de Spark pour ses options d'affichage pratique. `DataFrames` font partis de [Spark SQL](http://spark.apache.org/docs/latest/sql-programming-guide.html). \n",
    "\n",
    "Tout d'abord, nous avons besoin de créer une instance de [SQLContext](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SQLContext) qui nous permettra d'accéder à ces fonctionnalités."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sqlContext = org.apache.spark.sql.SQLContext@27a9da47\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "warning: there was one deprecation warning; re-run with -deprecation for details\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SQLContext@27a9da47"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sqlContext = new SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, nous convertissons le `RDD` en `DataFrame` avec `sqlContext.createDataFrame` et nous utilisons `toDF` (convertir vers un autre `DataFrame` ?) avec un nouveau pour chaque \"colonne\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ii1DF = [word: string, locations_counts: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[word: string, locations_counts: string]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ii1DF = sqlContext.createDataFrame(ii1).toDF(\"word\", \"locations_counts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La _cellule magique_ `%%dataframe` founit un sympathique affichage tabulaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>word</th><th>locations_counts</th></tr><tr><td>a</td><td>(loveslabourslost,507),(merrywivesofwindsor,494),(muchadoaboutnothing,492),(asyoulikeit,461),(tamingoftheshrew,445),(twelfthnight,416),(midsummersnightsdream,281),(comedyoferrors,254)</td></tr><tr><td>abandon</td><td>(asyoulikeit,4),(tamingoftheshrew,1),(twelfthnight,1)</td></tr><tr><td>abate</td><td>(loveslabourslost,1),(midsummersnightsdream,1),(tamingoftheshrew,1)</td></tr><tr><td>abatement</td><td>(twelfthnight,1)</td></tr><tr><td>abbess</td><td>(comedyoferrors,8)</td></tr><tr><td>abbey</td><td>(comedyoferrors,9)</td></tr><tr><td>abbominable</td><td>(loveslabourslost,1)</td></tr><tr><td>abbreviated</td><td>(loveslabourslost,1)</td></tr><tr><td>abed</td><td>(asyoulikeit,1),(twelfthnight,1)</td></tr><tr><td>abetting</td><td>(comedyoferrors,1)</td></tr></table>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%dataframe\n",
    "ii1DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, maintenant explorons la nouvelle implémentation. Je commence comme précédemment, en appelant `wholeTextFiles` :\n",
    "\n",
    "```scala\n",
    "val ii = sc.wholeTextFiles(shakespeare.toString).\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction que je passe à `flatMap` maintenant ressemble à ça :\n",
    "\n",
    "```scala\n",
    "flatMap { \n",
    "    case (location, contents) => \n",
    "        val words = contents.split(\"\"\"\\W+\"\"\").\n",
    "            filter(word => word.size > 0)                      // #1\n",
    "        val fileName = location.split(pathSeparator).last\n",
    "        words.map(word => ((word.toLowerCase, fileName), 1))   // #2\n",
    "}.\n",
    "```\n",
    "\n",
    "Comparez la avec la version précédente (en ignorant les améliorations pour les mots vides et les majuscules, marqués avec les commentaires \\#1 et \\#2) :\n",
    "\n",
    "```scala\n",
    "flatMap { location_contents_tuple2 => \n",
    "    val words = location_contents_tuple2._2.split(\"\"\"\\W+\"\"\")\n",
    "    val fileName = location_contents_tuple2._1.split(pathSeparator).last\n",
    "    words.map(word => ((word, fileName), 1))\n",
    "}.\n",
    "```\n",
    "\n",
    "À la place de `location_contents_tuple2` un nom de variable pour tout le tuple, J'ai écrit `case (location, contents)`. Le mot-clé `case` indique que j'effectue un _pattern match_ sur l'objet passé à la fonction. Si c'est un tuple de deux éléments (et je sais que nous serons toujours dans ce cas), alors on _extrait_ le premier élément et l'assigner à une variable nommée `location` puis on extrait le second élément et on l'assigne à un variable nommée `contents`.\n",
    "\n",
    "Maintenant, au lieu d'accèder à la position (`location`) et au contenu (`content`) avec la syntaxe quelque peu obscure et verbeuse `location_contents_tuple2._1` et `location_contents_tuple2._2`, respectivement, j'utilise des noms éloquents, `location` et `content`. Le code devient plus concis et plus lisible.\n",
    "\n",
    "Je vais ci-dessous explorer d'avantage le pattern matching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'étape `reduceByKey` reste inchangé :\n",
    "\n",
    "```scala\n",
    "reduceByKey((count1, count2) => count1 + count2).\n",
    "```\n",
    "\n",
    "Pour plus de clarté, ceci n'est pas une expression de pattern-matching ; il n'y a de mot-clé `case`. C'est juste une fonction \"régulière\" qui prend deux paramètres, pour les deux éléments que je veux additionner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mon amélioration préféré est dans la ligne suivante :\n",
    "\n",
    "```scala\n",
    "map { \n",
    "    case ((word, fileName), count) => (word, (fileName, count)) \n",
    "}.\n",
    "```\n",
    "\n",
    "Comparez la avec l'obscure version précédente :\n",
    "\n",
    "```scala\n",
    "map { word_file_count_tup3 => \n",
    "    (word_file_count_tup3._1._1, (word_file_count_tup3._1._2, word_file_count_tup3._2)) \n",
    "}.\n",
    "```\n",
    "\n",
    "La nouvelle implémentation apporte de la clarté sur ce que je suis en train de faire : juste déplacer des parenthèses ! C'est tout ce que ça prend pour aller des clés `(word, fileName)` avec `count` en tant que valeur à des clés `word` et `(fileName, count)` en tant que valeur. Notez que le pattern matchimg fonctionne très bien avec les structures imbriquées, comme `((word, fileName), count)`.\n",
    "\n",
    "J'espère que vous pouvez apprécier à quel point cette expression est élégante et concise ! NOtez comment je pense à la transformation suivante que j'ai besoin de faire en préparation pour le group-by final, de basculer de `((word, fileName), count)` à `(word, (fileName, count))` et _je l'ai simplement écrit exactement tel que je me l'étais représenté !_\n",
    "\n",
    "Du code comme celui-ci fait de l'écriture de code Scala Spark une expérience sublime pour moi. J'espère que c'est le cas pour vous aussi ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les deux prochaines expressions sont inchangées :\n",
    "\n",
    "```scala\n",
    "groupByKey.\n",
    "sortByKey(ascending = true).\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le `mapValues` final utilise maintenant le pattern matching pour trier le `Vector` dans chaque enregistrement :\n",
    "\n",
    "```scala\n",
    "mapValues { iterable => \n",
    "    val vect = iterable.toVector.sortBy { \n",
    "        case (fileName, count) => (-count, fileName) \n",
    "    }\n",
    "    vect.mkString(\",\")\n",
    "}\n",
    "```\n",
    "\n",
    "Comparé le à la version originale, c'est à nouveau plus facile à lire :\n",
    "\n",
    "```scala\n",
    "mapValues { iterable => \n",
    "    val vect = iterable.toVector.sortBy { file_count_tup2 => \n",
    "        (-file_count_tup2._2, file_count_tup2._1)\n",
    "    }\n",
    "    vect.mkString(\",\")\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction que j'ai passé à `sortBy` retourne un tuple utilisé pour trié, avec `-count` pour forcé le trie numérique _descendant_ (le plus grand en premier) et `fileName` pour trié dans un deuxième temps par nom de fichier, pour les décomptes identiques. Je pourrais ignorer le trie par nom de fichier et simplement retourner `-count` (sans tuple). Cependant, si vous avez besoin d'une sortie reproductible dans un système réparti comme Spark, par exemple pour la validation par tests unitaires, alors le trie secondaire par nom de fichier utile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Final Version: Supporting SQL Queries\n",
    "To play with some more Spark, let's write SQL queries to explore the resulting data. \n",
    "\n",
    "To do this, let's first refine the output. Instead of creating a string for the list of `(location,count)` pairs, which is opaque to our SQL schema (i.e., just a string), let's \"unzip\" the collection into two arrays, one for the `locations` and one for the `counts`. That way, if we ask for the first element of each array, we'll have nicely separate fields that work better with Spark SQL queries.\n",
    "\n",
    "\"Zipping\" and \"unzipping\" work like a mechanical zipper. If I have a collection of tuples, say `List[(String, Int)]`, I convert this single collection of \"zippered\" values into two collections (in a tuple) of single values, `(List[String], List[Int])`. Zipping is the inverse operation.\n",
    "\n",
    "Here is our final implementation, `ii1` rewritten with this change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ii = MapPartitionsRDD[54] at map at <console>:55\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[54] at map at <console>:55"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ii = sc.wholeTextFiles(shakespeare.toString).\n",
    "    flatMap {\n",
    "        case (location, contents) => \n",
    "            val words = contents.split(\"\"\"\\W+\"\"\").\n",
    "                filter(word => word.size > 0)                      // #1\n",
    "            val fileName = location.split(pathSeparator).last\n",
    "            words.map(word => ((word.toLowerCase, fileName), 1))   // #2\n",
    "    }.\n",
    "    reduceByKey((count1, count2) => count1 + count2).\n",
    "    map { \n",
    "        case ((word, fileName), count) => (word, (fileName, count)) \n",
    "    }.\n",
    "    groupByKey.\n",
    "    sortByKey(ascending = true).\n",
    "    map {                         // Must use map now, because we'll format new records. \n",
    "      case (word, iterable) =>    // Hence, pattern match on the whole input record.\n",
    "\n",
    "        val vect = iterable.toVector.sortBy { \n",
    "            case (fileName, count) => (-count, fileName) \n",
    "        }\n",
    "\n",
    "        // Use `Vector.unzip`, which returns a single, two element tuple, where each\n",
    "        // element is a collection, one for the locations and one for the counts. \n",
    "        // I use pattern matching to extract these two collections into variables.\n",
    "        val (locations, counts) = vect.unzip  \n",
    "        \n",
    "        // Lastly, I'll compute the total count across all locations and return \n",
    "        // a new record with all four fields. The `reduceLeft` method takes a function\n",
    "        // that knows how to \"reduce\" the collection down to a final value, working \n",
    "        // from the left.\n",
    "        val totalCount = counts.reduceLeft((n1,n2) => n1+n2)\n",
    "        \n",
    "        (word, totalCount, locations, counts)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've changed the ending `mapValues` call to a `map` call, because we'll construct entirely new records, not just new values with the same keys. Hence the full records, two-element tuples are passed in, rather than just the values, so we'll pattern match on the tuple:\n",
    "\n",
    "\n",
    "```scala\n",
    "    map {                         // Must use map now, because we'll format new records.\n",
    "      case (word, iterable) =>    // Hence, pattern match on the whole input record.\n",
    "\n",
    "        val vect = iterable.toVector.sortBy { \n",
    "            case (fileName, count) => (-count, fileName) \n",
    "        }\n",
    "```\n",
    "\n",
    "\n",
    "We have a `Vector[String, Int]` of two-element tuples `(fileName, count)`. We use `Vector.unzip` to create a single, two element tuple, where each element is now a collection, one for the locations and one for the counts. The type is `(Vector[String], Vector[Int])`.\n",
    "\n",
    "We can also use pattern matching with assignment! We immediately decompose the two-element tuple:\n",
    "\n",
    "```scala\n",
    "        // I use pattern matching to extract these two collections into variables.\n",
    "        val (locations, counts) = vect.unzip  \n",
    "```\n",
    "\n",
    "Finally, it's convenient to know how many locations and counts we have, so we'll compute another new column for the their count and format a four-element tuple as the final output.\n",
    "\n",
    "```scala\n",
    "        // Lastly, I'll compute the total count across all locations and return \n",
    "        // a new record with all four fields. The `reduceLeft` method takes a function\n",
    "        // that knows how to \"reduce\" the collection down to a final value, working \n",
    "        // from the left.\n",
    "        val totalCount = counts.reduceLeft((n1,n2) => n1+n2)\n",
    "\n",
    "        (word, totalCount, locations, counts)\n",
    "    }\n",
    "```    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay! Now let's create a [DataFrame](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrame) with this data. The `toDF` method just returns the same `DataFrame`, but with appropriate names for the columns, instead of the synthesized names that `createDataFrame` generates (e.g., `_c1`, `_c2`, etc.)\n",
    "\n",
    "Caching the `DataFrame` in memory prevents Spark from recomputing `ii` from the input files _every time_ I write a query!\n",
    "\n",
    "Finally, to use SQL, I need to \"register\" a temporary table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "iiDF = [word: string, total_count: int ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "warning: there was one deprecation warning; re-run with -deprecation for details\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[word: string, total_count: int ... 2 more fields]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val iiDF = sqlContext.createDataFrame(ii).toDF(\"word\", \"total_count\", \"locations\", \"counts\")\n",
    "iiDF.cache\n",
    "iiDF.registerTempTable(\"inverted_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remind ourselves of the schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- word: string (nullable = true)\n",
      " |-- total_count: integer (nullable = false)\n",
      " |-- locations: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- counts: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iiDF.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following SQL query extracts the top location by count for each word, as well as the total count across all locations for the word. The Spark SQL dialect supports Hive SQL syntax for extracting elements from arrays, maps, and structs ([details](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF#LanguageManualUDF-CollectionFunctions)). Here I access the first element (index zero) from each array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+-----------+-----------+--...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "+-----------+-----------+----------------+---------+\n",
       "|       word|total_count|    top_location|top_count|\n",
       "+-----------+-----------+----------------+---------+\n",
       "|          a|       3350|loveslabourslost|      507|\n",
       "|    abandon|          6|     asyoulikeit|        4|\n",
       "|      abate|          3|loveslabourslost|        1|\n",
       "|  abatement|          1|    twelfthnight|        1|\n",
       "|     abbess|          8|  comedyoferrors|        8|\n",
       "|      abbey|          9|  comedyoferrors|        9|\n",
       "|abbominable|          1|loveslabourslost|        1|\n",
       "|abbreviated|          1|loveslabourslost|        1|\n",
       "|       abed|          2|     asyoulikeit|        1|\n",
       "|   abetting|          1|  comedyoferrors|        1|\n",
       "+-----------+-----------+----------------+---------+\n",
       "only showing top 10 rows\n",
       "\n"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%SQL\n",
    "SELECT word, total_count, locations[0] AS top_location, counts[0] AS top_count \n",
    "FROM inverted_index "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the output formatting for the `%%SQL` \"cell magic\" is not configurable. The `%%DataFrame` magic handles variable width layout and also provides more display options. First, to see its options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "%%dataframe [arguments]\n",
       "DATAFRAME_CODE\n",
       "\n",
       "DATAFRAME_CODE can be any numbered lines of code, as long as the\n",
       "last line is a reference to a variable which is a DataFrame.\n",
       "    Option    Description                       \n",
       "------    -----------                       \n",
       "--limit   The number of records to return   \n",
       "            (default: 10)                   \n",
       "--output  The type of the output: html, csv,\n",
       "            json (default: html)            \n"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now here's the previous query again, with the a `WHERE` clause added for good measure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topLocations = [word: string, total_count: int ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[word: string, total_count: int ... 2 more fields]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val topLocations = sqlContext.sql(\"\"\"\n",
    "    SELECT word,  total_count, locations[0] AS top_location, counts[0] AS top_count\n",
    "    FROM inverted_index \n",
    "    WHERE word LIKE '%love%' OR word LIKE '%hate%'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the `%%dataframe` _magic_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>word</th><th>total_count</th><th>top_location</th><th>top_count</th></tr><tr><td>beloved</td><td>11</td><td>tamingoftheshrew</td><td>4</td></tr><tr><td>cloven</td><td>1</td><td>loveslabourslost</td><td>1</td></tr><tr><td>cloves</td><td>1</td><td>loveslabourslost</td><td>1</td></tr><tr><td>glove</td><td>3</td><td>loveslabourslost</td><td>2</td></tr><tr><td>glover</td><td>1</td><td>merrywivesofwindsor</td><td>1</td></tr><tr><td>gloves</td><td>5</td><td>merrywivesofwindsor</td><td>3</td></tr><tr><td>hate</td><td>22</td><td>midsummersnightsdream</td><td>9</td></tr><tr><td>hated</td><td>6</td><td>midsummersnightsdream</td><td>4</td></tr><tr><td>hateful</td><td>5</td><td>midsummersnightsdream</td><td>3</td></tr><tr><td>hates</td><td>5</td><td>asyoulikeit</td><td>2</td></tr><tr><td>hateth</td><td>1</td><td>midsummersnightsdream</td><td>1</td></tr><tr><td>love</td><td>662</td><td>loveslabourslost</td><td>121</td></tr><tr><td>loved</td><td>38</td><td>asyoulikeit</td><td>13</td></tr><tr><td>lovely</td><td>15</td><td>midsummersnightsdream</td><td>7</td></tr><tr><td>lover</td><td>33</td><td>asyoulikeit</td><td>14</td></tr><tr><td>lovers</td><td>31</td><td>midsummersnightsdream</td><td>17</td></tr><tr><td>loves</td><td>51</td><td>muchadoaboutnothing</td><td>10</td></tr><tr><td>lovest</td><td>8</td><td>tamingoftheshrew</td><td>3</td></tr><tr><td>loveth</td><td>2</td><td>loveslabourslost</td><td>1</td></tr><tr><td>unloved</td><td>1</td><td>midsummersnightsdream</td><td>1</td></tr><tr><td>whate</td><td>4</td><td>tamingoftheshrew</td><td>3</td></tr><tr><td>whatever</td><td>1</td><td>tamingoftheshrew</td><td>1</td></tr></table>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%dataframe --limit 100\n",
    "topLocations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A _natural language processing_ (NLP) expert might tell you that _love_, _loved_, _loves_, etc. are really the same word, because they are different conjugations of the verb _to love_ and _love_ is a noun, too. Similarly, should _gloves_ (plural) and _glove_ (singular) be handled differently?\n",
    "\n",
    "What we really should do is extract the _stems_ of these words and use those instead. NLP toolkits handle this _stemming_ for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's also a useful `show` method on `DataFrames`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+--------------------+---------+\n",
      "|   word|total_count|        top_location|top_count|\n",
      "+-------+-----------+--------------------+---------+\n",
      "|beloved|         11|    tamingoftheshrew|        4|\n",
      "| cloven|          1|    loveslabourslost|        1|\n",
      "| cloves|          1|    loveslabourslost|        1|\n",
      "|  glove|          3|    loveslabourslost|        2|\n",
      "| glover|          1| merrywivesofwindsor|        1|\n",
      "| gloves|          5| merrywivesofwindsor|        3|\n",
      "|   hate|         22|midsummersnightsd...|        9|\n",
      "|  hated|          6|midsummersnightsd...|        4|\n",
      "|hateful|          5|midsummersnightsd...|        3|\n",
      "|  hates|          5|         asyoulikeit|        2|\n",
      "| hateth|          1|midsummersnightsd...|        1|\n",
      "|   love|        662|    loveslabourslost|      121|\n",
      "|  loved|         38|         asyoulikeit|       13|\n",
      "| lovely|         15|midsummersnightsd...|        7|\n",
      "|  lover|         33|         asyoulikeit|       14|\n",
      "| lovers|         31|midsummersnightsd...|       17|\n",
      "|  loves|         51| muchadoaboutnothing|       10|\n",
      "| lovest|          8|    tamingoftheshrew|        3|\n",
      "| loveth|          2|    loveslabourslost|        1|\n",
      "|unloved|          1|midsummersnightsd...|        1|\n",
      "+-------+-----------+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topLocations.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, it truncates column widths and only prints 20 rows. You can override both:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+---------------------+---------+\n",
      "|word    |total_count|top_location         |top_count|\n",
      "+--------+-----------+---------------------+---------+\n",
      "|beloved |11         |tamingoftheshrew     |4        |\n",
      "|cloven  |1          |loveslabourslost     |1        |\n",
      "|cloves  |1          |loveslabourslost     |1        |\n",
      "|glove   |3          |loveslabourslost     |2        |\n",
      "|glover  |1          |merrywivesofwindsor  |1        |\n",
      "|gloves  |5          |merrywivesofwindsor  |3        |\n",
      "|hate    |22         |midsummersnightsdream|9        |\n",
      "|hated   |6          |midsummersnightsdream|4        |\n",
      "|hateful |5          |midsummersnightsdream|3        |\n",
      "|hates   |5          |asyoulikeit          |2        |\n",
      "|hateth  |1          |midsummersnightsdream|1        |\n",
      "|love    |662        |loveslabourslost     |121      |\n",
      "|loved   |38         |asyoulikeit          |13       |\n",
      "|lovely  |15         |midsummersnightsdream|7        |\n",
      "|lover   |33         |asyoulikeit          |14       |\n",
      "|lovers  |31         |midsummersnightsdream|17       |\n",
      "|loves   |51         |muchadoaboutnothing  |10       |\n",
      "|lovest  |8          |tamingoftheshrew     |3        |\n",
      "|loveth  |2          |loveslabourslost     |1        |\n",
      "|unloved |1          |midsummersnightsdream|1        |\n",
      "|whate   |4          |tamingoftheshrew     |3        |\n",
      "|whatever|1          |tamingoftheshrew     |1        |\n",
      "+--------+-----------+---------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topLocations.show(numRows = 40, truncate = false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** Named Parameters\n",
    "\n",
    "> I used _named parameters_ here, `show(numRows = 40, truncate = false)`, for legibility. They are optional in Scala, as long as you pass the values in the same order as the parameters are declared. You can also use named parameters to write the arguments in any order you want, not just declaration order. So, I could have just written `(40, false)`, but then you would rightly wonder what `false` means in this context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercises:** \n",
    "\n",
    "See the <a href=\"#ExerciseSolutions\">Appendix</a> for the solutions to the first two exercises.\n",
    "\n",
    "* The `glove`, `gloves`, `whate` and `whatever` aren't really the `love` and `hate` we wanted ;) How might you change the query so be more specific.\n",
    "* Modify the query to return the top two locations and counts.\n",
    "* Before moving on, try writing other queries. Edit the query in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------+\n",
      "|word       |total_count|locations                                                                                                                                       |counts                                  |\n",
      "+-----------+-----------+------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------+\n",
      "|a          |3350       |[loveslabourslost, merrywivesofwindsor, muchadoaboutnothing, asyoulikeit, tamingoftheshrew, twelfthnight, midsummersnightsdream, comedyoferrors]|[507, 494, 492, 461, 445, 416, 281, 254]|\n",
      "|abandon    |6          |[asyoulikeit, tamingoftheshrew, twelfthnight]                                                                                                   |[4, 1, 1]                               |\n",
      "|abate      |3          |[loveslabourslost, midsummersnightsdream, tamingoftheshrew]                                                                                     |[1, 1, 1]                               |\n",
      "|abatement  |1          |[twelfthnight]                                                                                                                                  |[1]                                     |\n",
      "|abbess     |8          |[comedyoferrors]                                                                                                                                |[8]                                     |\n",
      "|abbey      |9          |[comedyoferrors]                                                                                                                                |[9]                                     |\n",
      "|abbominable|1          |[loveslabourslost]                                                                                                                              |[1]                                     |\n",
      "|abbreviated|1          |[loveslabourslost]                                                                                                                              |[1]                                     |\n",
      "|abed       |2          |[asyoulikeit, twelfthnight]                                                                                                                     |[1, 1]                                  |\n",
      "|abetting   |1          |[comedyoferrors]                                                                                                                                |[1]                                     |\n",
      "+-----------+-----------+------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sql1 = [word: string, total_count: int ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[word: string, total_count: int ... 2 more fields]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sql1 = sqlContext.sql(\"\"\"\n",
    "    SELECT * FROM inverted_index\n",
    "\"\"\")\n",
    "sql1.show(10, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing the \"Stop Words\"\n",
    "Did you notice that one record we saw above was for the word \"a\". Not very useful if you're using this data for text searching, _sentiment mining_, etc. So called _stop words_, like _a_, _an_, _the_, _he_, _she_, _it_, etc., could also be removed.\n",
    "\n",
    "Recall the `filter` logic I added to remove \"\", `word => word.size > 0`. I could replace it with `word => keep(word)`, where `keep` is a method that does any additional filtering I want, like removing stop words.\n",
    "\n",
    "**Exercise:**\n",
    "\n",
    "* Implement the `keep(word: String):Boolean` method and change the `filter` function to use it. Have `keep` return `false` for a small, hard-coded list of stop words (make up your own list or search for one). (See the <a href=\"#ExerciseSolutions\">Appendix</a> for the solution.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plus sur la syntaxe de Pattern Matching\n",
    "Nous avons seulement gratté la surface du pattern matching. Découvrons la plus en détail.\n",
    "\n",
    "Ici nous avons une autre fonction anonyme qui utilise du pattern matching qui étend la fonction précédente que nous avons passé à `flatMap` :\n",
    "\n",
    "```scala\n",
    "{\n",
    "    case (location, \"\") => \n",
    "        Array.empty[((String, String), Int)]  // Return an empty array\n",
    "    case (location, contents) => \n",
    "        val words = contents.split(\"\"\"\\W+\"\"\")\n",
    "        val fileName = location.split(pathSep).last\n",
    "        words.map(word => ((word, fileName), 1))\n",
    "}.\n",
    "```\n",
    "\n",
    "Vous pouvez avoir plusieurs clauses `case`, certains d'eux pouvant correspondre sur des valeurs littérales spécifiques (\"\" dans ce cas-là) et d'autres qui sont plus générales. La première clause case s'occupe des fichiers sans contenu. La seconde clause est la même qu'avant.\n",
    "\n",
    "Le pattern matching est _eager_. L'ordre est important car le premier match réalisé sera celui écrit en premier. Si vous inversez l'ordre ici, alors la `case (location, \"\")` ne marchera jamais et le compilateur vous affichera un avertissement \"unreachable code\".\n",
    "\n",
    "Notez que vous n'avez pas à mettre les lignes après le `=>` dans les accolades `{...}` (bien que vous pouvez le faire). Les mots-clés `=>` et `case` (ou le dernier `}`) sont suffisants pour délimiter ces blocs. Aussi pour un bloc contenant une seule expression, comme pour la première clause, vous pouvez mettre l'expression sur la même ligne après le `=>` si vous voulez (et si ça rentre).\n",
    "\n",
    "FInalement si aucun des case n'est réalisé, alors une exception [MatchError](http://www.scala-lang.org/api/current/index.html#scala.MatchError) exception est lancée. Dans notre cas, nous savons _toujours_ que nous avons des tuples à deux éléments, ainsi les exemples jusqu'ici sont corrects.\n",
    "\n",
    "Voici un exemple final imaginé pour illustrer ce qui est possible, en utilisant une séquence d'objets de types différent :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found an Int:   1\n",
      "Found a Double: 3.14159\n",
      "Found a Long:   2\n",
      "Found a Float:  4.4\n",
      "Found a two-element tuple with elements of arbitrary type: (one, 1)\n",
      "Found a two-element tuple with elements of arbitrary type: (404.0, boo)\n",
      "Found a three-element tuple with 1st and 3th elements: (11, 12) and 31\n",
      "Found something else: hello\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "stuff = List(1, 3.14159, 2, 4.4, (one,1), (404.0,boo), ((11,12),21,31), hello)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "List(1, 3.14159, 2, 4.4, (one,1), (404.0,boo), ((11,12),21,31), hello)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val stuff = Seq(1, 3.14159, 2L, 4.4F, (\"one\", 1), (404F, \"boo\"), ((11, 12), 21, 31), \"hello\")\n",
    "\n",
    "stuff.foreach {\n",
    "    case i: Int               => println(s\"Found an Int:   $i\")\n",
    "    case l: Long              => println(s\"Found a Long:   $l\")\n",
    "    case f: Float             => println(s\"Found a Float:  $f\")\n",
    "    case d: Double            => println(s\"Found a Double: $d\")\n",
    "    case (x1, x2) => \n",
    "        println(s\"Found a two-element tuple with elements of arbitrary type: ($x1, $x2)\")\n",
    "    case ((x1a, x1b), _, x3) => \n",
    "        println(s\"Found a three-element tuple with 1st and 3th elements: ($x1a, $x1b) and $x3\")\n",
    "    case default              => println(s\"Found something else: $default\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelques notes.\n",
    "* Un littéral comme `1`est inféré en tant que `Int`, tandis que `3.14159` est inféré en tant que `Double`. Ajoutez `L` ou `F`, pour inférer respectivement `Long` ou `Float` à la place.\n",
    "* Notez comment nous avons mélangé la vérifaction de types spécifiques, par exemple `i: Int` avec des types moins typés comme `(x1, x2)`, qui attend un tuple à deux éléments. Tous les mots `i`, `l`, `f`, `d`, `x1`, `x2`, `x3`, et `default` sont des noms de variables arbitraires. `default` n'est pas un mot-clé, mais un choix arbitraire pour le nom d'une variable. Nous pourrions utiliser ce que nous voulons.\n",
    "* La dernière clause `default` définit une variable sans information de type. Ainsi, cela match _tout_, ce qui est la raison pour laquelle cette clause doit apparaître en dernier. C'est la syntaxe utilisée quand vous n'êtes pas sûr du type des choses que vous être en train de faire correspondre et que vous voulez éviter une possible [MatchError](http://www.scala-lang.org/api/current/index.html#scala.MatchError).\n",
    "* Si vous voulez matcher quelque chose qui _existe_, mais dont vous n'avez pas besoin de mettre dans une variable, alors utilisez `_` comme dans l'exemple du tuple à trois éléments.\n",
    "* L'exemple du tuple à trois éléments montre que \"l'imbriquage\" arbitraire d'expressions est supportée, où le premier élément attendu est un tuple de deux éléments.\n",
    "\n",
    "Toutes les fonctions anonymes que nous avons vues qui utilisent ces clauses de pattern matching ont ce format :\n",
    "\n",
    "```scala\n",
    "{ \n",
    "    case firstCase => ...\n",
    "    case secondCase => ...\n",
    "    ... \n",
    "}```\n",
    "\n",
    "Ce format a un nom spécial. Il est appelé _fonction partielle_. Tout ce que cela veut dire est seulement que nous \"promettons\" d'accepter des arguments qui correspondent à au moins une de nos clauses `case` et non pas n'importe quelle entrée.\n",
    "\n",
    "L'autre type de fonction anonyme que nous avons vu est la _fonction totale_.\n",
    "\n",
    "Souvenez-vous lorsque il a été dit que les fonctions totales peuvent utiliser soit `(...)` ou `{...}` autour d'elles, en fonction du \"look\" que vous voulez leur donner. Pour les _fonction partielles_, vous _devez_ utiliser `{...}`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Souvenez-vous que nous avons utilisé du pattern matching avec affectation :\n",
    "\n",
    "```scala\n",
    "val (locations, counts) = vect.unzip  \n",
    "```\n",
    "[Vector.unzip](http://www.scala-lang.org/api/current/#scala.collection.immutable.Vector) retourne un tuple de deux éléments, où chaque élément est une collection. Nous matchons sur ce tuple et affectons chaque pièce à une variable. Voici un autre exemple imaginé avec un tuple comportant des éléments imbriqués."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A, B, C1, C2, D\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "a = A\n",
       "b = B\n",
       "c1 = C1\n",
       "c2 = C2\n",
       "d = D\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "D"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val (a, (b, (c1, c2), d)) = (\"A\", (\"B\", (\"C1\", \"C2\"), \"D\"))\n",
    "println(s\" $a, $b, $c1, $c2, $d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essayez d'ajouter un élément `\"E\"` au côté droit du tuple sans changer le côté gauche. Que se passe-t-il ?\n",
    "Essayez d'enlever le `\"D\"` et le `\"E\"`. Que se passe-t-il maintenant ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous reviendrons à un dernier exemple de pattern matching quand nous discuterons des _case classes_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scala's Object Model\n",
    "Scala is a _hybrid_, object-oriented and functional programming language. The philosophy of Scala is that you exploit object orientation for encapsulation of details, i.e., _modularity_, but use functional programming for its logical precision when implementing those details. Most of what we've seen so far falls into the functional programming camp. Much of data manipulation and analysis is really Mathematics. Functional programming tries to stay close to how functions and values work in Mathematics.\n",
    "\n",
    "However, when writing non-trivial Spark programs, it's occasionally useful to exploit the object-oriented features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes vs. Instances\n",
    "Scala utilise la même différenciation entre les classes et les instances que vous trouverez en Java. Les classes sont comme des _templates_ utilisées pour créer des instances.\n",
    "\n",
    "Nous avons parlé des _types_ de diverses choses, comme `word` qui est un `String` et `totalCount` qui est un `Int`. Une classe définit un _type_ aussi.\n",
    "\n",
    "Voici un exemple d'une classe que nous pourrions utiliser pour représenter l'index inversé des lignes que nous venons de créer :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class IIRecord1\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "hello,3,[one,two],[1,2]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class IIRecord1(\n",
    "    word: String, \n",
    "    total_count: Int, \n",
    "    locations: Array[String], \n",
    "    counts: Array[Int]) {\n",
    "    \n",
    "    /** CSV formatted string, but use [a,b,c] for the arrays */\n",
    "    override def toString: String = {\n",
    "        val locStr = locations.mkString(\"[\", \",\", \"]\")  // i.e., \"[a,b,c]\"\n",
    "        val cntStr = counts.mkString(\"[\", \",\", \"]\")  // i.e., \"[1,2,3]\"\n",
    "        s\"$word,$total_count,$locStr,$cntStr\"\n",
    "    }\n",
    "}\n",
    "\n",
    "new IIRecord1(\"hello\", 3, Array(\"one\", \"two\"), Array(1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quand nous définissons une classe, la liste des arguments après le nom de la classe est la liste des arguments pour le _constructeur primaire_. Vous pouvez définir des constructeurs secondaires aussi mais ce n'est pas très commun de le faire pour des raisons que nous allons voir sous peu.\n",
    "\n",
    "Notez que quand vous overridez une méthode qui est définie dans une classe parent comme `Object.toString` de Java, Scala a besoin que vous ajoutiez le mot-clé `override`.\n",
    "\n",
    "Nous avons crée une _instance_ de `IIRecord1` en utilisant `new` comme en Java.\n",
    "\n",
    "Finalement, remarque supplémentaire, nous avons utilisé jusqu'à ici beaucoup de `Int` (entiers) pour les différents counts, mais en réalité dans le \"big data\" nous devrions probablement utiliser des `Long`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objets\n",
    "\n",
    "Nous avons intentionnellement utilisé le mot _instance_ pour les choses que nous créeons des classes. Cela est dû au fait que Scala possède la fonctionnalité de créer seulement une instance de la classe avec [Singleton Design Pattern](https://en.wikipedia.org/wiki/Singleton_pattern). Dans ce cas, il faudra utiliser le mot-clé `object`.\n",
    "\n",
    "En Java par exemple vous devez définir une classe avec la méthode `static void main(String[] arguments)` comme point d'entrée de votre programme. En Scala vous pouvez utiliser un `object` qui contiendra le `main` comme montré dans l'exemple suivant :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined object MySparkJob\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "object MySparkJob {\n",
    "\n",
    "    val greeting = \"Hello Spark!\"\n",
    "    \n",
    "    def main(arguments: Array[String]) = {\n",
    "        println(greeting)\n",
    "        \n",
    "        // Create your SparkContext, etc., etc.\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme pour les classes, le nom de l'objet peut être ce que vous voulez. Il n'y a pas de mot-clé `static` en Scala. A la place d'ajouter des méthodes et champs `static` dans les classes comme en Java, vous pouvez les mettre dans des objets comme montré ici.\n",
    "\n",
    "> **NOTE:** Parce que le compilateur Scala doit générer du byte code valide pour la JVM, ces définitions sont converties en un équivalent Java qui contient des définitions static."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Classes\n",
    "Tuples are handy for representing records and for decomposing them with pattern matching. However, it would be nice if the fields were _named_, as well as _typed_. A good use for a class, like our `IIRecord1` above, us to represent this structure and give us named fields. Let's now refine that class definition to exploit some extra, very useful features in Scala.\n",
    "\n",
    "Consider the following definition of a _case class_ that represents our final record type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class IIRecord\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "case class IIRecord(\n",
    "    word: String, \n",
    "    total_count: Int = 0, \n",
    "    locations: Array[String] = Array.empty, \n",
    "    counts: Array[Int] = Array.empty) {\n",
    "\n",
    "    /** \n",
    "     * Different than our CSV output above, but see toCSV.\n",
    "     * Array.toString is useless, so format these ourselves. \n",
    "     */\n",
    "    override def toString: String = \n",
    "        s\"\"\"IIRecord($word, $total_count, $locStr, $cntStr)\"\"\"\n",
    "    \n",
    "    /** CSV-formatted string, but use [a,b,c] for the arrays */\n",
    "    def toCSV: String = \n",
    "        s\"$word,$total_count,$locStr,$cntStr\"\n",
    "        \n",
    "    /** Return a JSON-formatted string for the instance. */\n",
    "    def toJSONString: String = \n",
    "        s\"\"\"{\n",
    "        |  \"word\":        \"$word\", \n",
    "        |  \"total_count\": $total_count, \n",
    "        |  \"locations\":   ${toJSONArrayString(locations)},\n",
    "        |  \"counts\"       ${toArrayString(counts, \", \")}\n",
    "        |}\n",
    "        |\"\"\".stripMargin\n",
    "\n",
    "    private def locStr = toArrayString(locations)\n",
    "    private def cntStr = toArrayString(counts)\n",
    "\n",
    "    // \"[_]\" means we don't care what type of elements; we're just\n",
    "    // calling toString on them!\n",
    "    private def toArrayString(array: Array[_], delim: String = \",\"): String = \n",
    "        array.mkString(\"[\", delim, \"]\")  // i.e., \"[a,b,c]\"\n",
    "\n",
    "    private def toJSONArrayString(array: Array[String]): String =\n",
    "        toArrayString(array.map(quote), \", \")\n",
    "    \n",
    "    private def quote(word: String): String = \"\\\"\" + word + \"\\\"\"  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I said that defining secondary constructors is not very common. In part, it's because I used a convenient feature, the ability to define default values for arguments to methods, including the primary constructor. The default values mean that I can create instances without providing all the arguments explicitly, as long as there is a default value defined, and similarly for calling methods. Consider these two examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "`toString` output:\n",
      "IIRecord(hello, 0, [], [])\n",
      "IIRecord(world!, 3, [one,two], [1,2])\n",
      "\n",
      "`toJSONString` output:\n",
      "{\n",
      "  \"word\":        \"hello\", \n",
      "  \"total_count\": 0, \n",
      "  \"locations\":   [],\n",
      "  \"counts\"       []\n",
      "}\n",
      "\n",
      "{\n",
      "  \"word\":        \"world!\", \n",
      "  \"total_count\": 3, \n",
      "  \"locations\":   [\"one\", \"two\"],\n",
      "  \"counts\"       [1, 2]\n",
      "}\n",
      "\n",
      "\n",
      "`toCSV` output:\n",
      "hello,0,[],[]\n",
      "world!,3,[one,two],[1,2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "hello = IIRecord(hello, 0, [], [])\n",
       "world = IIRecord(world!, 3, [one,two], [1,2])\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "IIRecord(world!, 3, [one,two], [1,2])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val hello = new IIRecord(\"hello\")\n",
    "val world = new IIRecord(\"world!\", 3, Array(\"one\", \"two\"), Array(1, 2))\n",
    "\n",
    "println(\"\\n`toString` output:\")\n",
    "println(hello)\n",
    "println(world)\n",
    "\n",
    "println(\"\\n`toJSONString` output:\")\n",
    "println(hello.toJSONString)\n",
    "println(world.toJSONString)\n",
    "\n",
    "println(\"\\n`toCSV` output:\")\n",
    "println(hello.toCSV)\n",
    "println(world.toCSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I added `toJSONString` to illustrate adding _public_ methods, the default visibility, and _private_ methods to a class definition. By the way, when there are no methods or non-field variables to define, I can omit the body complete; no empty `{}` required.\n",
    "\n",
    "Recall that the `override` keyword is required when redefining `toString`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, what about that `case` keyword? It tells the compiler to do several useful things for us, eliminating a lot of boilerplate that we would have to write for ourselves with other languages, especially Java:\n",
    "\n",
    "1. Treat each constructor argument as an immutable (`val`) private field of the instance.\n",
    "1. Generate a public reader method for the field with the same name (e.g., `word`).\n",
    "1. Generate _correct_ implementations of the `equals` and `hashCode` methods, which people often implement incorrectly, as well as a default `toString` method. You can use your own definitions by adding them explicitly to the body. We did this for `toString`, to format the arrays in a nicer way than the default `Array[_].toString` method.\n",
    "1. Generate an `object IIRecord`, i.e., with the same name. The object is called the _companion object_.\n",
    "1. Generate a \"factory\" method in the companion object that takes the same argument list and instantiates an instance.\n",
    "1. Generate helper methods in the companion object that support pattern matching.\n",
    "\n",
    "Points 1 and 2 make each argument behave as if they are public, read-only fields of the instance, but they are actually implemented as described.\n",
    "\n",
    "Point 3 is important for correct behavior. Case class instances are often used as keys in [Maps](http://www.scala-lang.org/api/current/index.html#scala.collection.Map) and [Sets](http://www.scala-lang.org/api/current/index.html#scala.collection.Set), Spark RDD and DataFrame methods, etc. In fact, you should _only_ use your case classes or Scala's built-in types with well-defined `hashCode` and `equals` methods (like `Int` and other number types, `String`, tuples, etc.) as keys.\n",
    "\n",
    "For point 4, the _companion object_ is generated automatically by the compiler. It adds the \"factory\" method discussed in point 5, and methods that support pattern matching, point 6. You can explicitly define these methods and others yourself, as well as fields to hold state. The compiler will still insert these other methods. However, see <a href=\"#Ambiguities\">Ambiguities with Companion Objects</a>. The bottom line is that you shouldn't define case classes in notebooks like this with extra methods in the companion object, due to parsing ambiguities.\n",
    "\n",
    "Point 5 means you actually rarely use `new` when creating instances. That is, the following are effectively equivalent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hello1 = IIRecord(hello1, 0, [], [])\n",
       "hello2 = IIRecord(hello2, 0, [], [])\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "IIRecord(hello2, 0, [], [])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val hello1 = new IIRecord(\"hello1\")\n",
    "val hello2 = IIRecord(\"hello2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What actually happens in the second case, without `new`? The \"factory\" method is actually called `apply`. In Scala, whenever you put an argument list after any _instance_, including these `objects`, as in the `hello2` case, Scala looks for an `apply` method to call. The arguments have to match the argument list for apply (number of arguments, types of arguments, accounting for default argument values, etc.). Hence, the `hello2` declaration is really this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hello2b = IIRecord(hello2b, 0, [], [])\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "IIRecord(hello2b, 0, [], [])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val hello2b = IIRecord.apply(\"hello2b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can exploit this feature, too, in your other classes. We talked about word stemming above. Suppose you write a stemming library and declare an object for as the entry point. Here, I'll just do something simple; assume a trailing \"s\" means the word is a plural and remove it (a bad assumption...):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined object stem\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n",
      "dog\n"
     ]
    }
   ],
   "source": [
    "object stem {\n",
    "    def apply(word: String): String = word.replaceFirst(\"s$\", \"\") // insert real implementation!\n",
    "}\n",
    "\n",
    "println(stem(\"dog\"))\n",
    "println(stem(\"dogs\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how it looks like I'm calling a function or method named `stem`. Scala allows object and class names to start with a lower case letter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, point 6 means we can use our custom case classes in pattern matching expressions. I won't go into the methods actually implemented in the companion object and how they support pattern matching. I'll just use the \"magic\" in the following example that \"parses\" or previously-defined `hello` and `world` instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "List(hello with no occurrences., world! occurs 3 times: (one,1), (two,2))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Seq(hello, world).map {\n",
    "    case IIRecord(word, 0, _, _) => s\"$word with no occurrences.\"\n",
    "    case IIRecord(word, cnt, locs, cnts) => \n",
    "        s\"$word occurs $cnt times: ${locs.zip(cnts).mkString(\", \")}\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first case clause ignores the locations and counts, because I know they will be empty arrays if the total count is 0! \n",
    "\n",
    "The second case clause uses the `zip` method to put the locations and counts back together. Recall we used `unzip` to create the separate collections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets and DataFrames\n",
    "So far, we've mostly used Spark's RDD API. It's common to use case classes to represent the \"schema\" of records when working with RDDs, but also with a new type, [Dataset[T]](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset), analogous to `RDD[T]`, where the `T` represents the type of records.\n",
    "\n",
    "A problem with [DataFrames](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrame) is the fact that the fields are untyped until you try to access them. `Datasets` restore the type safety of `RDDs` by using a case class as the definition of the schema. \n",
    "\n",
    "`Datasets` were introduced in Spark 1.6.0, but they are somewhat incomplete in the 1.6.X releases. In Spark 2.0.0, `Dataset` becomes the \"parent\" class of `DataFrame`. This means that you'll be encouraged to use the greater type safety of `Dataset`, but you can still use `DataFrame` if you want. Now, `DataFrame` will be the equivalent of `Dataset[Row]`, where [Row](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Row) is the loosely-typed representation of the row and its columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out. But first, we need to import some SparkSQL-related code. Scala lets you import code almost anywhere, whereas Java requires imports at the beginning of source files. Scala also lets you import members of instances, not just the static imports supported by Java. \n",
    "\n",
    "So, the next cell imports some \"implicits\" from the [SQLContext](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SQLContext) instance already in scope. Unfortunately, due to a scoping ambiguity involving notebooks and the Scala interpreter, we need to assign `sqlContext` to a new variable, _then_ import from that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sqlc = org.apache.spark.sql.SQLContext@27a9da47\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SQLContext@27a9da47"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sqlc = sqlContext\n",
    "import sqlc.implicits._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll explain what \"implicits\" are <a href=\"#implicits\">later</a>. For now, suffice it to say that they are used to \"allow\" us to call the `as` method on our `iiDF` `DataFrame`, which converts it to a `Dataset[IIRecord]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "iiDS = [word: string, total_count: int ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[word: string, total_count: int ... 2 more fields]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val iiDS = iiDF.as[IIRecord]\n",
    "iiDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+--------------------+--------------------+\n",
      "|       word|total_count|           locations|              counts|\n",
      "+-----------+-----------+--------------------+--------------------+\n",
      "|          a|       3350|[loveslabourslost...|[507, 494, 492, 4...|\n",
      "|    abandon|          6|[asyoulikeit, tam...|           [4, 1, 1]|\n",
      "|      abate|          3|[loveslabourslost...|           [1, 1, 1]|\n",
      "|  abatement|          1|      [twelfthnight]|                 [1]|\n",
      "|     abbess|          8|    [comedyoferrors]|                 [8]|\n",
      "|      abbey|          9|    [comedyoferrors]|                 [9]|\n",
      "|abbominable|          1|  [loveslabourslost]|                 [1]|\n",
      "|abbreviated|          1|  [loveslabourslost]|                 [1]|\n",
      "|       abed|          2|[asyoulikeit, twe...|              [1, 1]|\n",
      "|   abetting|          1|    [comedyoferrors]|                 [1]|\n",
      "|abhominable|          1|  [loveslabourslost]|                 [1]|\n",
      "|      abhor|          5|[asyoulikeit, com...|     [1, 1, 1, 1, 1]|\n",
      "|     abhors|          2|      [twelfthnight]|                 [2]|\n",
      "|      abide|          5|[merrywivesofwind...|              [3, 2]|\n",
      "|     abides|          1|[muchadoaboutnoth...|                 [1]|\n",
      "|    ability|          2|[muchadoaboutnoth...|              [1, 1]|\n",
      "|     abject|          2|[comedyoferrors, ...|              [1, 1]|\n",
      "|     abjure|          1|[midsummersnights...|                 [1]|\n",
      "|    abjured|          2|[tamingoftheshrew...|              [1, 1]|\n",
      "|       able|          9|[merrywivesofwind...|     [4, 2, 1, 1, 1]|\n",
      "+-----------+-----------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iiDS.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Scala for Spark 102\"\n",
    "We've covered a lot already in this notebook, focusing on the most important topics you need to know about Scala for daily use. Let's call them the \"Scala for Spark 101\" material.\n",
    "\n",
    "At this point, I suggest you create a new notebook and play with Spark using what you've learned so far, then come back to this point if you run into something we didn't cover already. Chances are you're ready to learn the next bits of useful Scala, the \"102\" material."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Everything in a Package\n",
    "In Java, `import foo.bar.*;` means import everything in the `bar` package.\n",
    "\n",
    "In Scala, `*` is actually a legal method name; think of defining multiplication for custom numeric types, like `Matrix`. Hence, this import statement in Scala would be ambigious. Therefore, Scala uses `_` instead of `*`, `import foo.bar._` (with the semicolon inferred)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incidentally, what would that `*` method definition look like? Something like this:\n",
    "\n",
    "```scala\n",
    "case class Matrix(rows: Array[Array[Double]]) {  // Each row is an Array[Double]\n",
    "\n",
    "    /** Multiple this matrix by another. */\n",
    "    def *(other: Matrix): Matrix = ...\n",
    "    \n",
    "    /** Add this matrix by another. */\n",
    "    def +(other: Matrix): Matrix = ...\n",
    "    \n",
    "    ...\n",
    "}\n",
    "\n",
    "val row1: Array[Array[Double]] = ...\n",
    "val row2: Array[Array[Double]] = ...\n",
    "val m1 = Matrix(rows1)\n",
    "val m2 = Matrix(rows2)\n",
    "val m1_times_m2 = m1 * m2\n",
    "val m1_plus_m2 = m1 + m2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operator Syntax\n",
    "\n",
    "Wait!! What's this `m1 * m2` stuff?? Shouldn't it be `m1.*(m2)`. It would be really convenient to use \"operator syntax\", more precisely called _infix operator notation_ for many methods like `*` and `+` here. The Scala parser supports this with a simple relaxation of the rules; when a method takes a single argument, you can omit the period `.` and parentheses `(...)`. Hence the following really is equivalent:\n",
    "\n",
    "```scala\n",
    "val m1_times_m2 = m1.*(m2)\n",
    "val m1_times_m2 = m1 * m2\n",
    "```\n",
    "\n",
    "This convenience can lead to confusing code, especially for beginners to Scala, so use it cautiously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traits\n",
    "_Traits_ are similar to Java 8 _interfaces_, used to define abstractions, but with the ability to provide \"default\" implementations of the methods declared. Unlike Java 8 interfaces, traits can also have fields representing \"state\" information about instances. There is a blury line between traits and _abstract classes_, again where some member methods or fields are not defined. In both cases, a subtype of a trait and/or an abstract class must define any undefined members if you want to construct instances of it.\n",
    "\n",
    "So, why have both traits and abstract classes? It's because Java only allows _single inheritance_; there can be only one _parent_ type, which is normally where you would use an abstract class, but Scala lets you \"mix in\" one or more additional traits (or use a trait as the parent class - yes, confusing). A great example \"mix in\" trait is one that implements logging. Any \"service\" type can mix in the logging trait to get \"instant\" access to this reusable functionality. Schematically, it looks like the following:\n",
    "\n",
    "```scala\n",
    "// Assume severity `Level` and `Logger` types defined elsewhere...\n",
    "trait Logging {\n",
    "\n",
    "    def log(level: Level, message: String): Unit = logger.log(level, message)\n",
    "    \n",
    "    private logger: Logger = ...\n",
    "}\n",
    "\n",
    "abstract class Service {\n",
    "    def run(): Unit   // No body, so abstract!\n",
    "}\n",
    "\n",
    "class MyService extends Service with Logging {\n",
    "    def run(): Unit = {\n",
    "        log(INFO, \"Staring MyService...\")\n",
    "        ...\n",
    "        log(INFO, \"Finished MyService\")\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "`Unit` is Scala's equivalent to Java's `void`. It actually is a true type with a single return value, unlike `void`, but we use it in the same sense of \"nothing useful will be returned\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranges\n",
    "What if you want some numbers between a start and end value? Use a [Range](http://www.scala-lang.org/api/current/index.html#scala.collection.immutable.Range), which has a nice literal syntax, e.g., `1 until 100`, `2 to 200 by 3`. \n",
    "\n",
    "The `Range` always includes the lower bound. Using `to` in a `Range` makes it _inclusive_ at the upper bound. Using `until` makes it _exclusive_ at the upper bound. Use `by` to specify a delta, which defaults to `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Range(1, 2, 3, 4, 5, 6, 7, 8, 9)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 until 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Range(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 to 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Range(1, 4, 7, 10)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 to 10 by 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you need a small test data set to play with Spark, ranges can be convenient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0,CompactBuffer((7,0), (14,0), (21,0), (28,0), (35,0), (42,0), (49,0)))\n",
      "(1,CompactBuffer((1,1), (8,1), (15,1), (22,1), (29,1), (36,1), (43,1), (50,1)))\n",
      "(2,CompactBuffer((2,2), (9,2), (16,2), (23,2), (30,2), (37,2), (44,2)))\n",
      "(3,CompactBuffer((3,3), (10,3), (17,3), (24,3), (31,3), (38,3), (45,3)))\n",
      "(4,CompactBuffer((4,4), (11,4), (18,4), (25,4), (32,4), (39,4), (46,4)))\n",
      "(5,CompactBuffer((5,5), (12,5), (19,5), (26,5), (33,5), (40,5), (47,5)))\n",
      "(6,CompactBuffer((6,6), (13,6), (20,6), (27,6), (34,6), (41,6), (48,6)))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rdd7 = ShuffledRDD[101] at sortByKey at <console>:39\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ShuffledRDD[101] at sortByKey at <console>:39"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd7 = sc.parallelize(1 to 50).\n",
    "    map(i => (i, i%7)).\n",
    "    groupBy{ case (i, seven) => seven }.\n",
    "    sortByKey()\n",
    "rdd7.take(7).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[SparkContext](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext) also has a `range` method that effectively does the same thing as `sc.parallelize(some_range)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scala Interpreter (REPL) vs. Notebooks vs. Scala Compiler\n",
    "<a name=\"REPL\"></a>\n",
    "This notebook has been using a running Scala interpreter, a.k.a. _REPL_ (\"read, eval, print, loop\") to parse the Scala code. The Spark distribution comes with a `spark-shell` script that also lets you use the interpreter from the command line, but without the nice notebook UI.\n",
    "\n",
    "If you use `spark-shell`, there are a few other behavior changes you should know about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using :paste Mode\n",
    "By default the Scala interpreter treats _each line_ you enter separately. This can cause surprises compared to how the Scala _compiler_ works, where it treats all the code in the same file in the same context.\n",
    "\n",
    "For example, the following code, where the expression continues on the second line, is handled successfully by the compiler, but not by the interpreter.\n",
    "\n",
    "```scala\n",
    "(1 to 100)\n",
    ".map(i => i*i)\n",
    "```\n",
    "\n",
    "the Interpreter thinks it finished parsing the expression when it hit the new line after the literal [Range](http://www.scala-lang.org/api/current/index.html#scala.collection.immutable.Range), `1 to 100`. It then throws an error on the opening `.` on the next line. On the other hand, the compiler keeps compiling, ignoring the new line in this case. \n",
    "\n",
    "This notebook also does the same thing as the \"raw\" interpreter, but in some cases, notebooks will use an interpeter command, `:paste` that tells the parser to parse all of the lines that follow together, just like the compiler would parse them, until the \"end of input\", which you indicate with `CTRL-D`. \n",
    "\n",
    "You can't experiment with it through this notebook, but your session would look something like this:\n",
    "\n",
    "```scala\n",
    "scala> :paste\n",
    "// Entering paste mode (ctrl-D to finish)\n",
    "\n",
    "(1 to 10)\n",
    ".map(i => i*i)\n",
    "<CTRL-D>\n",
    "\n",
    "// Exiting paste mode, now interpreting.\n",
    "\n",
    "res0: scala.collection.immutable.IndexedSeq[Int] = Vector(1, 4, 9, 16, 25, 36, 49, 64, 81, 100)\n",
    "\n",
    "scala>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ambiguities with Companion Objects\n",
    "<a name=\"Ambiguities\"></a>\n",
    "As I wrote this notebook, I _wanted_ to demonstrate using the companion object `IIRecord` to define a method explicitly, but this leads to an ambiguity later on in the notebook if you attempt to use this method. The notebook gets confused between the case class and the object. \n",
    "\n",
    "While unfortunate, it's also true that once you start defining more involved case classes, with more than trivial methods and explicit additions to the default companion object, you should really define these types outside the notebook in a compiled library that you use within the notebook.\n",
    "\n",
    "The details are beyond our scope here, but basically, you set up a project with your Scala code and build it using your favorite build tool. [SBT](http://www.scala-sbt.org/) is a popular choice for Scala, but Maven, Gradle, etc. can be used. \n",
    "\n",
    "You want to generate a _jar_ file with the compiled artifacts, then when you start `spark-shell`, submit a Spark job with `spark-submit` or use a notebook environment like this one, you specify the jar for inclusion. For `spark-shell` and `spark-submit`, invoke it with the `--jars myproject.jar` option. For Toree with Jupyter, see the discussion on the [FAQ page](https://toree.incubator.apache.org/documentation/user/faq.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scala's Type Hierarchy\n",
    "Scala's type hierarchy is similar to Java's, but with some interesting differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Scala Type Hierarchy](http://docs.scala-lang.org/resources/images/classhierarchy.img_assist_custom.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Java, all _reference types_ are descended from [java.lang.Object](https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html). The name _reference type_ reflects the fact that the instances for all these types are allocated on the _heap_ and program variables are references to those heap locations.\n",
    "\n",
    "The primitives types, `int`, `long`, etc. are not considered part of the type hierarchy and are treated specially. This is in part a performance optimization, as instances of these types fit in CPU registers and the values are pushed onto stack frames. However, they have wrapper or \"boxed\" types, `Integer`, `Long`, etc., that are part of the type hierarchy, which you must use with Java's collections, for example (with the exception of arrays).\n",
    "\n",
    "Instead, Scala treats the primitives at the code level as basically the same as the reference types. You don't use `new Int(100)` for example, but you can call methods on `Int` instances. The code generated, in most cases, uses the optimized JVM primitives. \n",
    "\n",
    "Hence, the Scala type hierarchy defines a type [Any](http://www.scala-lang.org/api/current/#scala.Any) to be the a parent type of _both_ reference types and \"value\" types (for the primitives). Each of those subhierarchies have parent types, [AnyRef](http://www.scala-lang.org/api/current/#scala.AnyRef) is effectively the same as [java.lang.Object](https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html), and [AnyVal](http://www.scala-lang.org/api/current/#scala.AnyVal) is the parent of the value types.\n",
    "\n",
    "Finally, for better \"soundness\", the Scala type system defines a real type to represent [Null](http://www.scala-lang.org/api/current/#scala.Null) and [Nothing](http://www.scala-lang.org/api/current/#scala.Nothing). By defining `Null` to be the subtype of all reference types `AnyRefs` (but not `AnyVals`), it supports at the type level the (unfortunate) practice of using `null` for a reference value.\n",
    "\n",
    "However, `null` is not allowed for an `AnyVal`, so the true \"bottom type\" of the hierarchy is `Nothing`. Why is that useful. I'll explain in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"TryOptionNull\"></a>\n",
    "### Try vs. Option vs. null\n",
    "\n",
    "Recall the signature of our `curl` method near the beginning of this notebook:\n",
    "\n",
    "```scala\n",
    "def curl(sourceURLString: String, targetDirectoryString: String): File = ...\n",
    "```\n",
    "\n",
    "It returns a `File` when everything goes well, but it could throw an exception. An alternative is return a `Try[File]`, where the [Try](http://www.scala-lang.org/api/current/index.html#scala.util.Try) encapsulates both cases in the return value, as we'll discuss next. We'll also discuss an alternative, [Option](http://www.scala-lang.org/api/current/index.html#scala.Option)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose instead that we declared `curl` to return [util.Try[T]](http://www.scala-lang.org/api/current/index.html#scala.util.Try), where `T` is `java.io.File`. The only change to the body would be to simply add `Try` before the opening bracket:\n",
    "\n",
    "```\n",
    "def curl(sourceURLString: String, targetDirectoryString: String): Try[File] = Try {...}\n",
    "```\n",
    "\n",
    "Now, the reader knows from the method signature that it might fail somehow. If a call fails, the relevant exception will be returned wrapped in a subclass of `Try`, called [util.Failure[T]](http://www.scala-lang.org/api/current/index.html#scala.util.Failure). However, if `curl` succeeds, the `File` will be returned wrapped in the other subclass of `Try`, [util.Success[T]](http://www.scala-lang.org/api/current/index.html#scala.util.Success).\n",
    "\n",
    "Because of Scala's type safety, the caller of `curl` must determine which result was returned and handle it appropriately. That is, the caller must determine if a `Success` or `Failure` was returned and handle it appropriately.\n",
    "\n",
    "Scala does not have exception declarations like Java. So, looking at the signature of our original version, there's no obvious way to know if it throws an exception _or_ returns `null` on failure:\n",
    "\n",
    "```scala\n",
    "def curl(sourceURLString: String, targetDirectoryString: String): File = {...}\n",
    "```\n",
    "\n",
    "If we choose to catch exceptions internally and return `null`, the caller has to remember to check for `null`. Otherwise, the infamous [NullPointerException](https://docs.oracle.com/javase/8/docs/api/java/lang/NullPointerException.html) might happen occasionally if the caller assumes a non-`null` value is returned. So, using `Try[T]` prevents us from this loophole. _It helps the user do the right thing!_\n",
    "\n",
    "Also, using `Try` rather than simply throwing an exception, means that `curl` always returns \"normally\", so the caller maintains full control of the call stack and special exception-catching logic isn't required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are all the possible valid subclasses of `Try`? Really, there are only two, `Success` and `Failure`. It would be a mistake to allow a user to define other subtypes, like `MaybeCouldFailButWhoKnows`, because users of `Try` in pattern matching will always want to know that there are only two possibilities. Scala adds a keyword to enforce this logical behavior. `Try` is actually declared as follows:\n",
    "\n",
    "```scala\n",
    "sealed abstract class Try[+T] extends AnyRef\n",
    "```\n",
    "\n",
    "(`AnyRef` is the same as Java's `Object` supertype.) The `sealed` keyword says that _no_ subclasses of `Try` can be declared, _except_ in the same source file (which the library author wrote). Hence, users of `Try` can't declare their own subclasses, subverting the logical structure of this type hierarchy and other user's code that relies on this structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we have a situation where it makes no sense to involve an exception, but we want the same logically handling? This is where [Option[T]](http://www.scala-lang.org/api/current/index.html#scala.Option) comes in. \n",
    "\n",
    "`Option` is analogous to `Try`, it is a `sealed` abstract type with two possible subtypes:\n",
    "\n",
    "* [Some[T]](http://www.scala-lang.org/api/current/index.html#scala.None): I have a an instance of `T` for your, inside the `Some[T]`.\n",
    "* [None](http://www.scala-lang.org/api/current/index.html#scala.None): I don't have a value for your, sorry.\n",
    "\n",
    "Note that a hash map is a great example where I either have a value for a given key or I don't. Therefore, for Scala's [Map[K,V]](http://www.scala-lang.org/api/current/index.html#scala.collection.Map) abstraction, where `K` is the key type and `V` is the value type, the `get` method has this signature:\n",
    "\n",
    "```scala\n",
    "def get(key: K): Option[V]\n",
    "```\n",
    "\n",
    "One again, you know from the type signature that you may or may not get a value instance for the input key, _and_ you **must** determine whether you got a `Some[V]` or a `None` as the result. Once again, we avoid returning a `null` value and risking a `NullPointerException` if we forget to handle it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, how do we determine which `Option[T]` was returned? Let's look a few examples using `Option`. Can you guess what they are doing? Check the [Option Scaladocs](http://www.scala-lang.org/api/current/#scala.Option) to confirm. `Try` can be used similarly, with a few other ways available that we won't discuss here (but see the [Try Scaladocs](http://www.scala-lang.org/api/current/#scala.util.Try))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "2\n",
      "3\n",
      "None\n",
      "5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "options = List(None, Some(2), Some(3), None, Some(5))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "List(None, Some(2), Some(3), None, Some(5))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val options = Seq(None, Some(2), Some(3), None, Some(5))\n",
    "\n",
    "options.foreach { o =>\n",
    "    println(o.getOrElse(\"None\"))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "2\n",
      "3\n",
      "None\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "options.foreach {\n",
    "    case None    => println(None)\n",
    "    case Some(i) => println(i)  // Note how we extract the enclosed value.\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you just want to ignore the `None` values, use a _for comprehension_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "for {\n",
    "    option <- options  // loop through the options, assign each to \"option\"\n",
    "    value  <- option   // extract the value from the Some, or if None, skip to the next loop\n",
    "} println(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you might wonder how `None` is declared. Consider this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some(hello)\n",
      "None\n",
      "Some(world!)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "opts = List(Some(hello), None, Some(world!))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "List(Some(hello), None, Some(world!))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val opts: Seq[Option[String]] = Seq(Some(\"hello\"), None, Some(\"world!\"))\n",
    "opts.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works, so it must mean that `None` is a valid subclass of `Option[String]`. That's actually true for all `Option[T]`. How can a single object be a valid subtype for _all_ of them? Here is how it's declared (omitting some details):\n",
    "\n",
    "```scala\n",
    "object None extends Option[Nothing] {...}\n",
    "\n",
    "```\n",
    "\n",
    "`None` carries no \"state\" information, because it doesn't wrap an instance like `Some[T]` does. Hence, we only need one instance for all uses, so it's declared as an object. Recall we mentioned above that the type system has a [Nothing](http://www.scala-lang.org/api/current/#scala.Nothing) type, which is a subtype of all other types. Without diving into too many details, if a variable is of type `Option[String]`, then you can use an `Option[Nothing]` for it (i.e., the latter is a subtype of the former). This is why `Nothing` is useful, for cases like `None`, so we can have one instance of it, but still obey the rules of Scala's object-oriented type system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implicits\n",
    "<a name=\"implicits\"></a>\n",
    "Scala has a powerful mechanism known as _implicits_ that is used in the Spark Scala API. Implicits are a big topic, so we'll focus just on the uses of it that are most important to understand.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Type Conversions\n",
    "We used `RDD` methods like `reduceByKey` above, but if you search for this method in the [RDD Scaladoc page](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD), you won't find it. Instead it's defined in the [PairRDDFunctions](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions) type (along with all the other `*ByKey` methods). So, how can we use these methods as if they are defined for `RDD`??\n",
    "\n",
    "When the Scala compiler sees code calling a method that doesn't exist on the type, it looks for an _implicit conversion_ in the current scope, which can transform the instance into another type (i.e., by wrapping it), where the other type provides the needed method. The full signature inferred for the method as it's used must match the definition in the wrapping class.\n",
    "\n",
    "> **Note:** If you don't find a method in the [Spark Scaladocs](http://spark.apache.org/docs/latest/api/scala/index.html#package) for a type where you think it should be defined, look for related helper types with the method.\n",
    "\n",
    "Here's a small Scala example of how this works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Person\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// A sample class. Note it doesn't define a `toJSON` method:\n",
    "case class Person(name: String, age: Int = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined object implicits\n",
       "p = Person(Dean Wampler,39)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{\"name\": Dean Wampler, \"age\": 39}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// To scope them, define implicit conversions within an object\n",
    "object implicits {\n",
    "\n",
    "    // `implicit` keyword tells the compiler to consider this conversion.\n",
    "    // It takes a `Person`, returning a new instance of `PersonToJSONString`,\n",
    "    // then resolves the invocation of `toJSON`.\n",
    "    implicit class PersonToJSONString(person: Person) {\n",
    "        def toJSON: String = s\"\"\"{\"name\": ${person.name}, \"age\": ${person.age}}\"\"\"\n",
    "    }\n",
    "}\n",
    "\n",
    "import implicits._        // Now it is visible in the current scope.\n",
    "\n",
    "val p = Person(\"Dean Wampler\", 39)\n",
    "\n",
    "// Magic conversion to `PersonToJSONString`, then `toJSON` is called.\n",
    "p.toJSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `RDDs`, the implicit conversions to `PairRDDFunctions` and other support types are handled for you. However, when you use Spark SQL and the [DataFrame](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrame) API, you'll need to import some of these conversions yourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sqlc = org.apache.spark.sql.SQLContext@27a9da47\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SQLContext@27a9da47"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sqlc = sqlContext\n",
    "import sqlc.implicits._  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|       word|total_count|\n",
      "+-----------+-----------+\n",
      "|          a|       3350|\n",
      "|    abandon|          6|\n",
      "|      abate|          3|\n",
      "|  abatement|          1|\n",
      "|     abbess|          8|\n",
      "|      abbey|          9|\n",
      "|abbominable|          1|\n",
      "|abbreviated|          1|\n",
      "|       abed|          2|\n",
      "|   abetting|          1|\n",
      "|abhominable|          1|\n",
      "|      abhor|          5|\n",
      "|     abhors|          2|\n",
      "|      abide|          5|\n",
      "|     abides|          1|\n",
      "|    ability|          2|\n",
      "|     abject|          2|\n",
      "|     abjure|          1|\n",
      "|    abjured|          2|\n",
      "|       able|          9|\n",
      "+-----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "wtc = [word: string, total_count: int]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[word: string, total_count: int]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val wtc = iiDF.select($\"word\", $\"total_count\")\n",
    "wtc.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column-reference syntax `$\"name\"` is implemented using the same mechanism in the Scala library that implements interpolated strings, `s\"$foo\"`. The `import sqlc.implicits._` makes it available. \n",
    "\n",
    "Note we imported something from an _instance_, rather than a package or type, as allowed in Java. This can be a useful feature in Scala, but it's also fragile, If you try `import sqlContext.implicits._`, you'll get a compiler error that a \"stable identifier\" is required. It turns out that doing the value assignment, `val sqlc = sqlContext` first meets this requirement. This is unique to the notebook environment. You normally won't see this problem if you use the `spark-shell` that comes with a Spark distribution or you write a Spark program and compile it with the Scala compiler.\n",
    "\n",
    "However, it would be better if Spark defined this `implicits` object on the `SQLContext` companion object instead of on instances of it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For completeness, but unrelated to implicits, the `DataFrame` API lets you write SQL-like queries with a programmatic API. If you want to use built in functions like `min`, `max`, etc. on columns, you need the following `import` statement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use `min`, `max`, `avg`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+------------------+\n",
      "|min(total_count)|max(total_count)|  avg(total_count)|\n",
      "+----------------+----------------+------------------+\n",
      "|               1|            5208|16.651743683350947|\n",
      "+----------------+----------------+------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "mma = [min(total_count): int, max(total_count): int ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[min(total_count): int, max(total_count): int ... 1 more field]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val mma = iiDF.select(min(\"total_count\"), max(\"total_count\"), avg(\"total_count\"))\n",
    "mma.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implicit Method Arguments\n",
    "One other use of implicits worth understanding is _implicit arguments_ to methods. You will encounter this mechanism used when you read the Spark Scaladocs, even though you might never realize you're actually using it in your code!\n",
    "\n",
    "Recall I mentioned previously that you can define default values for method arguments. I just used it for the `age` argument for `Person`:\n",
    "\n",
    "```scala\n",
    "case class Person(name: String, age: Int = 0)\n",
    "```\n",
    "\n",
    "Sometimes we need something more sophisticated. For example, our library might have a group of methods that need a special argument passed to them that provides useful \"context\" information, but you don't want the user to be required to explicitly pass this argument every time. Other times you might use implicit arguments to make the API \"cleaner\", but still have some control over what's allowed.\n",
    "\n",
    "Here's an example, that's partly inspired by Scala's [Seq.sum](http://www.scala-lang.org/api/current/#scala.collection.Seq) method. Wouldn't it be great if I happen to have a collection of things I can \"add\" together, if I could just call `sum` on the collection? Let's do this in a slightly different way, with a helper `sum` method outside of `Seq`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined trait Add\n",
       "defined object Adder\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "sum: [T](ts: Seq[T])(implicit adder: Add[T])T\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trait Add[T] {\n",
    "    def add(t1: T, t2: T): T\n",
    "}\n",
    "\n",
    "// Nested implicits so they don't conflict with the previous object implicits.\n",
    "object Adder {\n",
    "    object implicits {\n",
    "        implicit val intAdd = new Add[Int] { \n",
    "            def add(i1: Int, i2: Int): Int = i1+i2 \n",
    "        }\n",
    "        implicit val doubleAdd = new Add[Double] { \n",
    "            def add(d1: Double, d2: Double): Double = d1+d2 \n",
    "        }\n",
    "        implicit val stringAdd = new Add[String] { \n",
    "            def add(s1: String, s2: String): String = s1+s2 \n",
    "        }\n",
    "        // etc...\n",
    "    }\n",
    "}\n",
    "\n",
    "import Adder.implicits._\n",
    "\n",
    "// NOTE: TWO argument lists!\n",
    "def sum[T](ts: Seq[T])(implicit adder: Add[T]): T = {\n",
    "    ts.reduceLeft((t1, t2) => adder.add(t1, t2))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(0 to 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51.29999999999999"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(0.0 to 5.5 by 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "onetwothree"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(Seq(\"one\", \"two\", \"three\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:60: error: could not find implicit value for parameter adder: Add[Char]\n",
       "       sum(Seq('a', 'b', 'c'))   // Characters\n",
       "          ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Will fail, because there's no Add[Char] in scope:\n",
    "sum(Seq('a', 'b', 'c'))   // Characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the implicit values `intAdd`, `doubleAdd`, and `stringAdd`, were used by the Scala interpreter for the `adder` argument in the second _argument list_ for `sum`. Note that you have to use a second argument list and all arguments there must be implicit. \n",
    "\n",
    "We could have avoided using implicit arguments if we defined custom `sum` methods for every type. That would have been simpler in this trivial case, but for nontrivial methods, the duplication is worth avoiding. Another advantage of this mechanism is that the user can define her own implicit `Add[T]` instances for domain types (say for example, `Money`) and they would \"just work\".\n",
    "\n",
    "The Scala collections API uses this mechanism to know how to construct a new collection of the same kind as the input collection when you use `map`, `flatMap`, `reduceLeft`, etc.\n",
    "\n",
    "Spark uses this pattern for [Encoders](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Encoder) in  Spark SQL. Encoders are used to serialize values into the new, compact memory encoding introduced in the _Tungsten_ project (see for example, [here](https://spark-summit.org/2015/events/deep-dive-into-project-tungsten-bringing-spark-closer-to-bare-metal/)). Here's an example of creating a [Dataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset), where the `toDS` method is first \"added\" to a Scala [Seq](http://www.scala-lang.org/api/current/#scala.collection.Seq) through an implicit conversion (specifically [SQLImplicits.localSeqToDatasetHolder](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SQLImplicits), which is brought into scope by the `import sqlc.implicits._` statement earlier) and then `toDS` uses `Encoders` internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[value: int]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0 to 10).toDS()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "I appreciate the effort you put into studying this notebook. I hope you enjoyed it as much as I enjoyed writing it. Please post issues on how I can improve it to the [GitHub repo](https://github.com/deanwampler/JustEnoughScalaForSpark). \n",
    "\n",
    "Now you know the core elements of Scala that you need for using the Spark Scala API. I hope you can appreciate the power and elegance of Scala. I hope you will choose to use it for all of your data engineering tasks, not just for Spark. \n",
    "\n",
    "What about data science? There are many people who use Scala for data science in Spark, but today Python and R have much richer libraries for Mathematics and Machine Learning. That will change over time, but for now, you'll need to decide which language best fits your needs.\n",
    "\n",
    "As you use Scala, there will be more things you'll want to understand that we haven't covered, including common idioms, conventions, and tools used in the Scala community. The references at the beginning of the notebook will give you the information you need.\n",
    "\n",
    "Best wishes.\n",
    "\n",
    "[Dean Wampler, Ph.D.](mailto:deanwampler@gmail.com)<br/>\n",
    "[@deanwampler](http://twitter.com/deanwampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Exercise Solutions\n",
    "<a name=\"ExerciseSolutions\"></a>\n",
    "Let's discuss the solutions to exercises that weren't already solved earlier in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter for Plays that Have \"of\" in the Name\n",
    "You can add the condition (comment `// <== here`) immediate after defining `play`. You could do it later, after either of the subsequent two expressions, but then you're doing needless computation. Change `true` to `false` to print plays that don't contain \"of\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/data/shakespeare/tamingoftheshrew\tSuccess!\n",
      "/home/jovyan/work/data/shakespeare/comedyoferrors\tSuccess!\n",
      "/home/jovyan/work/data/shakespeare/merrywivesofwindsor\tSuccess!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "list2 = List(/home/jovyan/work/data/shakespeare/tamingoftheshrew\tSuccess!, /home/jovyan/work/data/shakespeare/comedyoferrors\tSuccess!, /home/jovyan/work/data/shakespeare/merrywivesofwindsor\tSuccess!)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "List(/home/jovyan/work/data/shakespeare/tamingoftheshrew\tSuccess!, /home/jovyan/work/data/shakespeare/comedyoferrors\tSuccess!, /home/jovyan/work/data/shakespeare/merrywivesofwindsor\tSuccess!)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val list2 = for {\n",
    "    play <- plays \n",
    "    if (play.contains(\"of\") == true)                            // <== here\n",
    "    playFileString = targetDirName + pathSeparator + play\n",
    "    playFile = new File(playFileString)\n",
    "} yield {\n",
    "    val successString = if (playFile.exists) \"Success!\" else \"NOT FOUND!!\"\n",
    "    \"%-40s\\t%s\".format(playFileString, successString)\n",
    "}\n",
    "list2.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Specific \"Love\" and \"Hate\" Words\n",
    "One reasonable choice to prevent seeing `glove`, `whatever`, etc. is to only find words that start with `love` and `have`. Let's also keep `unlove`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+--------------------+---------+\n",
      "|   word|total_count|        top_location|top_count|\n",
      "+-------+-----------+--------------------+---------+\n",
      "|   hate|         22|midsummersnightsd...|        9|\n",
      "|  hated|          6|midsummersnightsd...|        4|\n",
      "|hateful|          5|midsummersnightsd...|        3|\n",
      "|  hates|          5|         asyoulikeit|        2|\n",
      "| hateth|          1|midsummersnightsd...|        1|\n",
      "|   love|        662|    loveslabourslost|      121|\n",
      "|  loved|         38|         asyoulikeit|       13|\n",
      "| lovely|         15|midsummersnightsd...|        7|\n",
      "|  lover|         33|         asyoulikeit|       14|\n",
      "| lovers|         31|midsummersnightsd...|       17|\n",
      "|  loves|         51| muchadoaboutnothing|       10|\n",
      "| lovest|          8|    tamingoftheshrew|        3|\n",
      "| loveth|          2|    loveslabourslost|        1|\n",
      "|unloved|          1|midsummersnightsd...|        1|\n",
      "+-------+-----------+--------------------+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "topLocationsLoveHate = [word: string, total_count: int ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[word: string, total_count: int ... 2 more fields]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val topLocationsLoveHate = sqlContext.sql(\"\"\"\n",
    "    SELECT word,  total_count, locations[0] AS top_location, counts[0] AS top_count\n",
    "    FROM inverted_index \n",
    "    WHERE word LIKE 'love%' OR word LIKE 'unlove%' OR word LIKE 'hate%'\n",
    "\"\"\")\n",
    "topLocationsLoveHate.show(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return the Top Two Locations and Counts\n",
    "We used the `DataFrame` API to write a SQL query that returned the top location and count. Adding the next one is straightforward. What do you observe is returned when there isn't a second location and count?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topTwoLocations = [word: string, total_count: int ... 4 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[word: string, total_count: int ... 4 more fields]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val topTwoLocations = sqlContext.sql(\"\"\"\n",
    "    SELECT word, total_count, \n",
    "        locations[0] AS first_location,  counts[0] AS first_count,\n",
    "        locations[1] AS second_location, counts[1] AS second_count\n",
    "    FROM inverted_index \n",
    "    WHERE word LIKE '%love%' OR word LIKE '%hate%'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------------------+-----------+--------------------+------------+\n",
      "|    word|total_count|      first_location|first_count|     second_location|second_count|\n",
      "+--------+-----------+--------------------+-----------+--------------------+------------+\n",
      "| beloved|         11|    tamingoftheshrew|          4|         asyoulikeit|           3|\n",
      "|  cloven|          1|    loveslabourslost|          1|                null|        null|\n",
      "|  cloves|          1|    loveslabourslost|          1|                null|        null|\n",
      "|   glove|          3|    loveslabourslost|          2|        twelfthnight|           1|\n",
      "|  glover|          1| merrywivesofwindsor|          1|                null|        null|\n",
      "|  gloves|          5| merrywivesofwindsor|          3|         asyoulikeit|           1|\n",
      "|    hate|         22|midsummersnightsd...|          9|         asyoulikeit|           6|\n",
      "|   hated|          6|midsummersnightsd...|          4|         asyoulikeit|           2|\n",
      "| hateful|          5|midsummersnightsd...|          3|    loveslabourslost|           1|\n",
      "|   hates|          5|         asyoulikeit|          2| merrywivesofwindsor|           1|\n",
      "|  hateth|          1|midsummersnightsd...|          1|                null|        null|\n",
      "|    love|        662|    loveslabourslost|        121|         asyoulikeit|         119|\n",
      "|   loved|         38|         asyoulikeit|         13| muchadoaboutnothing|          13|\n",
      "|  lovely|         15|midsummersnightsd...|          7|    tamingoftheshrew|           5|\n",
      "|   lover|         33|         asyoulikeit|         14|midsummersnightsd...|          10|\n",
      "|  lovers|         31|midsummersnightsd...|         17|         asyoulikeit|           6|\n",
      "|   loves|         51| muchadoaboutnothing|         10| merrywivesofwindsor|           9|\n",
      "|  lovest|          8|    tamingoftheshrew|          3| muchadoaboutnothing|           2|\n",
      "|  loveth|          2|    loveslabourslost|          1|    tamingoftheshrew|           1|\n",
      "| unloved|          1|midsummersnightsd...|          1|                null|        null|\n",
      "|   whate|          4|    tamingoftheshrew|          3|         asyoulikeit|           1|\n",
      "|whatever|          1|    tamingoftheshrew|          1|                null|        null|\n",
      "+--------+-----------+--------------------+-----------+--------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topTwoLocations.show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stop Words\n",
    "Recall you were asked to implement a `keep(word: String):Boolean` method that filters stop words.\n",
    "\n",
    "First, let's implement `keep`. You can find lists of stop words on the web. One such list for English can be found [here]( * From http://norm.al/2009/04/14/list-of-english-stop-words/). It includes many words that you might not consider stop words. Nevertheless, I'll just use a smaller list here.\n",
    "\n",
    "Note that I'll use a Scala [Set](http://www.scala-lang.org/api/current/index.html#scala.collection.immutable.Set) to hold the stop words. We want _O(1)_ look-up performance. We just want to know if the word is in the set or not.\n",
    "\n",
    "I'll also add \"\", so I can remove the explicit test for it.\n",
    "\n",
    "Finally, we'll embed the whole thing in a new Scala `object`. This extra encapsulation is a way to work around occasional problems with \"task not serializable\" errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WARNING**: The definition in the next cell may trigger a `Task not serializable` error in the cell that follows, where it is used. Of so, this is \"quirk\" of the Scala interpreter running with the notebook environment. This code should work without issues in Spark applications that you write, i.e., that you compile into applications with `scalac`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined object IIStopWords\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "object IIStopWords {\n",
    "    val stopWords = Set(\"\", \"a\", \"an\", \"and\", \"I\", \"he\", \"she\", \"it\", \"the\")\n",
    "\n",
    "    /**\n",
    "     * If the set contains the word, we return false - we don't want to keep it!\n",
    "     * Note we assume the word has already been converted to lower case!\n",
    "     */\n",
    "    def keep(word: String): Boolean = stopWords.contains(word) == false  \n",
    "    \n",
    "    def compute(sc: org.apache.spark.SparkContext, input: String) = {\n",
    "        sc.wholeTextFiles(input).\n",
    "        flatMap {\n",
    "            case (location, contents) => \n",
    "                val words = contents.split(\"\"\"\\W+\"\"\").\n",
    "                    map(word => word.toLowerCase).  // Do this early, before keep()\n",
    "                    filter(word => keep(word))      // <== filter here\n",
    "                val fileName = location.split(java.io.File.separator).last\n",
    "                words.map(word => ((word, fileName), 1))\n",
    "        }.\n",
    "        reduceByKey((count1, count2) => count1 + count2).\n",
    "        map { \n",
    "            case ((word, fileName), count) => (word, (fileName, count)) \n",
    "        }.\n",
    "        groupByKey.\n",
    "        sortByKey(ascending = true).\n",
    "        map { \n",
    "            case (word, iterable) => \n",
    "                val vect = iterable.toVector.sortBy { \n",
    "                    case (fileName, count) => (-count, fileName) \n",
    "                }\n",
    "                val (locations, counts) = vect.unzip  \n",
    "                val totalCount = counts.reduceLeft((n1,n2) => n1+n2)        \n",
    "                (word, totalCount, locations, counts)\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lastException = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Task not serializable\n",
       "StackTrace:   at org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:335)\n",
       "  at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:159)\n",
       "  at org.apache.spark.SparkContext.clean(SparkContext.scala:2292)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$flatMap$1.apply(RDD.scala:380)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$flatMap$1.apply(RDD.scala:379)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
       "  at org.apache.spark.rdd.RDD.flatMap(RDD.scala:379)\n",
       "  at IIStopWords$.compute(<console>:80)\n",
       "  ... 52 elided\n",
       "Caused by: java.io.NotSerializableException: IIStopWords$\n",
       "Serialization stack:\n",
       "\t- object not serializable (class: IIStopWords$, value: IIStopWords$@a4424d)\n",
       "\t- field (class: IIStopWords$$anonfun$3, name: $outer, type: class IIStopWords$)\n",
       "\t- object (class IIStopWords$$anonfun$3, <function1>)\n",
       "  at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:40)\n",
       "  at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)\n",
       "  at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)\n",
       "  at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:342)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val iiStopWords = IIStopWords.compute(sc, \"/home/jovyan/work/data/shakespeare\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Unknown Error\n",
       "Message: lastException: Throwable = null\n",
       "<console>:58: error: not found: value iiStopWords\n",
       "       iiStopWords.take(100).foreach(println)\n",
       "       ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iiStopWords.take(100).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last thing, we now have `filter(word => keep(word))`, but note how we used `println` in the previous cell to see results. We can do something similar with `filter` and instead write `filter(keep)`. \n",
    "\n",
    "What does this mean exactly? It tells the compiler \"convert the _method_ `keep` to a _function_ and pass that to `filter`.\" This works because `keep` already does what `filter` wants, take a single string argument and return a boolean result.\n",
    "\n",
    "Passing `keep` is actually different than passing `word => keep(word)`, which is an _anonymous_ function that _calls_ keep. We are using `keep` as the function itself, rather than constructing a function that uses `keep`."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
